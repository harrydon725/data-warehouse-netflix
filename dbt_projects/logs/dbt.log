[0m09:00:23.384945 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112774f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127ef850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127eff90>]}


============================== 09:00:23.387743 | 7f5c82df-7294-45fc-9b99-5365ae4b50b7 ==============================
[0m09:00:23.387743 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:00:23.388262 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'quiet': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'version_check': 'True', 'log_format': 'default', 'fail_fast': 'False', 'empty': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'partial_parse': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'no_print': 'None', 'printer_width': '80', 'debug': 'False', 'invocation_command': 'dbt debug', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'warn_error': 'None'}
[0m09:00:23.401754 [info ] [MainThread]: dbt version: 1.10.13
[0m09:00:23.402002 [info ] [MainThread]: python version: 3.11.2
[0m09:00:23.402183 [info ] [MainThread]: python path: /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/venv/bin/python
[0m09:00:23.402332 [info ] [MainThread]: os info: macOS-14.8.1-arm64-arm-64bit
[0m09:00:23.505382 [info ] [MainThread]: Using profiles dir at /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects
[0m09:00:23.506008 [info ] [MainThread]: Using profiles.yml file at /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/profiles.yml
[0m09:00:23.506190 [info ] [MainThread]: Using dbt_project.yml file at /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/dbt_project.yml
[0m09:00:23.507703 [info ] [MainThread]: adapter type: postgres
[0m09:00:23.507892 [info ] [MainThread]: adapter version: 1.9.1
[0m09:00:23.586048 [info ] [MainThread]: Configuration:
[0m09:00:23.586548 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m09:00:23.587091 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m09:00:23.587522 [info ] [MainThread]: Required dependencies:
[0m09:00:23.587738 [debug] [MainThread]: Executing "git --help"
[0m09:00:23.613858 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m09:00:23.614617 [debug] [MainThread]: STDERR: "b''"
[0m09:00:23.614838 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m09:00:23.615028 [info ] [MainThread]: Connection:
[0m09:00:23.615235 [info ] [MainThread]:   host: localhost
[0m09:00:23.615367 [info ] [MainThread]:   port: 5433
[0m09:00:23.615496 [info ] [MainThread]:   user: postgres
[0m09:00:23.615621 [info ] [MainThread]:   database: mydb
[0m09:00:23.615744 [info ] [MainThread]:   schema: dbt_warehouse
[0m09:00:23.615867 [info ] [MainThread]:   connect_timeout: 10
[0m09:00:23.615988 [info ] [MainThread]:   role: None
[0m09:00:23.616110 [info ] [MainThread]:   search_path: None
[0m09:00:23.616231 [info ] [MainThread]:   keepalives_idle: 0
[0m09:00:23.616399 [info ] [MainThread]:   sslmode: None
[0m09:00:23.616985 [info ] [MainThread]:   sslcert: None
[0m09:00:23.617147 [info ] [MainThread]:   sslkey: None
[0m09:00:23.617291 [info ] [MainThread]:   sslrootcert: None
[0m09:00:23.617422 [info ] [MainThread]:   application_name: dbt
[0m09:00:23.617549 [info ] [MainThread]:   retries: 1
[0m09:00:23.618048 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:00:23.693718 [debug] [MainThread]: Acquiring new postgres connection 'debug'
[0m09:00:23.744285 [debug] [MainThread]: Using postgres connection "debug"
[0m09:00:23.744509 [debug] [MainThread]: On debug: select 1 as id
[0m09:00:23.744658 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:00:23.941790 [debug] [MainThread]: SQL status: SELECT 1 in 0.197 seconds
[0m09:00:23.944889 [debug] [MainThread]: On debug: Close
[0m09:00:23.945153 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m09:00:23.945362 [info ] [MainThread]: [32mAll checks passed![0m
[0m09:00:23.951050 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 0.62133676, "process_in_blocks": "0", "process_kernel_time": 0.296403, "process_mem_max_rss": "123404288", "process_out_blocks": "0", "process_user_time": 0.914026}
[0m09:00:23.951337 [debug] [MainThread]: Command `dbt debug` succeeded at 09:00:23.951284 after 0.62 seconds
[0m09:00:23.951521 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m09:00:23.951701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127cf450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1127ce110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103010ed0>]}
[0m09:00:23.952179 [debug] [MainThread]: Flushing usage events
[0m09:00:24.625929 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:14:36.114049 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117cdb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117cc610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11179ce50>]}


============================== 09:14:36.117337 | 3d37253d-6948-40e1-8153-6fcc0423db1b ==============================
[0m09:14:36.117337 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:14:36.117718 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'version_check': 'True', 'invocation_command': 'dbt run', 'quiet': 'False', 'use_colors': 'True', 'log_format': 'default', 'log_cache_events': 'False', 'introspect': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'empty': 'False', 'printer_width': '80', 'target_path': 'None', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'warn_error': 'None', 'cache_selected_only': 'False', 'indirect_selection': 'eager'}
[0m09:14:36.276186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11261c7d0>]}
[0m09:14:36.304798 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067c9250>]}
[0m09:14:36.305528 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:14:36.366978 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:14:36.367430 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m09:14:36.367645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x112f767d0>]}
[0m09:14:36.894210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1131a34d0>]}
[0m09:14:36.926406 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:14:36.927609 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:14:36.942592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1136030d0>]}
[0m09:14:36.942848 [info ] [MainThread]: Found 1 model, 1 source, 451 macros
[0m09:14:36.943025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11353c890>]}
[0m09:14:36.943759 [info ] [MainThread]: 
[0m09:14:36.943937 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:14:36.944073 [info ] [MainThread]: 
[0m09:14:36.944312 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:14:36.944709 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m09:14:36.991028 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m09:14:36.991280 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m09:14:36.991466 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:14:37.079356 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.088 seconds
[0m09:14:37.080171 [debug] [ThreadPool]: On list_mydb: Close
[0m09:14:37.080583 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now create_mydb_dbt_warehouse)
[0m09:14:37.080880 [debug] [ThreadPool]: Creating schema "database: "mydb"
schema: "dbt_warehouse"
"
[0m09:14:37.083717 [debug] [ThreadPool]: Using postgres connection "create_mydb_dbt_warehouse"
[0m09:14:37.083897 [debug] [ThreadPool]: On create_mydb_dbt_warehouse: BEGIN
[0m09:14:37.084037 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:14:37.094756 [debug] [ThreadPool]: SQL status: BEGIN in 0.011 seconds
[0m09:14:37.094996 [debug] [ThreadPool]: Using postgres connection "create_mydb_dbt_warehouse"
[0m09:14:37.095175 [debug] [ThreadPool]: On create_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "create_mydb_dbt_warehouse"} */
create schema if not exists "dbt_warehouse"
[0m09:14:37.098447 [debug] [ThreadPool]: SQL status: CREATE SCHEMA in 0.003 seconds
[0m09:14:37.099042 [debug] [ThreadPool]: On create_mydb_dbt_warehouse: COMMIT
[0m09:14:37.099220 [debug] [ThreadPool]: Using postgres connection "create_mydb_dbt_warehouse"
[0m09:14:37.099364 [debug] [ThreadPool]: On create_mydb_dbt_warehouse: COMMIT
[0m09:14:37.101303 [debug] [ThreadPool]: SQL status: COMMIT in 0.002 seconds
[0m09:14:37.101476 [debug] [ThreadPool]: On create_mydb_dbt_warehouse: Close
[0m09:14:37.102087 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m09:14:37.105315 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:14:37.105490 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m09:14:37.105629 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:14:37.118026 [debug] [ThreadPool]: SQL status: BEGIN in 0.012 seconds
[0m09:14:37.118291 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:14:37.118511 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m09:14:37.128787 [debug] [ThreadPool]: SQL status: SELECT 0 in 0.010 seconds
[0m09:14:37.129543 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m09:14:37.129950 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m09:14:37.132454 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.132632 [debug] [MainThread]: On master: BEGIN
[0m09:14:37.132774 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:14:37.140168 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m09:14:37.140353 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.140571 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:14:37.154588 [debug] [MainThread]: SQL status: SELECT 33 in 0.014 seconds
[0m09:14:37.155691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11413f810>]}
[0m09:14:37.156055 [debug] [MainThread]: On master: ROLLBACK
[0m09:14:37.156484 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.156644 [debug] [MainThread]: On master: BEGIN
[0m09:14:37.157261 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m09:14:37.157427 [debug] [MainThread]: On master: COMMIT
[0m09:14:37.157586 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.157735 [debug] [MainThread]: On master: COMMIT
[0m09:14:37.158046 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:14:37.158207 [debug] [MainThread]: On master: Close
[0m09:14:37.160062 [debug] [Thread-1 (]: Began running node model.maker_warehouse.movies
[0m09:14:37.160346 [info ] [Thread-1 (]: 1 of 1 START sql view model dbt_warehouse.movies ............................... [RUN]
[0m09:14:37.160593 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly create_mydb_dbt_warehouse, now model.maker_warehouse.movies)
[0m09:14:37.160784 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.movies
[0m09:14:37.164918 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m09:14:37.165613 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.movies
[0m09:14:37.185323 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m09:14:37.185965 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:14:37.186164 [debug] [Thread-1 (]: On model.maker_warehouse.movies: BEGIN
[0m09:14:37.186335 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:14:37.192999 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m09:14:37.193220 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:14:37.193396 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  create view "mydb"."dbt_warehouse"."movies__dbt_tmp"
    
    
  as (
    SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
[0m09:14:37.201044 [debug] [Thread-1 (]: SQL status: CREATE VIEW in 0.007 seconds
[0m09:14:37.204476 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:14:37.204670 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m09:14:37.205574 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:14:37.213655 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m09:14:37.213861 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:14:37.214028 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m09:14:37.215358 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m09:14:37.218350 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m09:14:37.220733 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:14:37.220920 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop view if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m09:14:37.221384 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.000 seconds
[0m09:14:37.222532 [debug] [Thread-1 (]: On model.maker_warehouse.movies: Close
[0m09:14:37.223597 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3d37253d-6948-40e1-8153-6fcc0423db1b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11411c390>]}
[0m09:14:37.223918 [info ] [Thread-1 (]: 1 of 1 OK created sql view model dbt_warehouse.movies .......................... [[32mCREATE VIEW[0m in 0.06s]
[0m09:14:37.224192 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.movies
[0m09:14:37.224847 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.224996 [debug] [MainThread]: On master: BEGIN
[0m09:14:37.225131 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:14:37.230961 [debug] [MainThread]: SQL status: BEGIN in 0.006 seconds
[0m09:14:37.231150 [debug] [MainThread]: On master: COMMIT
[0m09:14:37.231302 [debug] [MainThread]: Using postgres connection "master"
[0m09:14:37.231433 [debug] [MainThread]: On master: COMMIT
[0m09:14:37.231808 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:14:37.231950 [debug] [MainThread]: On master: Close
[0m09:14:37.232148 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:14:37.232278 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m09:14:37.232400 [debug] [MainThread]: Connection 'list_mydb_dbt_warehouse' was properly closed.
[0m09:14:37.232549 [info ] [MainThread]: 
[0m09:14:37.232708 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 0.29 seconds (0.29s).
[0m09:14:37.233023 [debug] [MainThread]: Command end result
[0m09:14:37.243017 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:14:37.244155 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:14:37.246943 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m09:14:37.247109 [info ] [MainThread]: 
[0m09:14:37.247311 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:14:37.247453 [info ] [MainThread]: 
[0m09:14:37.247607 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m09:14:37.267803 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.1993917, "process_in_blocks": "0", "process_kernel_time": 0.242633, "process_mem_max_rss": "133578752", "process_out_blocks": "0", "process_user_time": 1.412249}
[0m09:14:37.268127 [debug] [MainThread]: Command `dbt run` succeeded at 09:14:37.268074 after 1.20 seconds
[0m09:14:37.268334 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104994ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1117cdb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048e7d10>]}
[0m09:14:37.268517 [debug] [MainThread]: Flushing usage events
[0m09:14:37.801163 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:18:35.773843 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056d3dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e3850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056e39d0>]}


============================== 09:18:35.776709 | 7ef9ab60-308c-4974-b640-a6d0edb4620c ==============================
[0m09:18:35.776709 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:18:35.777040 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'write_json': 'True', 'log_cache_events': 'False', 'debug': 'False', 'partial_parse': 'True', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'introspect': 'True', 'target_path': 'None', 'static_parser': 'True', 'version_check': 'True', 'printer_width': '80', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'no_print': 'None', 'quiet': 'False', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'use_colors': 'True', 'empty': 'False'}
[0m09:18:35.883814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d29450>]}
[0m09:18:35.911042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d54c50>]}
[0m09:18:35.911627 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:18:35.968364 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:18:36.037163 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:18:36.037457 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/facts/movies.sql
[0m09:18:36.154564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106398e50>]}
[0m09:18:36.211803 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:18:36.221290 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:18:36.239591 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1068cbfd0>]}
[0m09:18:36.239900 [info ] [MainThread]: Found 1 model, 1 source, 451 macros
[0m09:18:36.240093 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10689bad0>]}
[0m09:18:36.240931 [info ] [MainThread]: 
[0m09:18:36.241136 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:18:36.241380 [info ] [MainThread]: 
[0m09:18:36.241667 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:18:36.242130 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m09:18:36.299889 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m09:18:36.300105 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m09:18:36.300244 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:18:36.426659 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.126 seconds
[0m09:18:36.427405 [debug] [ThreadPool]: On list_mydb: Close
[0m09:18:36.428092 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m09:18:36.431509 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:18:36.431692 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m09:18:36.431832 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:18:36.440087 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m09:18:36.440272 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:18:36.440441 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m09:18:36.449097 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.008 seconds
[0m09:18:36.449771 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m09:18:36.450145 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m09:18:36.452482 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:36.452659 [debug] [MainThread]: On master: BEGIN
[0m09:18:36.452802 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:18:36.460029 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m09:18:36.460203 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:36.460397 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:18:36.468970 [debug] [MainThread]: SQL status: SELECT 34 in 0.008 seconds
[0m09:18:36.469931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10652f350>]}
[0m09:18:36.470185 [debug] [MainThread]: On master: ROLLBACK
[0m09:18:36.470576 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:36.470724 [debug] [MainThread]: On master: BEGIN
[0m09:18:36.471279 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m09:18:36.471539 [debug] [MainThread]: On master: COMMIT
[0m09:18:36.471752 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:36.471914 [debug] [MainThread]: On master: COMMIT
[0m09:18:36.472236 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:18:36.472396 [debug] [MainThread]: On master: Close
[0m09:18:36.473980 [debug] [Thread-1 (]: Began running node model.maker_warehouse.movies
[0m09:18:36.474246 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m09:18:36.474483 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb, now model.maker_warehouse.movies)
[0m09:18:36.474662 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.movies
[0m09:18:36.478542 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m09:18:36.478948 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.movies
[0m09:18:36.499142 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m09:18:36.499724 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:36.499907 [debug] [Thread-1 (]: On model.maker_warehouse.movies: BEGIN
[0m09:18:36.500059 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:18:36.507129 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m09:18:36.507319 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:36.507496 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
  
[0m09:18:38.329930 [debug] [Thread-1 (]: SQL status: SELECT 8472 in 1.821 seconds
[0m09:18:38.352356 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:38.352914 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m09:18:38.355208 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m09:18:38.357819 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:38.358326 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m09:18:38.359172 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:18:38.371426 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m09:18:38.371856 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:38.372240 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m09:18:38.378058 [debug] [Thread-1 (]: SQL status: COMMIT in 0.005 seconds
[0m09:18:38.382722 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m09:18:38.386074 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:18:38.386385 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop view if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m09:18:38.391393 [debug] [Thread-1 (]: SQL status: DROP VIEW in 0.005 seconds
[0m09:18:38.393332 [debug] [Thread-1 (]: On model.maker_warehouse.movies: Close
[0m09:18:38.395043 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7ef9ab60-308c-4974-b640-a6d0edb4620c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106714a50>]}
[0m09:18:38.395519 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 1.92s]
[0m09:18:38.395877 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.movies
[0m09:18:38.396749 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:38.396924 [debug] [MainThread]: On master: BEGIN
[0m09:18:38.397075 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:18:38.409369 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m09:18:38.409606 [debug] [MainThread]: On master: COMMIT
[0m09:18:38.409832 [debug] [MainThread]: Using postgres connection "master"
[0m09:18:38.410076 [debug] [MainThread]: On master: COMMIT
[0m09:18:38.410487 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:18:38.410701 [debug] [MainThread]: On master: Close
[0m09:18:38.410949 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:18:38.411136 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m09:18:38.411395 [debug] [MainThread]: Connection 'list_mydb_dbt_warehouse' was properly closed.
[0m09:18:38.412260 [info ] [MainThread]: 
[0m09:18:38.412562 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.17 seconds (2.17s).
[0m09:18:38.413302 [debug] [MainThread]: Command end result
[0m09:18:38.430210 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:18:38.431670 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:18:38.435227 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m09:18:38.435433 [info ] [MainThread]: 
[0m09:18:38.435665 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:18:38.435832 [info ] [MainThread]: 
[0m09:18:38.436022 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m09:18:38.437785 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.7047167, "process_in_blocks": "0", "process_kernel_time": 0.222811, "process_mem_max_rss": "131563520", "process_out_blocks": "0", "process_user_time": 1.121784}
[0m09:18:38.438077 [debug] [MainThread]: Command `dbt run` succeeded at 09:18:38.438027 after 2.71 seconds
[0m09:18:38.438326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100f20ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056f8e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100e73d10>]}
[0m09:18:38.438622 [debug] [MainThread]: Flushing usage events
[0m09:18:38.998238 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:26:57.489377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10996f290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099c4610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109995290>]}


============================== 09:26:57.492185 | 0c2b9903-84c8-4863-9d75-9587218a6b07 ==============================
[0m09:26:57.492185 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:26:57.492514 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'no_print': 'None', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'fail_fast': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'use_colors': 'True', 'empty': 'False', 'quiet': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'target_path': 'None', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'static_parser': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'printer_width': '80'}
[0m09:26:57.601448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109ff3c90>]}
[0m09:26:57.630885 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107048a10>]}
[0m09:26:57.631512 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:26:57.693762 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:26:57.765588 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 4 files added, 0 files changed.
[0m09:26:57.765872 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/dimensions/users.sql
[0m09:26:57.766058 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/dimensions/movies.sql
[0m09:26:57.766215 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/dimensions/genres.sql
[0m09:26:57.766363 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/facts/streams.sql
[0m09:26:57.766505 [debug] [MainThread]: Partial parsing: deleted file: maker_warehouse://models/facts/movies.sql
[0m09:26:57.890228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1099c5790>]}
[0m09:26:57.922869 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:26:57.924479 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:26:57.937082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10abffd50>]}
[0m09:26:57.937322 [info ] [MainThread]: Found 4 models, 1 source, 451 macros
[0m09:26:57.937498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ab30810>]}
[0m09:26:57.938354 [info ] [MainThread]: 
[0m09:26:57.938528 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:26:57.938664 [info ] [MainThread]: 
[0m09:26:57.938897 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:26:57.940734 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m09:26:57.998345 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m09:26:57.998565 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m09:26:57.998706 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:26:58.097109 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.098 seconds
[0m09:26:58.097871 [debug] [ThreadPool]: On list_mydb: Close
[0m09:26:58.098641 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m09:26:58.101850 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:26:58.102019 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m09:26:58.102156 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:26:58.110283 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m09:26:58.110490 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:26:58.110677 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m09:26:58.120690 [debug] [ThreadPool]: SQL status: SELECT 1 in 0.010 seconds
[0m09:26:58.121392 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m09:26:58.121771 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m09:26:58.124309 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.124483 [debug] [MainThread]: On master: BEGIN
[0m09:26:58.124627 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:26:58.142733 [debug] [MainThread]: SQL status: BEGIN in 0.018 seconds
[0m09:26:58.142984 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.143202 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:26:58.149874 [debug] [MainThread]: SQL status: SELECT 33 in 0.006 seconds
[0m09:26:58.150964 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10aa46710>]}
[0m09:26:58.151258 [debug] [MainThread]: On master: ROLLBACK
[0m09:26:58.151702 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.151869 [debug] [MainThread]: On master: BEGIN
[0m09:26:58.152519 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m09:26:58.152748 [debug] [MainThread]: On master: COMMIT
[0m09:26:58.152958 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.153127 [debug] [MainThread]: On master: COMMIT
[0m09:26:58.153503 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:26:58.153712 [debug] [MainThread]: On master: Close
[0m09:26:58.155746 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m09:26:58.155969 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m09:26:58.156410 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m09:26:58.156243 [info ] [Thread-1 (]: 1 of 4 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m09:26:58.156657 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m09:26:58.156887 [info ] [Thread-2 (]: 2 of 4 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m09:26:58.157142 [info ] [Thread-3 (]: 3 of 4 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m09:26:58.157428 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m09:26:58.157690 [info ] [Thread-4 (]: 4 of 4 START sql table model dbt_warehouse.users ............................... [RUN]
[0m09:26:58.157987 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m09:26:58.158252 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m09:26:58.158446 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m09:26:58.158685 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.users'
[0m09:26:58.158863 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m09:26:58.159062 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m09:26:58.163287 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m09:26:58.163512 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m09:26:58.165187 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m09:26:58.167107 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m09:26:58.168701 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m09:26:58.169117 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m09:26:58.169326 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m09:26:58.175467 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m09:26:58.187966 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m09:26:58.188159 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m09:26:58.190756 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m09:26:58.192345 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m09:26:58.194095 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m09:26:58.194494 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:26:58.194759 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.194937 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m09:26:58.195121 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:26:58.195345 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:26:58.195515 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m09:26:58.195689 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:26:58.195856 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m09:26:58.196010 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m09:26:58.196156 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m09:26:58.196381 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m09:26:58.196551 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m09:26:58.206371 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m09:26:58.206677 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.206952 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
  
[0m09:26:58.207200 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m09:26:58.207429 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m09:26:58.207601 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:26:58.207763 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m09:26:58.207943 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:26:58.208124 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    genres
FROM mydb.public.raw_netflix
  );
  
[0m09:26:58.208310 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:26:58.208488 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    user_id
FROM mydb.public.raw_netflix
  );
  
[0m09:26:58.208714 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

SELECT
    DISTINCT
    id,
    user_id,
    movie_id,
    genres,
    date
FROM mydb.public.raw_netflix
  );
  
[0m09:26:58.209744 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "date" does not exist
LINE 20:     date
             ^

[0m09:26:58.210097 [debug] [Thread-3 (]: On model.maker_warehouse.streams: ROLLBACK
[0m09:26:58.211055 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m09:26:58.217020 [debug] [Thread-3 (]: Database Error in model streams (models/facts/streams.sql)
  column "date" does not exist
  LINE 20:     date
               ^
  compiled code at target/run/maker_warehouse/models/facts/streams.sql
[0m09:26:58.218172 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a9cff50>]}
[0m09:26:58.218553 [error] [Thread-3 (]: 3 of 4 ERROR creating sql table model dbt_warehouse.streams .................... [[31mERROR[0m in 0.06s]
[0m09:26:58.218859 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m09:26:58.219118 [debug] [Thread-7 (]: Marking all children of 'model.maker_warehouse.streams' to be skipped because of status 'error'.  Reason: Database Error in model streams (models/facts/streams.sql)
  column "date" does not exist
  LINE 20:     date
               ^
  compiled code at target/run/maker_warehouse/models/facts/streams.sql.
[0m09:26:58.534490 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.325 seconds
[0m09:26:58.551873 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:26:58.552217 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m09:26:58.553472 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:26:58.563071 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m09:26:58.563818 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:26:58.564098 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m09:26:58.566202 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m09:26:58.571293 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m09:26:58.574118 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:26:58.574362 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m09:26:58.575277 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.001 seconds
[0m09:26:58.576618 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m09:26:58.577081 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae76210>]}
[0m09:26:58.577456 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.42s]
[0m09:26:58.577792 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m09:26:58.582208 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 0.375 seconds
[0m09:26:58.584050 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.584302 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m09:26:58.585222 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:26:58.586770 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.586944 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m09:26:58.587483 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m09:26:58.589283 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m09:26:58.589468 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.589629 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m09:26:58.595369 [debug] [Thread-2 (]: SQL status: COMMIT in 0.006 seconds
[0m09:26:58.596882 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m09:26:58.597279 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:26:58.597476 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m09:26:58.615041 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.017 seconds
[0m09:26:58.616231 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m09:26:58.616851 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10ae83650>]}
[0m09:26:58.617421 [info ] [Thread-2 (]: 2 of 4 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 0.46s]
[0m09:26:58.617900 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m09:26:58.824519 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 0.615 seconds
[0m09:26:58.829075 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:26:58.829370 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m09:26:58.830155 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:26:58.830948 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m09:26:58.831152 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:26:58.831324 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m09:26:58.895598 [debug] [Thread-4 (]: SQL status: COMMIT in 0.064 seconds
[0m09:26:58.900500 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m09:26:58.901514 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:26:58.901758 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m09:26:58.902359 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.000 seconds
[0m09:26:58.904413 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m09:26:58.904755 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0c2b9903-84c8-4863-9d75-9587218a6b07', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10f094090>]}
[0m09:26:58.905105 [info ] [Thread-4 (]: 4 of 4 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 0.75s]
[0m09:26:58.907724 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m09:26:58.908428 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.908579 [debug] [MainThread]: On master: BEGIN
[0m09:26:58.910651 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:26:58.920417 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m09:26:58.920700 [debug] [MainThread]: On master: COMMIT
[0m09:26:58.920883 [debug] [MainThread]: Using postgres connection "master"
[0m09:26:58.921040 [debug] [MainThread]: On master: COMMIT
[0m09:26:58.921416 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:26:58.921580 [debug] [MainThread]: On master: Close
[0m09:26:58.921825 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:26:58.921967 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m09:26:58.922091 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m09:26:58.922210 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m09:26:58.922331 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m09:26:58.922535 [info ] [MainThread]: 
[0m09:26:58.922705 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 0.98 seconds (0.98s).
[0m09:26:58.923224 [debug] [MainThread]: Command end result
[0m09:26:58.935484 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:26:58.936918 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:26:58.942113 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m09:26:58.942314 [info ] [MainThread]: 
[0m09:26:58.942517 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m09:26:58.942729 [info ] [MainThread]: 
[0m09:26:58.943054 [error] [MainThread]: [31mFailure in model streams (models/facts/streams.sql)[0m
[0m09:26:58.943337 [error] [MainThread]:   Database Error in model streams (models/facts/streams.sql)
  column "date" does not exist
  LINE 20:     date
               ^
  compiled code at target/run/maker_warehouse/models/facts/streams.sql
[0m09:26:58.943571 [info ] [MainThread]: 
[0m09:26:58.943805 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/facts/streams.sql
[0m09:26:58.944000 [info ] [MainThread]: 
[0m09:26:58.944209 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=4
[0m09:26:58.947186 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.4944057, "process_in_blocks": "0", "process_kernel_time": 0.227387, "process_mem_max_rss": "136151040", "process_out_blocks": "0", "process_user_time": 1.178614}
[0m09:26:58.947442 [debug] [MainThread]: Command `dbt run` failed at 09:26:58.947394 after 1.49 seconds
[0m09:26:58.947754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105214ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10702a6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10516c890>]}
[0m09:26:58.948163 [debug] [MainThread]: Flushing usage events
[0m09:26:59.453037 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m09:27:52.651112 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075e1550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075ef9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075ec910>]}


============================== 09:27:52.653987 | 60ccbd9f-05a1-4df6-b22b-8c6efb4bd712 ==============================
[0m09:27:52.653987 [info ] [MainThread]: Running with dbt=1.10.13
[0m09:27:52.654316 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'log_cache_events': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'indirect_selection': 'eager', 'partial_parse': 'True', 'static_parser': 'True', 'printer_width': '80', 'invocation_command': 'dbt run', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'write_json': 'True', 'fail_fast': 'False', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'cache_selected_only': 'False', 'introspect': 'True', 'target_path': 'None', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'debug': 'False', 'version_check': 'True', 'no_print': 'None', 'use_colors': 'True'}
[0m09:27:52.766449 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106f6d290>]}
[0m09:27:52.794636 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ccd590>]}
[0m09:27:52.795286 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m09:27:52.890346 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m09:27:52.961881 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m09:27:52.962182 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/facts/streams.sql
[0m09:27:53.080090 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084938d0>]}
[0m09:27:53.113090 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:27:53.114250 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:27:53.127067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108883d50>]}
[0m09:27:53.127331 [info ] [MainThread]: Found 4 models, 1 source, 451 macros
[0m09:27:53.127507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087a3550>]}
[0m09:27:53.128388 [info ] [MainThread]: 
[0m09:27:53.128581 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m09:27:53.128723 [info ] [MainThread]: 
[0m09:27:53.128976 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m09:27:53.130845 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m09:27:53.188013 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m09:27:53.188245 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m09:27:53.188408 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m09:27:53.256609 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.068 seconds
[0m09:27:53.257307 [debug] [ThreadPool]: On list_mydb: Close
[0m09:27:53.258026 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m09:27:53.261370 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:27:53.261548 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m09:27:53.261688 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m09:27:53.271595 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m09:27:53.271789 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m09:27:53.271960 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m09:27:53.295821 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.024 seconds
[0m09:27:53.296704 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m09:27:53.297198 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m09:27:53.300474 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:53.300676 [debug] [MainThread]: On master: BEGIN
[0m09:27:53.300842 [debug] [MainThread]: Opening a new connection, currently in state init
[0m09:27:53.314522 [debug] [MainThread]: SQL status: BEGIN in 0.014 seconds
[0m09:27:53.314814 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:53.315111 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m09:27:53.323931 [debug] [MainThread]: SQL status: SELECT 33 in 0.009 seconds
[0m09:27:53.325125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10835c490>]}
[0m09:27:53.325414 [debug] [MainThread]: On master: ROLLBACK
[0m09:27:53.325853 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:53.326026 [debug] [MainThread]: On master: BEGIN
[0m09:27:53.326602 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m09:27:53.326831 [debug] [MainThread]: On master: COMMIT
[0m09:27:53.327015 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:53.327178 [debug] [MainThread]: On master: COMMIT
[0m09:27:53.327552 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:27:53.327729 [debug] [MainThread]: On master: Close
[0m09:27:53.329536 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m09:27:53.329752 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m09:27:53.329965 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m09:27:53.330156 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m09:27:53.330435 [info ] [Thread-1 (]: 1 of 4 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m09:27:53.330732 [info ] [Thread-2 (]: 2 of 4 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m09:27:53.331000 [info ] [Thread-3 (]: 3 of 4 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m09:27:53.331261 [info ] [Thread-4 (]: 4 of 4 START sql table model dbt_warehouse.users ............................... [RUN]
[0m09:27:53.331517 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m09:27:53.331783 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m09:27:53.332033 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m09:27:53.332274 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.users'
[0m09:27:53.332457 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m09:27:53.332636 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m09:27:53.332824 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m09:27:53.332990 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m09:27:53.337169 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m09:27:53.338947 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m09:27:53.340735 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m09:27:53.342325 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m09:27:53.342837 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m09:27:53.343026 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m09:27:53.343192 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m09:27:53.362278 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m09:27:53.362503 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m09:27:53.364313 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m09:27:53.366751 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m09:27:53.368417 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m09:27:53.368924 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.369128 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.369320 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:27:53.369496 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m09:27:53.369716 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m09:27:53.369902 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:53.370061 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m09:27:53.370223 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m09:27:53.370376 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m09:27:53.370535 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m09:27:53.370683 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m09:27:53.370989 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m09:27:53.379068 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m09:27:53.379282 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.379568 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    genres
FROM mydb.public.raw_netflix
  );
  
[0m09:27:53.381108 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m09:27:53.381415 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.381610 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
  
[0m09:27:53.381815 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m09:27:53.381978 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:27:53.382172 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

SELECT
    DISTINCT
    id,
    user_id,
    movie_id,
    genres,
    datetime
FROM mydb.public.raw_netflix
  );
  
[0m09:27:53.382791 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m09:27:53.383007 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:53.383243 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    user_id
FROM mydb.public.raw_netflix
  );
  
[0m09:27:53.695082 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.315 seconds
[0m09:27:53.710382 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.710690 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m09:27:53.711723 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:27:53.713490 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.713683 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m09:27:53.715083 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:27:53.723750 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m09:27:53.724033 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.724223 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m09:27:53.727435 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m09:27:53.730688 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m09:27:53.733214 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m09:27:53.733461 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m09:27:53.742918 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.009 seconds
[0m09:27:53.745489 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m09:27:53.747172 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b22e90>]}
[0m09:27:53.747792 [info ] [Thread-1 (]: 1 of 4 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.41s]
[0m09:27:53.748273 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m09:27:53.821191 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 0.439 seconds
[0m09:27:53.825658 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.825933 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m09:27:53.827442 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:27:53.829138 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.829338 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m09:27:53.830019 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m09:27:53.830853 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m09:27:53.831035 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.831194 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m09:27:53.870098 [debug] [Thread-2 (]: SQL status: COMMIT in 0.038 seconds
[0m09:27:53.874710 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m09:27:53.876082 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m09:27:53.876663 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m09:27:53.886830 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.009 seconds
[0m09:27:53.888926 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m09:27:53.889709 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108b2fd90>]}
[0m09:27:53.890497 [info ] [Thread-2 (]: 2 of 4 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 0.56s]
[0m09:27:53.891154 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m09:27:54.033133 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 0.649 seconds
[0m09:27:54.040578 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:54.041235 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m09:27:54.042679 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:27:54.049405 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:54.049995 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m09:27:54.050962 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m09:27:54.052811 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m09:27:54.053233 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:54.053632 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m09:27:54.072824 [debug] [Thread-4 (]: SQL status: COMMIT in 0.019 seconds
[0m09:27:54.077140 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m09:27:54.078298 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m09:27:54.078748 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m09:27:54.092842 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.013 seconds
[0m09:27:54.095061 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m09:27:54.096000 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c25010>]}
[0m09:27:54.096770 [info ] [Thread-4 (]: 4 of 4 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 0.76s]
[0m09:27:54.097484 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m09:27:55.068007 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 1.685 seconds
[0m09:27:55.080810 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:27:55.082458 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m09:27:55.083915 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m09:27:55.085342 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m09:27:55.085562 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:27:55.085733 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m09:27:55.109729 [debug] [Thread-3 (]: SQL status: COMMIT in 0.024 seconds
[0m09:27:55.114285 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m09:27:55.115421 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m09:27:55.115997 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m09:27:55.116956 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.000 seconds
[0m09:27:55.119079 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m09:27:55.120069 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '60ccbd9f-05a1-4df6-b22b-8c6efb4bd712', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108bd9510>]}
[0m09:27:55.120891 [info ] [Thread-3 (]: 3 of 4 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 1.79s]
[0m09:27:55.121548 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m09:27:55.122945 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:55.123193 [debug] [MainThread]: On master: BEGIN
[0m09:27:55.123351 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m09:27:55.133305 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m09:27:55.133576 [debug] [MainThread]: On master: COMMIT
[0m09:27:55.133755 [debug] [MainThread]: Using postgres connection "master"
[0m09:27:55.133896 [debug] [MainThread]: On master: COMMIT
[0m09:27:55.134292 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m09:27:55.134451 [debug] [MainThread]: On master: Close
[0m09:27:55.134690 [debug] [MainThread]: Connection 'master' was properly closed.
[0m09:27:55.134829 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m09:27:55.134958 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m09:27:55.135078 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m09:27:55.135198 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m09:27:55.135426 [info ] [MainThread]: 
[0m09:27:55.135604 [info ] [MainThread]: Finished running 4 table models in 0 hours 0 minutes and 2.01 seconds (2.01s).
[0m09:27:55.136147 [debug] [MainThread]: Command end result
[0m09:27:55.156443 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m09:27:55.158971 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m09:27:55.163925 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m09:27:55.164131 [info ] [MainThread]: 
[0m09:27:55.164356 [info ] [MainThread]: [32mCompleted successfully[0m
[0m09:27:55.164541 [info ] [MainThread]: 
[0m09:27:55.164795 [info ] [MainThread]: Done. PASS=4 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=4
[0m09:27:55.168219 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.5543983, "process_in_blocks": "0", "process_kernel_time": 0.224303, "process_mem_max_rss": "131858432", "process_out_blocks": "0", "process_user_time": 1.219269}
[0m09:27:55.168670 [debug] [MainThread]: Command `dbt run` succeeded at 09:27:55.168616 after 2.56 seconds
[0m09:27:55.168997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e98ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075e1250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106941410>]}
[0m09:27:55.169223 [debug] [MainThread]: Flushing usage events
[0m09:27:55.678238 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:24:32.437448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cf0c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d490d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d70e50>]}


============================== 13:24:32.440508 | b25349ff-4c84-4f35-92b4-b29da7706fc8 ==============================
[0m13:24:32.440508 [info ] [MainThread]: Running with dbt=1.10.13
[0m13:24:32.440850 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'empty': 'False', 'log_cache_events': 'False', 'fail_fast': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'partial_parse': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'write_json': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'use_colors': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'invocation_command': 'dbt run', 'introspect': 'True', 'warn_error': 'None', 'version_check': 'True', 'target_path': 'None', 'static_parser': 'True'}
[0m13:24:32.822493 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b25349ff-4c84-4f35-92b4-b29da7706fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cf0190>]}
[0m13:24:32.852203 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b25349ff-4c84-4f35-92b4-b29da7706fc8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043d1410>]}
[0m13:24:32.852859 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:24:32.909637 [error] [MainThread]: Encountered an error:
Compilation Error
  dbt found 1 package(s) specified in packages.yml, but only 0 package(s) installed in dbt_packages. Run "dbt deps" to install package dependencies.
[0m13:24:32.919674 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 0.52596617, "process_in_blocks": "0", "process_kernel_time": 0.16334, "process_mem_max_rss": "110706688", "process_out_blocks": "0", "process_user_time": 0.831572}
[0m13:24:32.920033 [debug] [MainThread]: Command `dbt run` failed at 13:24:32.919972 after 0.53 seconds
[0m13:24:32.920281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d490d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d4ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041565d0>]}
[0m13:24:32.920483 [debug] [MainThread]: Flushing usage events
[0m13:24:33.458874 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:24:41.334154 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105034e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104ffdc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105087c90>]}


============================== 13:24:41.336691 | 050e1d85-8666-40f4-be86-4aa1d62dfd32 ==============================
[0m13:24:41.336691 [info ] [MainThread]: Running with dbt=1.10.13
[0m13:24:41.337017 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'invocation_command': 'dbt deps', 'use_colors': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'write_json': 'True', 'target_path': 'None', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'printer_width': '80', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'partial_parse': 'True', 'log_format': 'default', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'log_cache_events': 'False', 'quiet': 'False', 'indirect_selection': 'eager', 'fail_fast': 'False', 'version_check': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True'}
[0m13:24:41.395770 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '050e1d85-8666-40f4-be86-4aa1d62dfd32', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105087890>]}
[0m13:24:41.411642 [debug] [MainThread]: Set downloads directory='/var/folders/m2/7srz5b45113cbcx00jpnxjr40000gp/T/dbt-downloads-iru26env'
[0m13:24:41.411933 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m13:24:41.776643 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m13:24:41.780736 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m13:24:41.830492 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m13:24:41.846833 [info ] [MainThread]: Updating lock file in file path: /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/package-lock.yml
[0m13:24:41.849000 [debug] [MainThread]: Set downloads directory='/var/folders/m2/7srz5b45113cbcx00jpnxjr40000gp/T/dbt-downloads-2tai106e'
[0m13:24:41.852161 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m13:24:42.103043 [info ] [MainThread]: Installed from version 1.3.0
[0m13:24:42.103338 [info ] [MainThread]: Updated version available: 1.3.2
[0m13:24:42.103559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '050e1d85-8666-40f4-be86-4aa1d62dfd32', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105403fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105251990>]}
[0m13:24:42.103794 [info ] [MainThread]: 
[0m13:24:42.103966 [info ] [MainThread]: Updates available for packages: ['dbt-labs/dbt_utils']                 
Update your versions in packages.yml, then run dbt deps
[0m13:24:42.105027 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.8068481, "process_in_blocks": "0", "process_kernel_time": 0.159486, "process_mem_max_rss": "114917376", "process_out_blocks": "0", "process_user_time": 0.789738}
[0m13:24:42.105303 [debug] [MainThread]: Command `dbt deps` succeeded at 13:24:42.105252 after 0.81 seconds
[0m13:24:42.105505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1008b4ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10508fe90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10435d410>]}
[0m13:24:42.105701 [debug] [MainThread]: Flushing usage events
[0m13:24:42.479647 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m13:28:43.978432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a161d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b77ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b77f90>]}


============================== 13:28:43.981704 | a7a31023-4049-4a1d-8925-875b233bf5d1 ==============================
[0m13:28:43.981704 [info ] [MainThread]: Running with dbt=1.10.13
[0m13:28:43.982059 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'static_parser': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'quiet': 'False', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'indirect_selection': 'eager', 'warn_error': 'None', 'introspect': 'True', 'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'write_json': 'True', 'log_format': 'default', 'fail_fast': 'False', 'target_path': 'None', 'empty': 'False', 'invocation_command': 'dbt run', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False'}
[0m13:28:44.099639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105d83390>]}
[0m13:28:44.131266 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1041d5550>]}
[0m13:28:44.131965 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m13:28:44.192893 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m13:28:44.234642 [info ] [MainThread]: Unable to do partial parsing because a project dependency has been added
[0m13:28:44.234977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105ce8390>]}
[0m13:28:44.863430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107c29450>]}
[0m13:28:44.896204 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m13:28:44.897275 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m13:28:44.910263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107f57010>]}
[0m13:28:44.910503 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m13:28:44.910676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107df9610>]}
[0m13:28:44.911523 [info ] [MainThread]: 
[0m13:28:44.911700 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m13:28:44.911837 [info ] [MainThread]: 
[0m13:28:44.912059 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m13:28:44.913981 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m13:28:44.944334 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m13:28:44.944530 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m13:28:44.944665 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:28:45.140047 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.195 seconds
[0m13:28:45.141152 [debug] [ThreadPool]: On list_mydb: Close
[0m13:28:45.142347 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m13:28:45.147202 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m13:28:45.147454 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m13:28:45.147646 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m13:28:45.157581 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m13:28:45.157880 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m13:28:45.158108 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m13:28:45.169731 [debug] [ThreadPool]: SQL status: SELECT 4 in 0.011 seconds
[0m13:28:45.170942 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m13:28:45.171510 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m13:28:45.175580 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:45.175817 [debug] [MainThread]: On master: BEGIN
[0m13:28:45.175994 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:28:45.197840 [debug] [MainThread]: SQL status: BEGIN in 0.022 seconds
[0m13:28:45.198137 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:45.198376 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m13:28:45.245428 [debug] [MainThread]: SQL status: SELECT 33 in 0.047 seconds
[0m13:28:45.246739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107b31310>]}
[0m13:28:45.247061 [debug] [MainThread]: On master: ROLLBACK
[0m13:28:45.247610 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:45.247805 [debug] [MainThread]: On master: BEGIN
[0m13:28:45.248401 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m13:28:45.248568 [debug] [MainThread]: On master: COMMIT
[0m13:28:45.248719 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:45.248861 [debug] [MainThread]: On master: COMMIT
[0m13:28:45.252006 [debug] [MainThread]: SQL status: COMMIT in 0.003 seconds
[0m13:28:45.252244 [debug] [MainThread]: On master: Close
[0m13:28:45.254688 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m13:28:45.254928 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m13:28:45.255136 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m13:28:45.255354 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m13:28:45.255637 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m13:28:45.255905 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m13:28:45.256166 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m13:28:45.256736 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m13:28:45.256426 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m13:28:45.257109 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m13:28:45.257350 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m13:28:45.257632 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m13:28:45.257900 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m13:28:45.258083 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m13:28:45.258256 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m13:28:45.263193 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m13:28:45.263432 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m13:28:45.265062 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m13:28:45.266653 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m13:28:45.269266 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m13:28:45.271124 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m13:28:45.271399 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m13:28:45.271583 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m13:28:45.271756 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m13:28:45.292572 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m13:28:45.295515 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m13:28:45.338325 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m13:28:45.340496 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m13:28:45.341057 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:45.341340 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:45.341598 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m13:28:45.341822 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m13:28:45.342034 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m13:28:45.342218 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m13:28:45.342396 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:45.342560 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m13:28:45.342721 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m13:28:45.342976 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m13:28:45.343146 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m13:28:45.343415 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m13:28:45.352065 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m13:28:45.352313 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:45.352508 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
  
[0m13:28:45.353742 [debug] [Thread-1 (]: SQL status: BEGIN in 0.011 seconds
[0m13:28:45.354090 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m13:28:45.354348 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m13:28:45.354534 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:45.354715 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m13:28:45.354879 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:45.355054 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    genres
FROM mydb.public.raw_netflix
  );
  
[0m13:28:45.355245 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m13:28:45.355433 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

SELECT
    DISTINCT
    id,
    user_id,
    movie_id,
    genres,
    datetime
FROM mydb.public.raw_netflix
  );
  
[0m13:28:45.371742 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.016 seconds
[0m13:28:45.375647 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m13:28:45.376216 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m13:28:45.377234 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:45.386001 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m13:28:45.386363 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m13:28:45.386598 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m13:28:45.390408 [debug] [Thread-4 (]: SQL status: COMMIT in 0.004 seconds
[0m13:28:45.393866 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m13:28:45.396423 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m13:28:45.396635 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m13:28:45.397182 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.000 seconds
[0m13:28:45.398436 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m13:28:45.399515 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10837c6d0>]}
[0m13:28:45.399960 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.14s]
[0m13:28:45.400376 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m13:28:45.400600 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m13:28:45.400861 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m13:28:45.401122 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m13:28:45.401298 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m13:28:45.403627 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m13:28:45.404009 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m13:28:45.406004 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m13:28:45.406390 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:45.406589 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m13:28:45.406754 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m13:28:45.427886 [debug] [Thread-4 (]: SQL status: BEGIN in 0.021 seconds
[0m13:28:45.428211 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:45.428426 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    user_id
FROM mydb.public.raw_netflix
  );
  
[0m13:28:46.585983 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 1.230 seconds
[0m13:28:46.594515 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:46.595335 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m13:28:46.598848 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:28:46.602676 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:46.602903 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m13:28:46.604232 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:46.608891 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m13:28:46.609184 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:46.609380 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m13:28:46.619126 [debug] [Thread-1 (]: SQL status: COMMIT in 0.009 seconds
[0m13:28:46.623380 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m13:28:46.625689 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m13:28:46.625918 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m13:28:46.644470 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.018 seconds
[0m13:28:46.645983 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m13:28:46.648555 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107d46c10>]}
[0m13:28:46.651514 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 1.39s]
[0m13:28:46.652062 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m13:28:46.694329 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 1.342 seconds
[0m13:28:46.696683 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:46.696911 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m13:28:46.698226 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:46.699991 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:46.700461 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m13:28:46.701269 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:46.702120 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m13:28:46.702296 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:46.702464 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m13:28:46.752421 [debug] [Thread-2 (]: SQL status: COMMIT in 0.050 seconds
[0m13:28:46.754775 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m13:28:46.755319 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m13:28:46.755542 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m13:28:46.805022 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.049 seconds
[0m13:28:46.805949 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m13:28:46.806343 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107ce3410>]}
[0m13:28:46.806696 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 1.55s]
[0m13:28:46.806985 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m13:28:47.043448 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 1.615 seconds
[0m13:28:47.050328 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:47.051101 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m13:28:47.054960 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m13:28:47.060474 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:47.061012 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m13:28:47.062788 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:47.064957 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m13:28:47.065514 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:47.066048 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m13:28:47.150601 [debug] [Thread-4 (]: SQL status: COMMIT in 0.084 seconds
[0m13:28:47.156403 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m13:28:47.157767 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m13:28:47.158205 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m13:28:47.209173 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.050 seconds
[0m13:28:47.211498 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m13:28:47.212458 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082b36d0>]}
[0m13:28:47.213386 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 1.81s]
[0m13:28:47.214182 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m13:28:48.332706 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 2.974 seconds
[0m13:28:48.346367 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:48.346661 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m13:28:48.349169 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m13:28:48.353198 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:48.353433 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m13:28:48.354162 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m13:28:48.355158 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m13:28:48.355351 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:48.355593 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m13:28:48.389213 [debug] [Thread-3 (]: SQL status: COMMIT in 0.033 seconds
[0m13:28:48.391309 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m13:28:48.391883 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m13:28:48.392074 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m13:28:48.417650 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.025 seconds
[0m13:28:48.419829 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m13:28:48.420666 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a7a31023-4049-4a1d-8925-875b233bf5d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1071d5cd0>]}
[0m13:28:48.421298 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 3.16s]
[0m13:28:48.421634 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m13:28:48.423073 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:48.423232 [debug] [MainThread]: On master: BEGIN
[0m13:28:48.423369 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m13:28:48.433794 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m13:28:48.434049 [debug] [MainThread]: On master: COMMIT
[0m13:28:48.434223 [debug] [MainThread]: Using postgres connection "master"
[0m13:28:48.434375 [debug] [MainThread]: On master: COMMIT
[0m13:28:48.434852 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m13:28:48.435534 [debug] [MainThread]: On master: Close
[0m13:28:48.436282 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:28:48.436632 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m13:28:48.436803 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m13:28:48.436937 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m13:28:48.437095 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m13:28:48.437373 [info ] [MainThread]: 
[0m13:28:48.437574 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 3.53 seconds (3.53s).
[0m13:28:48.438212 [debug] [MainThread]: Command end result
[0m13:28:48.463436 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m13:28:48.471353 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m13:28:48.478171 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m13:28:48.478372 [info ] [MainThread]: 
[0m13:28:48.478584 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:28:48.478736 [info ] [MainThread]: 
[0m13:28:48.478920 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m13:28:48.481326 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 4.556682, "process_in_blocks": "0", "process_kernel_time": 0.254102, "process_mem_max_rss": "134316032", "process_out_blocks": "0", "process_user_time": 1.742512}
[0m13:28:48.481878 [debug] [MainThread]: Command `dbt run` succeeded at 13:28:48.481807 after 4.56 seconds
[0m13:28:48.482191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b52d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1023a0ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1022f3d10>]}
[0m13:28:48.482397 [debug] [MainThread]: Flushing usage events
[0m13:28:48.999050 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:09:56.285276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104cee590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107688610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10769bd10>]}


============================== 14:09:56.288983 | a41078ad-e2d6-43fe-9e06-5dd5eb601670 ==============================
[0m14:09:56.288983 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:09:56.289455 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'static_parser': 'True', 'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'warn_error': 'None', 'no_print': 'None', 'debug': 'False', 'fail_fast': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'introspect': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run'}
[0m14:09:56.428619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106fadf90>]}
[0m14:09:56.463148 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104d0ca90>]}
[0m14:09:56.464001 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:09:56.540175 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:09:56.632478 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m14:09:56.632876 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/facts/genres.sql
[0m14:09:56.633055 [debug] [MainThread]: Partial parsing: deleted file: maker_warehouse://models/dimensions/genres.sql
[0m14:09:56.770209 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10936a010>]}
[0m14:09:56.812363 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:09:56.814476 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:09:56.863073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109148610>]}
[0m14:09:56.863384 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m14:09:56.863592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10936a350>]}
[0m14:09:56.864613 [info ] [MainThread]: 
[0m14:09:56.864844 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:09:56.865041 [info ] [MainThread]: 
[0m14:09:56.865370 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:09:56.867504 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m14:09:56.903791 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m14:09:56.904109 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m14:09:56.904264 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:57.115588 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.211 seconds
[0m14:09:57.117568 [debug] [ThreadPool]: On list_mydb: Close
[0m14:09:57.119329 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m14:09:57.125817 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:09:57.127044 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m14:09:57.127349 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:09:57.143968 [debug] [ThreadPool]: SQL status: BEGIN in 0.017 seconds
[0m14:09:57.144360 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:09:57.144998 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m14:09:57.165484 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.020 seconds
[0m14:09:57.167710 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m14:09:57.169890 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m14:09:57.175906 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:57.176487 [debug] [MainThread]: On master: BEGIN
[0m14:09:57.176985 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:09:57.238564 [debug] [MainThread]: SQL status: BEGIN in 0.061 seconds
[0m14:09:57.239072 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:57.240014 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:09:57.284966 [debug] [MainThread]: SQL status: SELECT 33 in 0.044 seconds
[0m14:09:57.289339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076d8e50>]}
[0m14:09:57.290605 [debug] [MainThread]: On master: ROLLBACK
[0m14:09:57.293224 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:57.293718 [debug] [MainThread]: On master: BEGIN
[0m14:09:57.295342 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:09:57.295707 [debug] [MainThread]: On master: COMMIT
[0m14:09:57.296395 [debug] [MainThread]: Using postgres connection "master"
[0m14:09:57.297086 [debug] [MainThread]: On master: COMMIT
[0m14:09:57.299212 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m14:09:57.300592 [debug] [MainThread]: On master: Close
[0m14:09:57.306691 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m14:09:57.307610 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m14:09:57.308116 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m14:09:57.308507 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m14:09:57.309056 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m14:09:57.309808 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m14:09:57.310530 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m14:09:57.311127 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m14:09:57.312234 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m14:09:57.313463 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m14:09:57.313923 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m14:09:57.314481 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m14:09:57.314821 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m14:09:57.315405 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m14:09:57.319792 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m14:09:57.320408 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m14:09:57.333718 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m14:09:57.337898 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m14:09:57.339789 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m14:09:57.343223 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m14:09:57.345158 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m14:09:57.371426 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m14:09:57.373239 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m14:09:57.390595 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m14:09:57.392652 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m14:09:57.393061 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m14:09:57.406022 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m14:09:57.408415 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m14:09:57.409069 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:09:57.409289 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:57.409513 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.409702 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m14:09:57.409907 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m14:09:57.410110 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:57.410297 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m14:09:57.410499 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:09:57.410791 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:09:57.411056 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m14:09:57.411422 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:09:57.411952 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:09:57.445355 [debug] [Thread-1 (]: SQL status: BEGIN in 0.034 seconds
[0m14:09:57.446250 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:57.446802 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    genres
FROM mydb.public.raw_netflix
  );
  
[0m14:09:57.448754 [debug] [Thread-4 (]: SQL status: BEGIN in 0.037 seconds
[0m14:09:57.449437 [debug] [Thread-3 (]: SQL status: BEGIN in 0.039 seconds
[0m14:09:57.451052 [debug] [Thread-2 (]: SQL status: BEGIN in 0.039 seconds
[0m14:09:57.451950 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.452594 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:09:57.452840 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:57.453080 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m14:09:57.453672 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

SELECT
    DISTINCT
    id,
    user_id,
    movie_id,
    genres,
    datetime
FROM mydb.public.raw_netflix
  );
  
[0m14:09:57.454063 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    title,
    genres,
    release_date,
    movie_id
FROM mydb.public.raw_netflix
  );
  
[0m14:09:57.469572 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.015 seconds
[0m14:09:57.482534 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.484350 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test" rename to "test__dbt_backup"
[0m14:09:57.488834 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:09:57.495627 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.496363 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m14:09:57.499621 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:09:57.518685 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:09:57.519020 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.519215 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:09:57.524062 [debug] [Thread-4 (]: SQL status: COMMIT in 0.004 seconds
[0m14:09:57.532298 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m14:09:57.542408 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:09:57.542755 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m14:09:57.550495 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.007 seconds
[0m14:09:57.554615 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m14:09:57.557882 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10950e590>]}
[0m14:09:57.559287 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.24s]
[0m14:09:57.560173 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m14:09:57.560913 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m14:09:57.561879 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m14:09:57.562511 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m14:09:57.563342 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m14:09:57.570309 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m14:09:57.571660 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m14:09:57.579886 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m14:09:57.581392 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:57.582181 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m14:09:57.582867 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:09:57.599767 [debug] [Thread-4 (]: SQL status: BEGIN in 0.017 seconds
[0m14:09:57.600474 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:57.600888 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    


SELECT
    DISTINCT
    user_id
FROM mydb.public.raw_netflix
  );
  
[0m14:09:58.023060 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.571 seconds
[0m14:09:58.036628 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:58.037009 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m14:09:58.039026 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:09:58.041210 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:58.041806 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m14:09:58.043551 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:58.048042 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:09:58.048440 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:58.048800 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:09:58.052477 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m14:09:58.054320 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m14:09:58.056772 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:09:58.057019 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m14:09:58.083407 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.026 seconds
[0m14:09:58.085128 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m14:09:58.086640 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2ac3d0>]}
[0m14:09:58.087703 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.77s]
[0m14:09:58.088430 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m14:09:58.245746 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 0.791 seconds
[0m14:09:58.249966 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:58.250381 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m14:09:58.254079 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:09:58.256743 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:58.257039 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m14:09:58.258929 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:58.260374 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:09:58.260682 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:58.260973 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:09:58.274540 [debug] [Thread-2 (]: SQL status: COMMIT in 0.013 seconds
[0m14:09:58.276870 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m14:09:58.277345 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:09:58.277529 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m14:09:58.314464 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.037 seconds
[0m14:09:58.316026 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m14:09:58.316924 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1732d0>]}
[0m14:09:58.318094 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 1.00s]
[0m14:09:58.318505 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m14:09:58.775189 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 1.173 seconds
[0m14:09:58.801043 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:58.801653 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m14:09:58.806307 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m14:09:58.810675 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:58.810922 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m14:09:58.813431 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:09:58.814882 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:09:58.815215 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:58.815532 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:09:58.863229 [debug] [Thread-4 (]: SQL status: COMMIT in 0.047 seconds
[0m14:09:58.865067 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m14:09:58.865803 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:09:58.866078 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m14:09:58.895336 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.029 seconds
[0m14:09:58.897153 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m14:09:58.898107 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a0e4d90>]}
[0m14:09:58.899186 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 1.34s]
[0m14:09:58.900438 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m14:09:59.955450 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 2.499 seconds
[0m14:09:59.963336 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:09:59.963628 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m14:09:59.967464 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m14:09:59.970528 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:09:59.970783 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m14:09:59.971957 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:09:59.973622 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:09:59.973867 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:09:59.974192 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:09:59.997183 [debug] [Thread-3 (]: SQL status: COMMIT in 0.023 seconds
[0m14:10:00.002448 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m14:10:00.003396 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:10:00.003616 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m14:10:00.043095 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.039 seconds
[0m14:10:00.045483 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m14:10:00.047178 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a41078ad-e2d6-43fe-9e06-5dd5eb601670', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2eac90>]}
[0m14:10:00.048297 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 2.73s]
[0m14:10:00.048925 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m14:10:00.051499 [debug] [MainThread]: Using postgres connection "master"
[0m14:10:00.051774 [debug] [MainThread]: On master: BEGIN
[0m14:10:00.051955 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:10:00.085023 [debug] [MainThread]: SQL status: BEGIN in 0.033 seconds
[0m14:10:00.085623 [debug] [MainThread]: On master: COMMIT
[0m14:10:00.086546 [debug] [MainThread]: Using postgres connection "master"
[0m14:10:00.086979 [debug] [MainThread]: On master: COMMIT
[0m14:10:00.088197 [debug] [MainThread]: SQL status: COMMIT in 0.001 seconds
[0m14:10:00.088547 [debug] [MainThread]: On master: Close
[0m14:10:00.089072 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:10:00.089340 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m14:10:00.089548 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m14:10:00.089734 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m14:10:00.090196 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m14:10:00.090619 [info ] [MainThread]: 
[0m14:10:00.090918 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 3.23 seconds (3.23s).
[0m14:10:00.092122 [debug] [MainThread]: Command end result
[0m14:10:00.139079 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:10:00.150236 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:10:00.197374 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m14:10:00.197655 [info ] [MainThread]: 
[0m14:10:00.197953 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:10:00.198196 [info ] [MainThread]: 
[0m14:10:00.198484 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m14:10:00.201532 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.95987, "process_in_blocks": "0", "process_kernel_time": 0.330292, "process_mem_max_rss": "131891200", "process_out_blocks": "0", "process_user_time": 1.550193}
[0m14:10:00.202205 [debug] [MainThread]: Command `dbt run` succeeded at 14:10:00.202090 after 3.96 seconds
[0m14:10:00.203380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ed8ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b0790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1073bf710>]}
[0m14:10:00.203706 [debug] [MainThread]: Flushing usage events
[0m14:10:00.813131 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:48:22.852448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107156a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107154610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107177fd0>]}


============================== 14:48:22.855544 | 420f5283-00f0-4bc2-9c55-26f83e212646 ==============================
[0m14:48:22.855544 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:48:22.855896 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'introspect': 'True', 'indirect_selection': 'eager', 'version_check': 'True', 'target_path': 'None', 'warn_error': 'None', 'empty': 'False', 'cache_selected_only': 'False', 'write_json': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'no_print': 'None', 'partial_parse': 'True', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'use_colors': 'True', 'printer_width': '80', 'invocation_command': 'dbt run', 'quiet': 'False', 'log_format': 'default'}
[0m14:48:22.974027 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1077ff9d0>]}
[0m14:48:23.003234 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047d9ad0>]}
[0m14:48:23.003916 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:48:23.063137 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:48:23.142700 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 3 files changed.
[0m14:48:23.143046 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/dimensions/genres.sql
[0m14:48:23.143210 [debug] [MainThread]: Partial parsing: deleted file: maker_warehouse://models/facts/genres.sql
[0m14:48:23.143391 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/dimensions/movies.sql
[0m14:48:23.143562 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/facts/streams.sql
[0m14:48:23.143729 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/dimensions/users.sql
[0m14:48:23.291457 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108275910>]}
[0m14:48:23.355350 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:48:23.362433 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:48:23.378488 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1080867d0>]}
[0m14:48:23.378806 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m14:48:23.378998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1083b9c50>]}
[0m14:48:23.380054 [info ] [MainThread]: 
[0m14:48:23.380235 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:48:23.380374 [info ] [MainThread]: 
[0m14:48:23.380608 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:48:23.382535 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m14:48:23.412867 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m14:48:23.413082 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m14:48:23.413225 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:48:23.614017 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.201 seconds
[0m14:48:23.615367 [debug] [ThreadPool]: On list_mydb: Close
[0m14:48:23.616881 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m14:48:23.622264 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:48:23.622595 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m14:48:23.622807 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:48:23.636868 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m14:48:23.637166 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:48:23.637422 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m14:48:23.647023 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.009 seconds
[0m14:48:23.647979 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m14:48:23.648428 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m14:48:23.652093 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:23.652311 [debug] [MainThread]: On master: BEGIN
[0m14:48:23.652478 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:48:23.669651 [debug] [MainThread]: SQL status: BEGIN in 0.017 seconds
[0m14:48:23.670249 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:23.670514 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:48:23.698939 [debug] [MainThread]: SQL status: SELECT 33 in 0.028 seconds
[0m14:48:23.700410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082676d0>]}
[0m14:48:23.700773 [debug] [MainThread]: On master: ROLLBACK
[0m14:48:23.701348 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:23.701530 [debug] [MainThread]: On master: BEGIN
[0m14:48:23.702226 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m14:48:23.702458 [debug] [MainThread]: On master: COMMIT
[0m14:48:23.702648 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:23.702820 [debug] [MainThread]: On master: COMMIT
[0m14:48:23.703232 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:48:23.703486 [debug] [MainThread]: On master: Close
[0m14:48:23.706690 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m14:48:23.706931 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m14:48:23.707349 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m14:48:23.709121 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m14:48:23.709545 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m14:48:23.710821 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m14:48:23.714700 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m14:48:23.713893 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m14:48:23.714458 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m14:48:23.711889 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m14:48:23.745669 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m14:48:23.745970 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m14:48:23.746283 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m14:48:23.746557 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m14:48:23.748401 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m14:48:23.748674 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m14:48:23.748968 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m14:48:23.749162 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m14:48:23.750910 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m14:48:23.751147 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m14:48:23.751316 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m14:48:23.774751 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m14:48:23.776620 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m14:48:23.778763 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m14:48:23.779103 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m14:48:23.782224 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m14:48:23.782486 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:23.782723 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m14:48:23.783019 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:23.783330 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m14:48:23.789825 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:23.793052 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m14:48:23.793257 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m14:48:23.793446 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:48:23.793671 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m14:48:23.793918 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:48:23.794222 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:48:23.794407 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.794765 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m14:48:23.794935 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:48:23.808121 [debug] [Thread-2 (]: SQL status: BEGIN in 0.014 seconds
[0m14:48:23.808518 [debug] [Thread-4 (]: SQL status: BEGIN in 0.014 seconds
[0m14:48:23.808820 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:23.809513 [debug] [Thread-3 (]: SQL status: BEGIN in 0.015 seconds
[0m14:48:23.809709 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.809872 [debug] [Thread-1 (]: SQL status: BEGIN in 0.016 seconds
[0m14:48:23.810093 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    movie_id,
    title,
    genres,
    release_date,
    datetime,
    row_number() over (partition by movie_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  movie_id,      
  title,
  genres,
  release_date
from base
where rn = 1
  );
  
[0m14:48:23.810307 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:23.810488 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m14:48:23.810662 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:23.810863 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  id,
  user_id,        
  movie_id,       
  genres as genre, 
  datetime
from mydb.public.raw_netflix
  );
  
[0m14:48:23.811075 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  genres as genre  
from mydb.public.raw_netflix
  );
  
[0m14:48:23.824184 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.013 seconds
[0m14:48:23.828662 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.828929 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test" rename to "test__dbt_backup"
[0m14:48:23.831847 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:48:23.838571 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.839798 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m14:48:23.843688 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:48:23.864598 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:48:23.864977 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.865188 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:48:23.867102 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m14:48:23.871739 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m14:48:23.874953 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:48:23.875182 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m14:48:23.878302 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.003 seconds
[0m14:48:23.879788 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m14:48:23.880919 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fd9650>]}
[0m14:48:23.881279 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.13s]
[0m14:48:23.881558 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m14:48:23.881753 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m14:48:23.882003 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m14:48:23.882213 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m14:48:23.882380 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m14:48:23.884123 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m14:48:23.884514 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m14:48:23.886478 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m14:48:23.886946 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:23.887138 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m14:48:23.887303 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:48:23.912448 [debug] [Thread-4 (]: SQL status: BEGIN in 0.025 seconds
[0m14:48:23.912979 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:23.913423 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    user_id,
    datetime,
    row_number() over (partition by user_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  user_id 
from base
where rn = 1
  );
  
[0m14:48:25.925928 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 2.112 seconds
[0m14:48:25.958623 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:25.959372 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m14:48:25.962328 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:48:25.964770 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:25.965013 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m14:48:25.966217 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:25.967302 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:48:25.967752 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:25.968047 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:48:25.970718 [debug] [Thread-1 (]: SQL status: COMMIT in 0.002 seconds
[0m14:48:25.973418 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m14:48:25.975837 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:48:25.976087 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m14:48:26.010781 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.034 seconds
[0m14:48:26.013428 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m14:48:26.014962 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107feee10>]}
[0m14:48:26.016074 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 2.30s]
[0m14:48:26.017055 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m14:48:26.819755 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 3.008 seconds
[0m14:48:26.825841 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:26.826103 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m14:48:26.828909 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:48:26.830724 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:26.830922 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m14:48:26.833094 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:48:26.833977 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:48:26.834188 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:26.834368 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:48:26.881306 [debug] [Thread-2 (]: SQL status: COMMIT in 0.046 seconds
[0m14:48:26.887902 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m14:48:26.893459 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:48:26.894106 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m14:48:26.950715 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.056 seconds
[0m14:48:26.953290 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m14:48:26.956979 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108644750>]}
[0m14:48:26.963125 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 3.24s]
[0m14:48:26.969545 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m14:48:27.693488 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 3.780 seconds
[0m14:48:27.692929 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 3.876 seconds
[0m14:48:27.698364 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:27.700535 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:27.700794 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m14:48:27.701052 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m14:48:27.703169 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:48:27.703350 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:48:27.705040 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:27.706701 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:27.706902 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m14:48:27.707082 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m14:48:27.707916 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:27.708102 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:48:27.709294 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:48:27.710064 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:48:27.710593 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:27.712197 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:27.712359 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:48:27.712555 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:48:27.724539 [debug] [Thread-4 (]: SQL status: COMMIT in 0.012 seconds
[0m14:48:27.724723 [debug] [Thread-3 (]: SQL status: COMMIT in 0.012 seconds
[0m14:48:27.726260 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m14:48:27.728164 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m14:48:27.728624 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:48:27.728967 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:48:27.729270 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m14:48:27.729522 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m14:48:27.735039 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.005 seconds
[0m14:48:27.735758 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m14:48:27.736197 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082d42d0>]}
[0m14:48:27.736591 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 3.85s]
[0m14:48:27.736907 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m14:48:27.747247 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.017 seconds
[0m14:48:27.748182 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m14:48:27.748575 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '420f5283-00f0-4bc2-9c55-26f83e212646', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10c8d8550>]}
[0m14:48:27.748933 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 4.00s]
[0m14:48:27.749229 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m14:48:27.751707 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:27.751944 [debug] [MainThread]: On master: BEGIN
[0m14:48:27.752090 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:48:27.763786 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m14:48:27.764135 [debug] [MainThread]: On master: COMMIT
[0m14:48:27.764357 [debug] [MainThread]: Using postgres connection "master"
[0m14:48:27.764527 [debug] [MainThread]: On master: COMMIT
[0m14:48:27.764980 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:48:27.765195 [debug] [MainThread]: On master: Close
[0m14:48:27.765483 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:48:27.765642 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m14:48:27.765786 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m14:48:27.765947 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m14:48:27.766075 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m14:48:27.766384 [info ] [MainThread]: 
[0m14:48:27.766691 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 4.39 seconds (4.39s).
[0m14:48:27.767367 [debug] [MainThread]: Command end result
[0m14:48:27.795050 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:48:27.801661 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:48:27.807588 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m14:48:27.807830 [info ] [MainThread]: 
[0m14:48:27.808105 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:48:27.808306 [info ] [MainThread]: 
[0m14:48:27.808856 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m14:48:27.819313 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 5.007045, "process_in_blocks": "0", "process_kernel_time": 0.260143, "process_mem_max_rss": "133152768", "process_out_blocks": "0", "process_user_time": 1.29441}
[0m14:48:27.820028 [debug] [MainThread]: Command `dbt run` succeeded at 14:48:27.819922 after 5.01 seconds
[0m14:48:27.820606 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10717c890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1029a4ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070eff10>]}
[0m14:48:27.820933 [debug] [MainThread]: Flushing usage events
[0m14:48:28.468222 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m14:50:24.271344 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051c8310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051efdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1051effd0>]}


============================== 14:50:24.274254 | c9238787-a9d1-4673-815f-9e5b276fd60e ==============================
[0m14:50:24.274254 [info ] [MainThread]: Running with dbt=1.10.13
[0m14:50:24.274575 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'use_colors': 'True', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'static_parser': 'True', 'fail_fast': 'False', 'introspect': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'empty': 'False', 'cache_selected_only': 'False', 'version_check': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'no_print': 'None', 'quiet': 'False', 'log_cache_events': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'debug': 'False', 'log_format': 'default', 'use_experimental_parser': 'False'}
[0m14:50:24.396416 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1058413d0>]}
[0m14:50:24.426780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028516d0>]}
[0m14:50:24.427488 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m14:50:24.486882 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m14:50:24.568235 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:50:24.568563 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/facts/streams.sql
[0m14:50:24.690778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105889910>]}
[0m14:50:24.727090 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:50:24.728646 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:50:24.777500 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060d7710>]}
[0m14:50:24.777804 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m14:50:24.777998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104bca410>]}
[0m14:50:24.778889 [info ] [MainThread]: 
[0m14:50:24.779072 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m14:50:24.779209 [info ] [MainThread]: 
[0m14:50:24.779447 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m14:50:24.781397 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m14:50:24.849599 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m14:50:24.849841 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m14:50:24.849987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:50:24.963816 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.114 seconds
[0m14:50:24.964620 [debug] [ThreadPool]: On list_mydb: Close
[0m14:50:24.965509 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m14:50:24.968945 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:50:24.969142 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m14:50:24.969289 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m14:50:24.976257 [debug] [ThreadPool]: SQL status: BEGIN in 0.007 seconds
[0m14:50:24.976473 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m14:50:24.976647 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m14:50:24.987685 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.011 seconds
[0m14:50:24.988569 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m14:50:24.988982 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m14:50:24.992079 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:24.992255 [debug] [MainThread]: On master: BEGIN
[0m14:50:24.992396 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:50:25.000424 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m14:50:25.000610 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:25.000822 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m14:50:25.017514 [debug] [MainThread]: SQL status: SELECT 33 in 0.016 seconds
[0m14:50:25.019014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065cf5d0>]}
[0m14:50:25.019348 [debug] [MainThread]: On master: ROLLBACK
[0m14:50:25.019865 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:25.020087 [debug] [MainThread]: On master: BEGIN
[0m14:50:25.020775 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m14:50:25.020989 [debug] [MainThread]: On master: COMMIT
[0m14:50:25.021171 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:25.021332 [debug] [MainThread]: On master: COMMIT
[0m14:50:25.021777 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:50:25.022054 [debug] [MainThread]: On master: Close
[0m14:50:25.024389 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m14:50:25.024628 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m14:50:25.024817 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m14:50:25.025009 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m14:50:25.025290 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m14:50:25.025642 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m14:50:25.025922 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m14:50:25.026183 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m14:50:25.026496 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m14:50:25.026771 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m14:50:25.027021 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m14:50:25.027264 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m14:50:25.027459 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m14:50:25.027644 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m14:50:25.027822 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m14:50:25.027996 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m14:50:25.034612 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m14:50:25.036378 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m14:50:25.037982 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m14:50:25.039941 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m14:50:25.041095 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m14:50:25.042140 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m14:50:25.062245 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m14:50:25.064223 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m14:50:25.064398 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m14:50:25.064551 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m14:50:25.073305 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m14:50:25.075891 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m14:50:25.076102 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.076273 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:25.076578 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m14:50:25.076778 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.076973 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m14:50:25.077170 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:25.077338 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:50:25.077502 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m14:50:25.077655 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m14:50:25.077808 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m14:50:25.078037 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m14:50:25.078258 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m14:50:25.087822 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m14:50:25.088126 [debug] [Thread-2 (]: SQL status: BEGIN in 0.010 seconds
[0m14:50:25.088468 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.088729 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:25.088951 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  genres as genre  
from mydb.public.raw_netflix
  );
  
[0m14:50:25.089235 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m14:50:25.089449 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    movie_id,
    title,
    genres,
    release_date,
    datetime,
    row_number() over (partition by movie_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  movie_id,      
  title,
  genres,
  release_date
from base
where rn = 1
  );
  
[0m14:50:25.089824 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.090138 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m14:50:25.090401 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m14:50:25.090611 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:25.090858 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  id,
  user_id,        
  movie_id,       
  genres,
  datetime
from mydb.public.raw_netflix
  );
  
[0m14:50:25.096789 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.006 seconds
[0m14:50:25.100754 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.100978 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test" rename to "test__dbt_backup"
[0m14:50:25.101953 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:25.103691 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.103915 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m14:50:25.104499 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m14:50:25.120751 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:50:25.121942 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.123354 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m14:50:25.129278 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m14:50:25.134095 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m14:50:25.137827 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m14:50:25.138463 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m14:50:25.142685 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.004 seconds
[0m14:50:25.144340 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m14:50:25.145909 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a82bc10>]}
[0m14:50:25.146379 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.12s]
[0m14:50:25.146967 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m14:50:25.147222 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m14:50:25.147490 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m14:50:25.147898 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m14:50:25.148133 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m14:50:25.150428 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m14:50:25.150915 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m14:50:25.153414 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m14:50:25.153853 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:25.154143 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m14:50:25.154537 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m14:50:25.164446 [debug] [Thread-4 (]: SQL status: BEGIN in 0.010 seconds
[0m14:50:25.165218 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:25.165863 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    user_id,
    datetime,
    row_number() over (partition by user_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  user_id 
from base
where rn = 1
  );
  
[0m14:50:25.534355 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.444 seconds
[0m14:50:25.546664 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.547391 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m14:50:25.549288 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:25.552987 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.553275 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m14:50:25.555326 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m14:50:25.556557 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:50:25.556766 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.556935 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m14:50:25.561051 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m14:50:25.565427 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m14:50:25.570301 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m14:50:25.570560 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m14:50:25.581479 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.010 seconds
[0m14:50:25.583964 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m14:50:25.584643 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8aff50>]}
[0m14:50:25.585070 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.56s]
[0m14:50:25.585375 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m14:50:26.194750 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 1.103 seconds
[0m14:50:26.208841 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:26.209370 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m14:50:26.240637 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.031 seconds
[0m14:50:26.246027 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:26.246609 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m14:50:26.247654 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:26.249955 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:50:26.250292 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:26.250477 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m14:50:26.254820 [debug] [Thread-2 (]: SQL status: COMMIT in 0.004 seconds
[0m14:50:26.258945 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m14:50:26.259395 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m14:50:26.259574 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m14:50:26.263302 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:50:26.264189 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m14:50:26.264833 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066d3bd0>]}
[0m14:50:26.265328 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 1.24s]
[0m14:50:26.265690 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m14:50:26.996572 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 1.900 seconds
[0m14:50:27.014086 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:27.014398 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m14:50:27.017452 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m14:50:27.019952 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:27.020180 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m14:50:27.021033 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:27.022117 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:50:27.022462 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:27.022732 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m14:50:27.033147 [debug] [Thread-3 (]: SQL status: COMMIT in 0.010 seconds
[0m14:50:27.034923 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m14:50:27.035362 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m14:50:27.035538 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m14:50:27.048309 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.013 seconds
[0m14:50:27.048615 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 1.882 seconds
[0m14:50:27.049630 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m14:50:27.052210 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:27.052482 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m14:50:27.055721 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106664cd0>]}
[0m14:50:27.057839 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 2.03s]
[0m14:50:27.058852 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m14:50:27.060692 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m14:50:27.062852 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:27.063039 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m14:50:27.063954 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m14:50:27.064753 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:50:27.064934 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:27.065091 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m14:50:27.067942 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m14:50:27.071483 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m14:50:27.071994 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m14:50:27.072170 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m14:50:27.076747 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.004 seconds
[0m14:50:27.077805 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m14:50:27.078248 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9238787-a9d1-4673-815f-9e5b276fd60e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a8d6590>]}
[0m14:50:27.078622 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 1.93s]
[0m14:50:27.078927 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m14:50:27.080101 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:27.080253 [debug] [MainThread]: On master: BEGIN
[0m14:50:27.080392 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m14:50:27.091679 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m14:50:27.092012 [debug] [MainThread]: On master: COMMIT
[0m14:50:27.092229 [debug] [MainThread]: Using postgres connection "master"
[0m14:50:27.092402 [debug] [MainThread]: On master: COMMIT
[0m14:50:27.092792 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m14:50:27.092997 [debug] [MainThread]: On master: Close
[0m14:50:27.093272 [debug] [MainThread]: Connection 'master' was properly closed.
[0m14:50:27.093452 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m14:50:27.093589 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m14:50:27.093711 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m14:50:27.093833 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m14:50:27.094109 [info ] [MainThread]: 
[0m14:50:27.094298 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.31 seconds (2.31s).
[0m14:50:27.094950 [debug] [MainThread]: Command end result
[0m14:50:27.128410 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m14:50:27.131269 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m14:50:27.136023 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m14:50:27.136228 [info ] [MainThread]: 
[0m14:50:27.136452 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:50:27.136617 [info ] [MainThread]: 
[0m14:50:27.136798 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m14:50:27.138820 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.905772, "process_in_blocks": "0", "process_kernel_time": 0.271631, "process_mem_max_rss": "131858432", "process_out_blocks": "0", "process_user_time": 1.286706}
[0m14:50:27.139261 [debug] [MainThread]: Command `dbt run` succeeded at 14:50:27.139174 after 2.91 seconds
[0m14:50:27.139791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100a18ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105161590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1025de5d0>]}
[0m14:50:27.140059 [debug] [MainThread]: Flushing usage events
[0m14:50:27.516723 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:02:36.779854 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114977b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149655d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149cce50>]}


============================== 15:02:36.782689 | 12a050a9-393e-4f29-8018-39c300b9770c ==============================
[0m15:02:36.782689 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:02:36.783024 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'use_experimental_parser': 'False', 'write_json': 'True', 'introspect': 'True', 'printer_width': '80', 'partial_parse': 'True', 'warn_error': 'None', 'empty': 'False', 'invocation_command': 'dbt run -m streams', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'cache_selected_only': 'False', 'static_parser': 'True', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'log_cache_events': 'False', 'no_print': 'None', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'quiet': 'False'}
[0m15:02:36.783289 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m15:02:36.783492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11460a950>]}
[0m15:02:36.903199 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149758d0>]}
[0m15:02:36.932971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110da8310>]}
[0m15:02:36.933641 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:02:36.992579 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:02:37.071669 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:02:37.071857 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:02:37.088614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114965990>]}
[0m15:02:37.126196 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:37.127620 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:37.140258 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115b561d0>]}
[0m15:02:37.140502 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m15:02:37.140684 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x115ac0cd0>]}
[0m15:02:37.141365 [info ] [MainThread]: 
[0m15:02:37.141539 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:02:37.141679 [info ] [MainThread]: 
[0m15:02:37.141928 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:02:37.142327 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m15:02:37.174322 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m15:02:37.174530 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m15:02:37.174673 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:02:37.333457 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.159 seconds
[0m15:02:37.334337 [debug] [ThreadPool]: On list_mydb: Close
[0m15:02:37.337006 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m15:02:37.340379 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:37.340571 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:02:37.340712 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:02:37.354642 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m15:02:37.354910 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:37.355116 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:02:37.363864 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.009 seconds
[0m15:02:37.364698 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:02:37.365095 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:02:37.368227 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:37.368423 [debug] [MainThread]: On master: BEGIN
[0m15:02:37.368573 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:02:37.379424 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m15:02:37.379922 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:37.380176 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:02:37.393610 [debug] [MainThread]: SQL status: SELECT 33 in 0.013 seconds
[0m15:02:37.395206 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11619d290>]}
[0m15:02:37.395585 [debug] [MainThread]: On master: ROLLBACK
[0m15:02:37.396143 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:37.396368 [debug] [MainThread]: On master: BEGIN
[0m15:02:37.397121 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m15:02:37.397321 [debug] [MainThread]: On master: COMMIT
[0m15:02:37.397497 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:37.397746 [debug] [MainThread]: On master: COMMIT
[0m15:02:37.398153 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:37.398324 [debug] [MainThread]: On master: Close
[0m15:02:37.400192 [debug] [Thread-1 (]: Began running node model.maker_warehouse.streams
[0m15:02:37.400500 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m15:02:37.400815 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.streams)
[0m15:02:37.401026 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.streams
[0m15:02:37.406133 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m15:02:37.406919 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.streams
[0m15:02:37.481314 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m15:02:37.481892 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:37.482068 [debug] [Thread-1 (]: On model.maker_warehouse.streams: BEGIN
[0m15:02:37.482220 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:02:37.489157 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m15:02:37.489417 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:37.489621 [debug] [Thread-1 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  id,
  user_id,        
  movie_id,       
  genres,
  datetime
from mydb.public.raw_netflix
  );
  
[0m15:02:39.255708 [debug] [Thread-1 (]: SQL status: SELECT 671736 in 1.765 seconds
[0m15:02:39.279364 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:39.279784 [debug] [Thread-1 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m15:02:39.287763 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.008 seconds
[0m15:02:39.290726 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:39.290980 [debug] [Thread-1 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m15:02:39.292049 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:02:39.302351 [debug] [Thread-1 (]: On model.maker_warehouse.streams: COMMIT
[0m15:02:39.302635 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:39.302828 [debug] [Thread-1 (]: On model.maker_warehouse.streams: COMMIT
[0m15:02:39.329100 [debug] [Thread-1 (]: SQL status: COMMIT in 0.026 seconds
[0m15:02:39.334819 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m15:02:39.338770 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:02:39.339021 [debug] [Thread-1 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m15:02:39.357685 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.018 seconds
[0m15:02:39.359691 [debug] [Thread-1 (]: On model.maker_warehouse.streams: Close
[0m15:02:39.362439 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '12a050a9-393e-4f29-8018-39c300b9770c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1149c0750>]}
[0m15:02:39.363029 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 1.96s]
[0m15:02:39.363396 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.streams
[0m15:02:39.364635 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:39.364821 [debug] [MainThread]: On master: BEGIN
[0m15:02:39.364975 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:02:39.390381 [debug] [MainThread]: SQL status: BEGIN in 0.025 seconds
[0m15:02:39.390900 [debug] [MainThread]: On master: COMMIT
[0m15:02:39.391148 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:39.391321 [debug] [MainThread]: On master: COMMIT
[0m15:02:39.391933 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:39.392118 [debug] [MainThread]: On master: Close
[0m15:02:39.392484 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:02:39.392669 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m15:02:39.392898 [info ] [MainThread]: 
[0m15:02:39.393134 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 2.25 seconds (2.25s).
[0m15:02:39.393746 [debug] [MainThread]: Command end result
[0m15:02:39.417016 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:39.419151 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:39.423044 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:02:39.423241 [info ] [MainThread]: 
[0m15:02:39.423490 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:02:39.423678 [info ] [MainThread]: 
[0m15:02:39.423912 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:02:39.424352 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:02:39.429012 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.6871595, "process_in_blocks": "0", "process_kernel_time": 0.279043, "process_mem_max_rss": "129040384", "process_out_blocks": "0", "process_user_time": 1.080151}
[0m15:02:39.430293 [debug] [MainThread]: Command `dbt run` succeeded at 15:02:39.430223 after 2.69 seconds
[0m15:02:39.430900 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11460a950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10498d110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x114603a50>]}
[0m15:02:39.431171 [debug] [MainThread]: Flushing usage events
[0m15:02:39.892372 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:02:44.723531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c7a1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105614e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105623f10>]}


============================== 15:02:44.726094 | 2160b469-f84f-48cf-8bb6-eeb61c55e675 ==============================
[0m15:02:44.726094 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:02:44.726428 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'no_print': 'None', 'introspect': 'True', 'empty': 'False', 'quiet': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'target_path': 'None', 'log_format': 'default', 'indirect_selection': 'eager', 'static_parser': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'debug': 'False', 'use_colors': 'True', 'invocation_command': 'dbt run -m users', 'warn_error': 'None', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'send_anonymous_usage_stats': 'True'}
[0m15:02:44.726692 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m15:02:44.726907 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105615110>]}
[0m15:02:44.818821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105664e10>]}
[0m15:02:44.849755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102c9c610>]}
[0m15:02:44.850438 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:02:44.908361 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:02:44.981080 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:02:44.981283 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:02:44.997986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065c6790>]}
[0m15:02:45.035646 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:45.036749 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:45.050767 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066a1ed0>]}
[0m15:02:45.051089 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m15:02:45.051291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1062f9050>]}
[0m15:02:45.052108 [info ] [MainThread]: 
[0m15:02:45.052309 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:02:45.052463 [info ] [MainThread]: 
[0m15:02:45.052746 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:02:45.053259 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m15:02:45.082964 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m15:02:45.083197 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m15:02:45.083348 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:02:45.121726 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.038 seconds
[0m15:02:45.122502 [debug] [ThreadPool]: On list_mydb: Close
[0m15:02:45.125045 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m15:02:45.128487 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:45.128672 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:02:45.128812 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:02:45.142979 [debug] [ThreadPool]: SQL status: BEGIN in 0.014 seconds
[0m15:02:45.143191 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:45.143376 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:02:45.152357 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.009 seconds
[0m15:02:45.153237 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:02:45.153670 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:02:45.156788 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:45.156968 [debug] [MainThread]: On master: BEGIN
[0m15:02:45.157120 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:02:45.166112 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m15:02:45.166347 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:45.166601 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:02:45.175746 [debug] [MainThread]: SQL status: SELECT 33 in 0.009 seconds
[0m15:02:45.177034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106330510>]}
[0m15:02:45.177406 [debug] [MainThread]: On master: ROLLBACK
[0m15:02:45.177808 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:45.177972 [debug] [MainThread]: On master: BEGIN
[0m15:02:45.178748 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m15:02:45.179034 [debug] [MainThread]: On master: COMMIT
[0m15:02:45.179528 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:45.179682 [debug] [MainThread]: On master: COMMIT
[0m15:02:45.180059 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:45.180277 [debug] [MainThread]: On master: Close
[0m15:02:45.182430 [debug] [Thread-1 (]: Began running node model.maker_warehouse.users
[0m15:02:45.182743 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_warehouse.users ............................... [RUN]
[0m15:02:45.183044 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.users)
[0m15:02:45.183257 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.users
[0m15:02:45.188306 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m15:02:45.189313 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.users
[0m15:02:45.245459 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m15:02:45.246028 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:45.246222 [debug] [Thread-1 (]: On model.maker_warehouse.users: BEGIN
[0m15:02:45.246384 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:02:45.253962 [debug] [Thread-1 (]: SQL status: BEGIN in 0.008 seconds
[0m15:02:45.254196 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:45.254381 [debug] [Thread-1 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    user_id,
    datetime,
    row_number() over (partition by user_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  user_id 
from base
where rn = 1
  );
  
[0m15:02:46.258515 [debug] [Thread-1 (]: SQL status: SELECT 161918 in 1.004 seconds
[0m15:02:46.277602 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:46.277940 [debug] [Thread-1 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m15:02:46.279641 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:02:46.282620 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:46.282982 [debug] [Thread-1 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m15:02:46.283799 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:02:46.298177 [debug] [Thread-1 (]: On model.maker_warehouse.users: COMMIT
[0m15:02:46.298413 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:46.298600 [debug] [Thread-1 (]: On model.maker_warehouse.users: COMMIT
[0m15:02:46.310082 [debug] [Thread-1 (]: SQL status: COMMIT in 0.011 seconds
[0m15:02:46.315548 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m15:02:46.320081 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:02:46.320374 [debug] [Thread-1 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m15:02:46.324475 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.004 seconds
[0m15:02:46.325889 [debug] [Thread-1 (]: On model.maker_warehouse.users: Close
[0m15:02:46.327216 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2160b469-f84f-48cf-8bb6-eeb61c55e675', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106a8e150>]}
[0m15:02:46.327604 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 1.14s]
[0m15:02:46.327913 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.users
[0m15:02:46.328602 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:46.328767 [debug] [MainThread]: On master: BEGIN
[0m15:02:46.328919 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:02:46.336701 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m15:02:46.336917 [debug] [MainThread]: On master: COMMIT
[0m15:02:46.337081 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:46.337227 [debug] [MainThread]: On master: COMMIT
[0m15:02:46.337649 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:46.337846 [debug] [MainThread]: On master: Close
[0m15:02:46.338094 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:02:46.338245 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m15:02:46.338415 [info ] [MainThread]: 
[0m15:02:46.338596 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 1.29 seconds (1.29s).
[0m15:02:46.338932 [debug] [MainThread]: Command end result
[0m15:02:46.358618 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:46.360216 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:46.365324 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:02:46.365537 [info ] [MainThread]: 
[0m15:02:46.365833 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:02:46.365996 [info ] [MainThread]: 
[0m15:02:46.366185 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:02:46.367198 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:02:46.370147 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.681504, "process_in_blocks": "0", "process_kernel_time": 0.17205, "process_mem_max_rss": "127041536", "process_out_blocks": "0", "process_user_time": 1.070925}
[0m15:02:46.370847 [debug] [MainThread]: Command `dbt run` succeeded at 15:02:46.370790 after 1.68 seconds
[0m15:02:46.371240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100e65110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dbc090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100dbccd0>]}
[0m15:02:46.371476 [debug] [MainThread]: Flushing usage events
[0m15:02:46.767700 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:02:50.781845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105cd3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105f3c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11020a8d0>]}


============================== 15:02:50.784582 | 891d027a-b53c-42ca-99d0-f5df79f63f33 ==============================
[0m15:02:50.784582 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:02:50.784916 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'introspect': 'True', 'printer_width': '80', 'debug': 'False', 'write_json': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'invocation_command': 'dbt run -m movies', 'use_colors': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'static_parser': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'version_check': 'True', 'log_cache_events': 'False', 'empty': 'False', 'log_format': 'default', 'cache_selected_only': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m15:02:50.785196 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m15:02:50.785402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1105cce50>]}
[0m15:02:50.872540 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c1cc50>]}
[0m15:02:50.901738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1049fd590>]}
[0m15:02:50.902489 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:02:50.962753 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:02:51.037791 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:02:51.037998 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:02:51.054915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114ac510>]}
[0m15:02:51.091752 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:51.092710 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:51.104878 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11158ef10>]}
[0m15:02:51.105128 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m15:02:51.105310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e33590>]}
[0m15:02:51.106289 [info ] [MainThread]: 
[0m15:02:51.106494 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:02:51.106644 [info ] [MainThread]: 
[0m15:02:51.106898 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:02:51.107308 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m15:02:51.135273 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m15:02:51.135516 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m15:02:51.135675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:02:51.174157 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.038 seconds
[0m15:02:51.174927 [debug] [ThreadPool]: On list_mydb: Close
[0m15:02:51.177505 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m15:02:51.180764 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:51.180946 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:02:51.181086 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:02:51.199198 [debug] [ThreadPool]: SQL status: BEGIN in 0.018 seconds
[0m15:02:51.199420 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:02:51.199607 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:02:51.208026 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.008 seconds
[0m15:02:51.208937 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:02:51.209379 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:02:51.213233 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:51.213525 [debug] [MainThread]: On master: BEGIN
[0m15:02:51.213721 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:02:51.222010 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m15:02:51.222227 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:51.222453 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:02:51.229550 [debug] [MainThread]: SQL status: SELECT 33 in 0.007 seconds
[0m15:02:51.230925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1114acf90>]}
[0m15:02:51.231251 [debug] [MainThread]: On master: ROLLBACK
[0m15:02:51.231691 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:51.231897 [debug] [MainThread]: On master: BEGIN
[0m15:02:51.232565 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m15:02:51.232751 [debug] [MainThread]: On master: COMMIT
[0m15:02:51.232926 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:51.233080 [debug] [MainThread]: On master: COMMIT
[0m15:02:51.233503 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:51.233672 [debug] [MainThread]: On master: Close
[0m15:02:51.235527 [debug] [Thread-1 (]: Began running node model.maker_warehouse.movies
[0m15:02:51.235838 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m15:02:51.236160 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.movies)
[0m15:02:51.236387 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.movies
[0m15:02:51.241790 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m15:02:51.242224 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.movies
[0m15:02:51.292695 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m15:02:51.293274 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.293462 [debug] [Thread-1 (]: On model.maker_warehouse.movies: BEGIN
[0m15:02:51.293625 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:02:51.300556 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m15:02:51.300804 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.300999 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    movie_id,
    title,
    genres,
    release_date,
    datetime,
    row_number() over (partition by movie_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  movie_id,      
  title,
  genres,
  release_date
from base
where rn = 1
  );
  
[0m15:02:51.947811 [debug] [Thread-1 (]: SQL status: SELECT 8472 in 0.646 seconds
[0m15:02:51.968027 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.968337 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m15:02:51.969301 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:02:51.971522 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.971835 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m15:02:51.972571 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m15:02:51.982841 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m15:02:51.983053 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.983429 [debug] [Thread-1 (]: On model.maker_warehouse.movies: COMMIT
[0m15:02:51.984467 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m15:02:51.989687 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m15:02:51.992417 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:02:51.992785 [debug] [Thread-1 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m15:02:51.996178 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m15:02:51.997631 [debug] [Thread-1 (]: On model.maker_warehouse.movies: Close
[0m15:02:51.998766 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '891d027a-b53c-42ca-99d0-f5df79f63f33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110c3c190>]}
[0m15:02:51.999110 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 0.76s]
[0m15:02:51.999408 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.movies
[0m15:02:52.000029 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:52.000201 [debug] [MainThread]: On master: BEGIN
[0m15:02:52.000346 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:02:52.007301 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m15:02:52.007501 [debug] [MainThread]: On master: COMMIT
[0m15:02:52.007654 [debug] [MainThread]: Using postgres connection "master"
[0m15:02:52.007798 [debug] [MainThread]: On master: COMMIT
[0m15:02:52.008164 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:02:52.008393 [debug] [MainThread]: On master: Close
[0m15:02:52.008626 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:02:52.008789 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m15:02:52.008968 [info ] [MainThread]: 
[0m15:02:52.009142 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.90 seconds (0.90s).
[0m15:02:52.009480 [debug] [MainThread]: Command end result
[0m15:02:52.030619 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:02:52.032964 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:02:52.036127 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:02:52.036296 [info ] [MainThread]: 
[0m15:02:52.036510 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:02:52.036658 [info ] [MainThread]: 
[0m15:02:52.036821 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:02:52.037099 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:02:52.038106 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 1.2907182, "process_in_blocks": "0", "process_kernel_time": 0.167746, "process_mem_max_rss": "131366912", "process_out_blocks": "0", "process_user_time": 1.068808}
[0m15:02:52.038318 [debug] [MainThread]: Command `dbt run` succeeded at 15:02:52.038280 after 1.29 seconds
[0m15:02:52.038508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x100651110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11012de10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1005a8cd0>]}
[0m15:02:52.038678 [debug] [MainThread]: Flushing usage events
[0m15:02:52.468155 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:03:14.319447 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d19b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d27f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d44710>]}


============================== 15:03:14.322256 | 296d3405-b5c2-4963-b265-95d88d6d1d88 ==============================
[0m15:03:14.322256 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:03:14.322595 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'fail_fast': 'False', 'partial_parse': 'True', 'target_path': 'None', 'log_format': 'default', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'version_check': 'True', 'use_experimental_parser': 'False', 'introspect': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'invocation_command': 'dbt run -m genres', 'no_print': 'None', 'warn_error': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'debug': 'False', 'write_json': 'True'}
[0m15:03:14.322857 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
Usage of `--models`, `--model`, and `-m` is deprecated in favor of `--select` or
`-s`.
[0m15:03:14.323061 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108d1a250>]}
[0m15:03:14.436955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109347e90>]}
[0m15:03:14.465814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063a1990>]}
[0m15:03:14.466418 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:03:14.525063 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:03:14.605272 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:03:14.605459 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:03:14.621982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c83850>]}
[0m15:03:14.658775 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:03:14.659849 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:03:14.672174 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109da6550>]}
[0m15:03:14.672419 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m15:03:14.672598 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109b89c50>]}
[0m15:03:14.673274 [info ] [MainThread]: 
[0m15:03:14.673447 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:03:14.673583 [info ] [MainThread]: 
[0m15:03:14.673820 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:03:14.674223 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m15:03:14.706267 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m15:03:14.706473 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m15:03:14.706610 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:03:14.766813 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.060 seconds
[0m15:03:14.767557 [debug] [ThreadPool]: On list_mydb: Close
[0m15:03:14.770104 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m15:03:14.773267 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:03:14.773436 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:03:14.773570 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:03:14.793514 [debug] [ThreadPool]: SQL status: BEGIN in 0.020 seconds
[0m15:03:14.793737 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:03:14.793922 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:03:14.800305 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.006 seconds
[0m15:03:14.801109 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:03:14.801552 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:03:14.804625 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:14.804808 [debug] [MainThread]: On master: BEGIN
[0m15:03:14.804955 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:03:14.814510 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m15:03:14.814753 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:14.814978 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:03:14.823068 [debug] [MainThread]: SQL status: SELECT 33 in 0.008 seconds
[0m15:03:14.824366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109417810>]}
[0m15:03:14.824679 [debug] [MainThread]: On master: ROLLBACK
[0m15:03:14.825126 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:14.825341 [debug] [MainThread]: On master: BEGIN
[0m15:03:14.825918 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m15:03:14.826101 [debug] [MainThread]: On master: COMMIT
[0m15:03:14.826268 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:14.826452 [debug] [MainThread]: On master: COMMIT
[0m15:03:14.826952 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:03:14.827129 [debug] [MainThread]: On master: Close
[0m15:03:14.829111 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m15:03:14.829402 [info ] [Thread-1 (]: 1 of 1 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m15:03:14.829705 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m15:03:14.829917 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m15:03:14.834870 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m15:03:14.835448 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m15:03:14.885584 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m15:03:14.886145 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:14.886325 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m15:03:14.886479 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:03:14.898414 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m15:03:14.898673 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:14.898870 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  genres as genre  
from mydb.public.raw_netflix
  );
  
[0m15:03:15.115570 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.216 seconds
[0m15:03:15.132068 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:15.132776 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m15:03:15.134013 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:03:15.136919 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:15.137418 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m15:03:15.138288 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m15:03:15.152094 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m15:03:15.152477 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:15.152797 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m15:03:15.154146 [debug] [Thread-1 (]: SQL status: COMMIT in 0.001 seconds
[0m15:03:15.159183 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m15:03:15.162742 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:03:15.163080 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m15:03:15.165936 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.003 seconds
[0m15:03:15.167404 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m15:03:15.168685 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '296d3405-b5c2-4963-b265-95d88d6d1d88', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a15bcd0>]}
[0m15:03:15.169057 [info ] [Thread-1 (]: 1 of 1 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.34s]
[0m15:03:15.169373 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m15:03:15.170077 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:15.170260 [debug] [MainThread]: On master: BEGIN
[0m15:03:15.170422 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:03:15.178378 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m15:03:15.178654 [debug] [MainThread]: On master: COMMIT
[0m15:03:15.178842 [debug] [MainThread]: Using postgres connection "master"
[0m15:03:15.179011 [debug] [MainThread]: On master: COMMIT
[0m15:03:15.179380 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:03:15.179574 [debug] [MainThread]: On master: Close
[0m15:03:15.179811 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:03:15.179968 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m15:03:15.180144 [info ] [MainThread]: 
[0m15:03:15.180362 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 0.51 seconds (0.51s).
[0m15:03:15.180739 [debug] [MainThread]: Command end result
[0m15:03:15.198350 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:03:15.200017 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:03:15.203555 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:03:15.203753 [info ] [MainThread]: 
[0m15:03:15.203993 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:03:15.204165 [info ] [MainThread]: 
[0m15:03:15.204354 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m15:03:15.204674 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- ModelParamUsageDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:03:15.206550 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 0.92572343, "process_in_blocks": "0", "process_kernel_time": 0.181127, "process_mem_max_rss": "129187840", "process_out_blocks": "0", "process_user_time": 1.053844}
[0m15:03:15.206829 [debug] [MainThread]: Command `dbt run` succeeded at 15:03:15.206781 after 0.93 seconds
[0m15:03:15.207044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104569110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108a56a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1044c0cd0>]}
[0m15:03:15.207248 [debug] [MainThread]: Flushing usage events
[0m15:03:15.649491 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:05:34.286712 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066ce410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109058e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10907bfd0>]}


============================== 15:05:34.289543 | bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881 ==============================
[0m15:05:34.289543 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:05:34.289884 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'no_print': 'None', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'debug': 'False', 'indirect_selection': 'eager', 'printer_width': '80', 'target_path': 'None', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'version_check': 'True', 'partial_parse': 'True', 'warn_error': 'None', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'static_parser': 'True', 'cache_selected_only': 'False', 'log_format': 'default', 'fail_fast': 'False', 'use_colors': 'True', 'empty': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'quiet': 'False'}
[0m15:05:34.408553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10970c510>]}
[0m15:05:34.438614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066e1610>]}
[0m15:05:34.439294 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:05:34.498768 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:05:34.581519 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:05:34.581800 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:05:34.600360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10904fed0>]}
[0m15:05:34.638319 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:05:34.639723 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:05:34.652752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e4f1d0>]}
[0m15:05:34.653008 [info ] [MainThread]: Found 5 models, 1 source, 567 macros
[0m15:05:34.653186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109eb9d50>]}
[0m15:05:34.654076 [info ] [MainThread]: 
[0m15:05:34.654255 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:05:34.654391 [info ] [MainThread]: 
[0m15:05:34.654644 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:05:34.656633 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m15:05:34.688110 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m15:05:34.688318 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m15:05:34.688461 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:34.817712 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.129 seconds
[0m15:05:34.818557 [debug] [ThreadPool]: On list_mydb: Close
[0m15:05:34.819432 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m15:05:34.823034 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:05:34.823250 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:05:34.823406 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m15:05:34.833377 [debug] [ThreadPool]: SQL status: BEGIN in 0.010 seconds
[0m15:05:34.833619 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:05:34.833813 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:05:34.843959 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.010 seconds
[0m15:05:34.844733 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:05:34.845202 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:05:34.848441 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:34.848626 [debug] [MainThread]: On master: BEGIN
[0m15:05:34.848770 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:05:34.864124 [debug] [MainThread]: SQL status: BEGIN in 0.015 seconds
[0m15:05:34.864377 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:34.864586 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:05:34.878071 [debug] [MainThread]: SQL status: SELECT 32 in 0.013 seconds
[0m15:05:34.879997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a514110>]}
[0m15:05:34.880449 [debug] [MainThread]: On master: ROLLBACK
[0m15:05:34.881756 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:34.881980 [debug] [MainThread]: On master: BEGIN
[0m15:05:34.883077 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m15:05:34.883300 [debug] [MainThread]: On master: COMMIT
[0m15:05:34.883487 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:34.883735 [debug] [MainThread]: On master: COMMIT
[0m15:05:34.884146 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:05:34.884320 [debug] [MainThread]: On master: Close
[0m15:05:34.887373 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m15:05:34.888073 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m15:05:34.888651 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m15:05:34.889368 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m15:05:34.889845 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m15:05:34.890916 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m15:05:34.891364 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m15:05:34.891781 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m15:05:34.892101 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m15:05:34.892364 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m15:05:34.893572 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m15:05:34.893834 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m15:05:34.894059 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m15:05:34.894262 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m15:05:34.894439 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m15:05:34.894609 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m15:05:34.903193 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m15:05:34.943842 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m15:05:34.946185 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m15:05:34.947954 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m15:05:34.948730 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m15:05:34.949009 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m15:05:34.955514 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m15:05:34.961690 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m15:05:34.968705 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m15:05:34.970768 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m15:05:34.973927 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m15:05:34.984247 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m15:05:34.985134 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:34.985352 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:34.985543 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:34.985746 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:34.985929 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m15:05:34.986114 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m15:05:34.986283 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m15:05:34.986449 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m15:05:34.986611 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:05:34.986771 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:05:34.986925 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:05:34.987079 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:05:34.997272 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m15:05:34.997731 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m15:05:34.998042 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:34.998261 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:34.998478 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m15:05:34.998755 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m15:05:34.998956 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  genres as genre  
from mydb.public.raw_netflix
  );
  
[0m15:05:34.999158 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m15:05:34.999329 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:34.999491 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:34.999727 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  id,
  user_id,        
  movie_id,       
  genres,
  datetime
from mydb.public.raw_netflix
  );
  
[0m15:05:34.999923 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    movie_id,
    title,
    genres,
    release_date,
    datetime,
    row_number() over (partition by movie_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  movie_id,      
  title,
  genres,
  release_date
from base
where rn = 1
  );
  
[0m15:05:35.004314 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.005 seconds
[0m15:05:35.008585 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:35.008801 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test" rename to "test__dbt_backup"
[0m15:05:35.009770 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:05:35.011821 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:35.012101 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m15:05:35.013202 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:05:35.033087 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m15:05:35.036191 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:35.036775 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m15:05:35.041914 [debug] [Thread-4 (]: SQL status: COMMIT in 0.005 seconds
[0m15:05:35.049794 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m15:05:35.053129 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m15:05:35.053378 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m15:05:35.056938 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.003 seconds
[0m15:05:35.060150 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m15:05:35.063919 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109e9b210>]}
[0m15:05:35.064438 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.17s]
[0m15:05:35.064855 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m15:05:35.065117 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m15:05:35.065484 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m15:05:35.065857 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m15:05:35.066060 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m15:05:35.068208 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m15:05:35.068832 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m15:05:35.075766 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m15:05:35.077587 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:35.077936 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m15:05:35.078294 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:05:35.094527 [debug] [Thread-4 (]: SQL status: BEGIN in 0.016 seconds
[0m15:05:35.095199 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:35.095682 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    user_id,
    datetime,
    row_number() over (partition by user_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  user_id 
from base
where rn = 1
  );
  
[0m15:05:35.513262 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 0.513 seconds
[0m15:05:35.527290 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:35.527647 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m15:05:35.531179 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m15:05:35.534557 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:35.534790 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m15:05:35.536670 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m15:05:35.538533 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m15:05:35.538780 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:35.539152 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m15:05:35.542920 [debug] [Thread-1 (]: SQL status: COMMIT in 0.003 seconds
[0m15:05:35.544961 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m15:05:35.549123 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m15:05:35.549364 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m15:05:35.554364 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.005 seconds
[0m15:05:35.555121 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m15:05:35.555570 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a607c10>]}
[0m15:05:35.555980 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 0.66s]
[0m15:05:35.556603 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m15:05:36.211593 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 1.209 seconds
[0m15:05:36.226510 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:36.227046 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m15:05:36.232132 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m15:05:36.237528 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:36.238541 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m15:05:36.242686 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.003 seconds
[0m15:05:36.243871 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m15:05:36.244097 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:36.244277 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m15:05:36.255143 [debug] [Thread-2 (]: SQL status: COMMIT in 0.010 seconds
[0m15:05:36.257560 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m15:05:36.258024 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m15:05:36.258214 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m15:05:36.266144 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.008 seconds
[0m15:05:36.267162 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m15:05:36.267983 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a4dd9d0>]}
[0m15:05:36.268686 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 1.38s]
[0m15:05:36.269065 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m15:05:36.634619 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 1.634 seconds
[0m15:05:36.637517 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:36.638158 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m15:05:36.643433 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.005 seconds
[0m15:05:36.645548 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:36.645776 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m15:05:36.646626 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:05:36.647681 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m15:05:36.647883 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:36.648054 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m15:05:36.671960 [debug] [Thread-3 (]: SQL status: COMMIT in 0.024 seconds
[0m15:05:36.675496 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m15:05:36.675914 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m15:05:36.676089 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m15:05:36.685721 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.009 seconds
[0m15:05:36.686482 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m15:05:36.686870 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a48f9d0>]}
[0m15:05:36.687222 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 1.79s]
[0m15:05:36.687519 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m15:05:36.888074 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 1.792 seconds
[0m15:05:36.891442 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:36.891688 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m15:05:36.892901 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:05:36.895117 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:36.895551 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m15:05:36.896869 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m15:05:36.898134 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m15:05:36.898413 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:36.898594 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m15:05:36.946962 [debug] [Thread-4 (]: SQL status: COMMIT in 0.048 seconds
[0m15:05:36.949345 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m15:05:36.949951 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m15:05:36.950234 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m15:05:36.953606 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.003 seconds
[0m15:05:36.954926 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m15:05:36.955284 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bdec8a60-f1dc-4ed6-8fe5-ffaddfd2b881', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5011d0>]}
[0m15:05:36.955644 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 1.89s]
[0m15:05:36.955917 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m15:05:36.957317 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:36.957526 [debug] [MainThread]: On master: BEGIN
[0m15:05:36.957684 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:05:36.968473 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m15:05:36.968739 [debug] [MainThread]: On master: COMMIT
[0m15:05:36.968926 [debug] [MainThread]: Using postgres connection "master"
[0m15:05:36.969091 [debug] [MainThread]: On master: COMMIT
[0m15:05:36.969475 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:05:36.969703 [debug] [MainThread]: On master: Close
[0m15:05:36.969984 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:05:36.970133 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m15:05:36.970266 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m15:05:36.970391 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m15:05:36.970515 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m15:05:36.970796 [info ] [MainThread]: 
[0m15:05:36.971001 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.32 seconds (2.32s).
[0m15:05:36.971705 [debug] [MainThread]: Command end result
[0m15:05:36.988547 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:05:36.993778 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:05:37.001099 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:05:37.001343 [info ] [MainThread]: 
[0m15:05:37.001574 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:05:37.001742 [info ] [MainThread]: 
[0m15:05:37.001943 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m15:05:37.003936 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 2.758035, "process_in_blocks": "0", "process_kernel_time": 0.2708, "process_mem_max_rss": "130170880", "process_out_blocks": "0", "process_user_time": 1.231156}
[0m15:05:37.004252 [debug] [MainThread]: Command `dbt run` succeeded at 15:05:37.004198 after 2.76 seconds
[0m15:05:37.004575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048a8ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108351150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1047fbd10>]}
[0m15:05:37.004806 [debug] [MainThread]: Flushing usage events
[0m15:05:37.415234 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:39:57.623420 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x118392e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bddbd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bdf91d0>]}


============================== 15:39:57.626256 | cf96d6bc-0053-440f-a053-ce2320b7f7d3 ==============================
[0m15:39:57.626256 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:39:57.626640 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'partial_parse': 'True', 'version_check': 'True', 'empty': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'target_path': 'None', 'use_experimental_parser': 'False', 'log_format': 'default', 'introspect': 'True', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'quiet': 'False', 'no_print': 'None', 'printer_width': '80', 'indirect_selection': 'eager', 'use_colors': 'True', 'debug': 'False', 'cache_selected_only': 'False', 'invocation_command': 'dbt test'}
[0m15:39:57.741901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'cf96d6bc-0053-440f-a053-ce2320b7f7d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11adf1e10>]}
[0m15:39:57.770029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'cf96d6bc-0053-440f-a053-ce2320b7f7d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1183c9450>]}
[0m15:39:57.770676 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:39:57.829313 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:39:57.909615 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:39:57.909928 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/schema.yaml
[0m15:39:57.956929 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'movie_data' in the 'models' section of file 'models/schema.yaml'
[0m15:39:58.078509 [error] [MainThread]: Encountered an error:
Parsing Error
  Invalid test config given in models/schema.yaml:
  	test definition dictionary must have exactly one key, got [('test', 'date_spine'), ('from', "ref('start_date')"), ('to', "ref('end_date')")] instead (3 keys)
  	@: UnparsedModelUpdate(original_file_path='mode...ne)
[0m15:39:58.080512 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.4987996, "process_in_blocks": "0", "process_kernel_time": 0.168977, "process_mem_max_rss": "121585664", "process_out_blocks": "0", "process_user_time": 0.919946}
[0m15:39:58.080813 [debug] [MainThread]: Command `dbt test` failed at 15:39:58.080761 after 0.50 seconds
[0m15:39:58.081026 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105090ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11bd77350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x11d16b450>]}
[0m15:39:58.081229 [debug] [MainThread]: Flushing usage events
[0m15:39:58.631869 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:43:58.072646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107588b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075fff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075fffd0>]}


============================== 15:43:58.075397 | 5a89e681-ed42-4827-8990-75a07000da7d ==============================
[0m15:43:58.075397 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:43:58.075724 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'partial_parse': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'warn_error': 'None', 'debug': 'False', 'target_path': 'None', 'write_json': 'True', 'printer_width': '80', 'use_colors': 'True', 'fail_fast': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'cache_selected_only': 'False', 'no_print': 'None', 'introspect': 'True', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt test --select streams', 'log_cache_events': 'False', 'log_format': 'default', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects'}
[0m15:43:58.191702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10762f110>]}
[0m15:43:58.221083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104c696d0>]}
[0m15:43:58.221699 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:43:58.279922 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:43:58.360640 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m15:43:58.360985 [debug] [MainThread]: Partial parsing: added file: maker_warehouse://models/schema.yaml
[0m15:43:58.445375 [warn ] [MainThread]: [[33mWARNING[0m][MissingArgumentsPropertyInGenericTestDeprecation]: Deprecated
functionality
Found top-level arguments to test `relationships`. Arguments to generic tests
should be nested under the `arguments` property.`
[0m15:43:58.445652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108702050>]}
[0m15:43:58.496697 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107cd3ad0>]}
[0m15:43:58.569410 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:43:58.572631 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:43:58.591921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10878a810>]}
[0m15:43:58.592183 [info ] [MainThread]: Found 5 models, 8 data tests, 1 source, 567 macros
[0m15:43:58.592364 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5a89e681-ed42-4827-8990-75a07000da7d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1084310d0>]}
[0m15:43:58.593026 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:43:58.593292 [debug] [MainThread]: Command end result
[0m15:43:58.607092 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:43:58.608097 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:43:58.609835 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:43:58.610092 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingArgumentsPropertyInGenericTestDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m15:43:58.611537 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.5774508, "process_in_blocks": "0", "process_kernel_time": 0.165429, "process_mem_max_rss": "124354560", "process_out_blocks": "0", "process_user_time": 1.032713}
[0m15:43:58.611757 [debug] [MainThread]: Command `dbt test` succeeded at 15:43:58.611716 after 0.58 seconds
[0m15:43:58.612019 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102e31110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10731ef50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102d88a90>]}
[0m15:43:58.612290 [debug] [MainThread]: Flushing usage events
[0m15:43:59.066348 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:44:27.166319 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065cf6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f8a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065f91d0>]}


============================== 15:44:27.169406 | e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4 ==============================
[0m15:44:27.169406 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:44:27.169745 [debug] [MainThread]: running dbt with arguments {'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'log_format': 'default', 'empty': 'None', 'invocation_command': 'dbt test --select streams', 'log_cache_events': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'write_json': 'True', 'partial_parse': 'True', 'no_print': 'None', 'debug': 'False', 'version_check': 'True', 'target_path': 'None', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'fail_fast': 'False', 'introspect': 'True', 'cache_selected_only': 'False', 'use_colors': 'True', 'quiet': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'indirect_selection': 'eager', 'use_experimental_parser': 'False'}
[0m15:44:27.273015 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065cfd90>]}
[0m15:44:27.301669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x102ac8450>]}
[0m15:44:27.302158 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:44:27.360344 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:44:27.445192 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:44:27.445598 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/schema.yaml
[0m15:44:27.671634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10770b090>]}
[0m15:44:27.715242 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:44:27.716356 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:44:27.732627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10837b810>]}
[0m15:44:27.732876 [info ] [MainThread]: Found 5 models, 7 data tests, 1 source, 567 macros
[0m15:44:27.733056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e8dab6c1-bc1e-42f2-ac3a-7566179ae5f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107640ad0>]}
[0m15:44:27.733715 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m15:44:27.733990 [debug] [MainThread]: Command end result
[0m15:44:27.747528 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:44:27.748387 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:44:27.750297 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:44:27.751873 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 0.62158376, "process_in_blocks": "0", "process_kernel_time": 0.153109, "process_mem_max_rss": "122716160", "process_out_blocks": "0", "process_user_time": 1.108984}
[0m15:44:27.752100 [debug] [MainThread]: Command `dbt test` succeeded at 15:44:27.752059 after 0.62 seconds
[0m15:44:27.752280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10095d110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1065faf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10620b050>]}
[0m15:44:27.752449 [debug] [MainThread]: Flushing usage events
[0m15:44:28.182730 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:51:08.471282 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075cf7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f3c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075f3f90>]}


============================== 15:51:08.474025 | e4db9457-11cb-4b91-b447-58e9d736eae6 ==============================
[0m15:51:08.474025 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:51:08.474360 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False', 'invocation_command': 'dbt test --select movies', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'printer_width': '80', 'fail_fast': 'False', 'introspect': 'True', 'no_print': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'quiet': 'False', 'partial_parse': 'True', 'target_path': 'None', 'empty': 'None', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'static_parser': 'True', 'use_colors': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'warn_error': 'None'}
[0m15:51:08.591820 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1063ff690>]}
[0m15:51:08.620371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1043cc610>]}
[0m15:51:08.620978 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:51:08.679193 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:51:08.768048 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:51:08.768363 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/schema.yaml
[0m15:51:08.817827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f4e850>]}
[0m15:51:08.862046 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:51:08.863289 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:51:08.906789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076c4250>]}
[0m15:51:08.907079 [info ] [MainThread]: Found 5 models, 7 data tests, 1 source, 567 macros
[0m15:51:08.907269 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110e4a610>]}
[0m15:51:08.908133 [info ] [MainThread]: 
[0m15:51:08.908318 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:51:08.908461 [info ] [MainThread]: 
[0m15:51:08.908705 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:51:08.910800 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m15:51:08.942922 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:51:08.943608 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:51:08.943854 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:51:09.138977 [debug] [ThreadPool]: SQL status: BEGIN in 0.195 seconds
[0m15:51:09.139319 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:51:09.139558 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:51:09.158763 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.019 seconds
[0m15:51:09.163742 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:51:09.165461 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:51:09.172118 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.172402 [debug] [MainThread]: On master: BEGIN
[0m15:51:09.172588 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:51:09.183055 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m15:51:09.183397 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.183872 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:51:09.196589 [debug] [MainThread]: SQL status: SELECT 32 in 0.012 seconds
[0m15:51:09.198550 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e4db9457-11cb-4b91-b447-58e9d736eae6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x110f2ee10>]}
[0m15:51:09.200486 [debug] [MainThread]: On master: ROLLBACK
[0m15:51:09.201924 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.202443 [debug] [MainThread]: On master: BEGIN
[0m15:51:09.205093 [debug] [MainThread]: SQL status: BEGIN in 0.002 seconds
[0m15:51:09.205397 [debug] [MainThread]: On master: COMMIT
[0m15:51:09.205801 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.206476 [debug] [MainThread]: On master: COMMIT
[0m15:51:09.208335 [debug] [MainThread]: SQL status: COMMIT in 0.002 seconds
[0m15:51:09.208602 [debug] [MainThread]: On master: Close
[0m15:51:09.213252 [debug] [Thread-1 (]: Began running node test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781
[0m15:51:09.213470 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_movies_datetime.f46a9edf1c
[0m15:51:09.213666 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_movies_id.a4729c6b05
[0m15:51:09.213846 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_movies_movie_id.8dc9aab371
[0m15:51:09.214031 [info ] [Thread-1 (]: 1 of 7 START test has_at_least_one_row_movies_ ................................. [RUN]
[0m15:51:09.214263 [info ] [Thread-2 (]: 2 of 7 START test not_null_movies_datetime ..................................... [RUN]
[0m15:51:09.214496 [info ] [Thread-3 (]: 3 of 7 START test not_null_movies_id ........................................... [RUN]
[0m15:51:09.214708 [info ] [Thread-4 (]: 4 of 7 START test not_null_movies_movie_id ..................................... [RUN]
[0m15:51:09.214966 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781)
[0m15:51:09.215224 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_movies_datetime.f46a9edf1c'
[0m15:51:09.215458 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_movies_id.a4729c6b05'
[0m15:51:09.215685 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_movies_movie_id.8dc9aab371'
[0m15:51:09.215866 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781
[0m15:51:09.216034 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_movies_datetime.f46a9edf1c
[0m15:51:09.216201 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_movies_id.a4729c6b05
[0m15:51:09.216362 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_movies_movie_id.8dc9aab371
[0m15:51:09.242784 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_movies_datetime.f46a9edf1c"
[0m15:51:09.265651 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_movies_id.a4729c6b05"
[0m15:51:09.268129 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_movies_movie_id.8dc9aab371"
[0m15:51:09.268874 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_movies_datetime.f46a9edf1c
[0m15:51:09.269070 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_movies_id.a4729c6b05
[0m15:51:09.269267 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_movies_movie_id.8dc9aab371
[0m15:51:09.278192 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_movies_datetime.f46a9edf1c"
[0m15:51:09.279711 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_movies_id.a4729c6b05"
[0m15:51:09.281161 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_movies_movie_id.8dc9aab371"
[0m15:51:09.281710 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_movies_datetime.f46a9edf1c"
[0m15:51:09.281956 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_movies_id.a4729c6b05"
[0m15:51:09.282178 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_movies_datetime.f46a9edf1c: BEGIN
[0m15:51:09.282363 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_movies_movie_id.8dc9aab371"
[0m15:51:09.282602 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_movies_id.a4729c6b05: BEGIN
[0m15:51:09.282789 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:51:09.282988 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_movies_movie_id.8dc9aab371: BEGIN
[0m15:51:09.283165 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:51:09.283568 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:51:09.285324 [debug] [Thread-1 (]: Compilation Error in test has_at_least_one_row_movies_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:51:09.285634 [error] [Thread-1 (]: 1 of 7 ERROR has_at_least_one_row_movies_ ...................................... [[31mERROR[0m in 0.07s]
[0m15:51:09.286057 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781
[0m15:51:09.286319 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_movies_user_id.2431798528
[0m15:51:09.286580 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781' to be skipped because of status 'error'.  Reason: Compilation Error in test has_at_least_one_row_movies_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:51:09.286787 [info ] [Thread-1 (]: 5 of 7 START test not_null_movies_user_id ...................................... [RUN]
[0m15:51:09.287427 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.has_at_least_one_row_movies_.0cb322c781, now test.maker_warehouse.not_null_movies_user_id.2431798528)
[0m15:51:09.287629 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_movies_user_id.2431798528
[0m15:51:09.292481 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_movies_user_id.2431798528"
[0m15:51:09.293006 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_movies_user_id.2431798528
[0m15:51:09.294899 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_movies_user_id.2431798528"
[0m15:51:09.295617 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_movies_user_id.2431798528"
[0m15:51:09.296436 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_movies_user_id.2431798528: BEGIN
[0m15:51:09.296733 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:51:09.298088 [debug] [Thread-4 (]: SQL status: BEGIN in 0.014 seconds
[0m15:51:09.298813 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_movies_movie_id.8dc9aab371"
[0m15:51:09.299484 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_movies_movie_id.8dc9aab371: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_movies_movie_id.8dc9aab371"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."movies"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m15:51:09.299951 [debug] [Thread-2 (]: SQL status: BEGIN in 0.017 seconds
[0m15:51:09.300331 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_movies_datetime.f46a9edf1c"
[0m15:51:09.300529 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_movies_datetime.f46a9edf1c: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_movies_datetime.f46a9edf1c"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."movies"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m15:51:09.304294 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "datetime" does not exist
LINE 16: select datetime
                ^

[0m15:51:09.304764 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_movies_datetime.f46a9edf1c: ROLLBACK
[0m15:51:09.305524 [debug] [Thread-3 (]: SQL status: BEGIN in 0.022 seconds
[0m15:51:09.305984 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_movies_datetime.f46a9edf1c: Close
[0m15:51:09.306187 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_movies_id.a4729c6b05"
[0m15:51:09.307289 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_movies_id.a4729c6b05: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_movies_id.a4729c6b05"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."movies"
where id is null



  
  
      
    ) dbt_internal_test
[0m15:51:09.308283 [debug] [Thread-3 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 16: select id
                ^

[0m15:51:09.308561 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_movies_id.a4729c6b05: ROLLBACK
[0m15:51:09.309462 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_movies_id.a4729c6b05: Close
[0m15:51:09.311078 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m15:51:09.311366 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_movies_user_id.2431798528"
[0m15:51:09.311667 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_movies_user_id.2431798528: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_movies_user_id.2431798528"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."movies"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:51:09.313496 [debug] [Thread-2 (]: Database Error in test not_null_movies_datetime (models/schema.yaml)
  column "datetime" does not exist
  LINE 16: select datetime
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_datetime.sql
[0m15:51:09.313769 [debug] [Thread-1 (]: Postgres adapter: Postgres error: column "user_id" does not exist
LINE 16: select user_id
                ^

[0m15:51:09.314203 [error] [Thread-2 (]: 2 of 7 ERROR not_null_movies_datetime .......................................... [[31mERROR[0m in 0.10s]
[0m15:51:09.314820 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_movies_user_id.2431798528: ROLLBACK
[0m15:51:09.315546 [debug] [Thread-3 (]: Database Error in test not_null_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_id.sql
[0m15:51:09.315848 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_movies_datetime.f46a9edf1c
[0m15:51:09.316155 [error] [Thread-3 (]: 3 of 7 ERROR not_null_movies_id ................................................ [[31mERROR[0m in 0.10s]
[0m15:51:09.316511 [debug] [Thread-2 (]: Began running node test.maker_warehouse.unique_movies_id.69344ff782
[0m15:51:09.316772 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_movies_id.a4729c6b05
[0m15:51:09.316991 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.not_null_movies_datetime.f46a9edf1c' to be skipped because of status 'error'.  Reason: Database Error in test not_null_movies_datetime (models/schema.yaml)
  column "datetime" does not exist
  LINE 16: select datetime
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_datetime.sql.
[0m15:51:09.317233 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_movies_user_id.2431798528: Close
[0m15:51:09.317423 [info ] [Thread-2 (]: 6 of 7 START test unique_movies_id ............................................. [RUN]
[0m15:51:09.317666 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.018 seconds
[0m15:51:09.317854 [debug] [Thread-3 (]: Began running node test.maker_warehouse.unique_movies_movie_id.91746df71e
[0m15:51:09.318116 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.not_null_movies_id.a4729c6b05' to be skipped because of status 'error'.  Reason: Database Error in test not_null_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_id.sql.
[0m15:51:09.319030 [debug] [Thread-2 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_movies_datetime.f46a9edf1c, now test.maker_warehouse.unique_movies_id.69344ff782)
[0m15:51:09.322968 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_movies_movie_id.8dc9aab371: ROLLBACK
[0m15:51:09.323597 [info ] [Thread-3 (]: 7 of 7 START test unique_movies_movie_id ....................................... [RUN]
[0m15:51:09.324040 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.unique_movies_id.69344ff782
[0m15:51:09.324893 [debug] [Thread-1 (]: Database Error in test not_null_movies_user_id (models/schema.yaml)
  column "user_id" does not exist
  LINE 16: select user_id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_user_id.sql
[0m15:51:09.325168 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_movies_id.a4729c6b05, now test.maker_warehouse.unique_movies_movie_id.91746df71e)
[0m15:51:09.336670 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.unique_movies_movie_id.91746df71e
[0m15:51:09.331231 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_movies_movie_id.8dc9aab371: Close
[0m15:51:09.335562 [error] [Thread-1 (]: 5 of 7 ERROR not_null_movies_user_id ........................................... [[31mERROR[0m in 0.05s]
[0m15:51:09.339027 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.unique_movies_movie_id.91746df71e"
[0m15:51:09.334541 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.unique_movies_id.69344ff782"
[0m15:51:09.339356 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_movies_user_id.2431798528
[0m15:51:09.339672 [info ] [Thread-4 (]: 4 of 7 PASS not_null_movies_movie_id ........................................... [[32mPASS[0m in 0.12s]
[0m15:51:09.340333 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.not_null_movies_user_id.2431798528' to be skipped because of status 'error'.  Reason: Database Error in test not_null_movies_user_id (models/schema.yaml)
  column "user_id" does not exist
  LINE 16: select user_id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_user_id.sql.
[0m15:51:09.340610 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.unique_movies_movie_id.91746df71e
[0m15:51:09.340830 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.unique_movies_id.69344ff782
[0m15:51:09.341067 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_movies_movie_id.8dc9aab371
[0m15:51:09.342612 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.unique_movies_movie_id.91746df71e"
[0m15:51:09.344109 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.unique_movies_id.69344ff782"
[0m15:51:09.344585 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_movies_movie_id.91746df71e"
[0m15:51:09.344770 [debug] [Thread-3 (]: On test.maker_warehouse.unique_movies_movie_id.91746df71e: BEGIN
[0m15:51:09.344950 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.unique_movies_id.69344ff782"
[0m15:51:09.345303 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:51:09.345509 [debug] [Thread-2 (]: On test.maker_warehouse.unique_movies_id.69344ff782: BEGIN
[0m15:51:09.345747 [debug] [Thread-2 (]: Opening a new connection, currently in state closed
[0m15:51:09.354317 [debug] [Thread-2 (]: SQL status: BEGIN in 0.009 seconds
[0m15:51:09.354561 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m15:51:09.354788 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.unique_movies_id.69344ff782"
[0m15:51:09.355005 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_movies_movie_id.91746df71e"
[0m15:51:09.355215 [debug] [Thread-2 (]: On test.maker_warehouse.unique_movies_id.69344ff782: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_movies_id.69344ff782"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."movies"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:51:09.355416 [debug] [Thread-3 (]: On test.maker_warehouse.unique_movies_movie_id.91746df71e: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_movies_movie_id.91746df71e"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."movies"
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:51:09.356298 [debug] [Thread-2 (]: Postgres adapter: Postgres error: column "id" does not exist
LINE 15:     id as unique_field,
             ^

[0m15:51:09.356555 [debug] [Thread-2 (]: On test.maker_warehouse.unique_movies_id.69344ff782: ROLLBACK
[0m15:51:09.357247 [debug] [Thread-2 (]: On test.maker_warehouse.unique_movies_id.69344ff782: Close
[0m15:51:09.358773 [debug] [Thread-2 (]: Database Error in test unique_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/unique_movies_id.sql
[0m15:51:09.359076 [error] [Thread-2 (]: 6 of 7 ERROR unique_movies_id .................................................. [[31mERROR[0m in 0.04s]
[0m15:51:09.359360 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.unique_movies_id.69344ff782
[0m15:51:09.359607 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.unique_movies_id.69344ff782' to be skipped because of status 'error'.  Reason: Database Error in test unique_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/unique_movies_id.sql.
[0m15:51:09.359796 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.004 seconds
[0m15:51:09.360868 [debug] [Thread-3 (]: On test.maker_warehouse.unique_movies_movie_id.91746df71e: ROLLBACK
[0m15:51:09.361362 [debug] [Thread-3 (]: On test.maker_warehouse.unique_movies_movie_id.91746df71e: Close
[0m15:51:09.361674 [info ] [Thread-3 (]: 7 of 7 PASS unique_movies_movie_id ............................................. [[32mPASS[0m in 0.04s]
[0m15:51:09.361939 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.unique_movies_movie_id.91746df71e
[0m15:51:09.363151 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.363354 [debug] [MainThread]: On master: BEGIN
[0m15:51:09.363501 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:51:09.370752 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m15:51:09.371043 [debug] [MainThread]: On master: COMMIT
[0m15:51:09.371217 [debug] [MainThread]: Using postgres connection "master"
[0m15:51:09.371360 [debug] [MainThread]: On master: COMMIT
[0m15:51:09.371671 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:51:09.371865 [debug] [MainThread]: On master: Close
[0m15:51:09.372106 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:51:09.372256 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_movies_user_id.2431798528' was properly closed.
[0m15:51:09.372388 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_movies_id.69344ff782' was properly closed.
[0m15:51:09.372513 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_movies_movie_id.91746df71e' was properly closed.
[0m15:51:09.372634 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_movies_movie_id.8dc9aab371' was properly closed.
[0m15:51:09.372813 [info ] [MainThread]: 
[0m15:51:09.372981 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 0.46 seconds (0.46s).
[0m15:51:09.373650 [debug] [MainThread]: Command end result
[0m15:51:09.389781 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:51:09.391076 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:51:09.394953 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:51:09.395222 [info ] [MainThread]: 
[0m15:51:09.395540 [info ] [MainThread]: [31mCompleted with 5 errors, 0 partial successes, and 0 warnings:[0m
[0m15:51:09.395974 [info ] [MainThread]: 
[0m15:51:09.396215 [error] [MainThread]: [31mFailure in test has_at_least_one_row_movies_ (models/schema.yaml)[0m
[0m15:51:09.396403 [error] [MainThread]:   Compilation Error in test has_at_least_one_row_movies_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:51:09.396634 [info ] [MainThread]: 
[0m15:51:09.397411 [error] [MainThread]: [31mFailure in test not_null_movies_datetime (models/schema.yaml)[0m
[0m15:51:09.397986 [error] [MainThread]:   Database Error in test not_null_movies_datetime (models/schema.yaml)
  column "datetime" does not exist
  LINE 16: select datetime
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_datetime.sql
[0m15:51:09.398171 [info ] [MainThread]: 
[0m15:51:09.398350 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/schema.yaml/not_null_movies_datetime.sql
[0m15:51:09.398499 [info ] [MainThread]: 
[0m15:51:09.398668 [error] [MainThread]: [31mFailure in test not_null_movies_id (models/schema.yaml)[0m
[0m15:51:09.398834 [error] [MainThread]:   Database Error in test not_null_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 16: select id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_id.sql
[0m15:51:09.398964 [info ] [MainThread]: 
[0m15:51:09.399116 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/schema.yaml/not_null_movies_id.sql
[0m15:51:09.399248 [info ] [MainThread]: 
[0m15:51:09.399403 [error] [MainThread]: [31mFailure in test not_null_movies_user_id (models/schema.yaml)[0m
[0m15:51:09.399561 [error] [MainThread]:   Database Error in test not_null_movies_user_id (models/schema.yaml)
  column "user_id" does not exist
  LINE 16: select user_id
                  ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/not_null_movies_user_id.sql
[0m15:51:09.399690 [info ] [MainThread]: 
[0m15:51:09.399838 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/schema.yaml/not_null_movies_user_id.sql
[0m15:51:09.399968 [info ] [MainThread]: 
[0m15:51:09.400215 [error] [MainThread]: [31mFailure in test unique_movies_id (models/schema.yaml)[0m
[0m15:51:09.400402 [error] [MainThread]:   Database Error in test unique_movies_id (models/schema.yaml)
  column "id" does not exist
  LINE 15:     id as unique_field,
               ^
  compiled code at target/run/maker_warehouse/models/schema.yaml/unique_movies_id.sql
[0m15:51:09.400550 [info ] [MainThread]: 
[0m15:51:09.400701 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/schema.yaml/unique_movies_id.sql
[0m15:51:09.400841 [info ] [MainThread]: 
[0m15:51:09.400994 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=5 SKIP=0 NO-OP=0 TOTAL=7
[0m15:51:09.402570 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 0.969335, "process_in_blocks": "0", "process_kernel_time": 0.230928, "process_mem_max_rss": "128876544", "process_out_blocks": "0", "process_user_time": 1.123457}
[0m15:51:09.402805 [debug] [MainThread]: Command `dbt test` failed at 15:51:09.402765 after 0.97 seconds
[0m15:51:09.403003 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106d479d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101084090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1171bf710>]}
[0m15:51:09.403195 [debug] [MainThread]: Flushing usage events
[0m15:51:09.867175 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:19.259073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109189450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091dff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1091dffd0>]}


============================== 15:57:19.261900 | f17768d1-09ed-422c-95e6-3a4cd35de63b ==============================
[0m15:57:19.261900 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:57:19.262269 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'log_cache_events': 'False', 'indirect_selection': 'eager', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'log_format': 'default', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'empty': 'None', 'partial_parse': 'True', 'target_path': 'None', 'write_json': 'True', 'no_print': 'None', 'invocation_command': 'dbt test --select streams', 'quiet': 'False', 'version_check': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'static_parser': 'True', 'introspect': 'True', 'debug': 'False'}
[0m15:57:19.379084 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108ef7010>]}
[0m15:57:19.408411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10683ce50>]}
[0m15:57:19.409074 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:57:19.520269 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:57:19.608999 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:57:19.609383 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/schema.yaml
[0m15:57:19.842582 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1074a7810>]}
[0m15:57:19.885882 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:57:19.888675 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:57:19.906348 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a5a3d50>]}
[0m15:57:19.906599 [info ] [MainThread]: Found 5 models, 7 data tests, 1 source, 567 macros
[0m15:57:19.906778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a32b1d0>]}
[0m15:57:19.907619 [info ] [MainThread]: 
[0m15:57:19.907809 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:57:19.907947 [info ] [MainThread]: 
[0m15:57:19.908176 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:57:19.910071 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m15:57:19.942338 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:57:19.942567 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:57:19.942701 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:57:20.083408 [debug] [ThreadPool]: SQL status: BEGIN in 0.141 seconds
[0m15:57:20.083702 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:57:20.083897 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:57:20.109454 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.025 seconds
[0m15:57:20.110448 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:57:20.111069 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:57:20.115000 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.115219 [debug] [MainThread]: On master: BEGIN
[0m15:57:20.115372 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:20.127004 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m15:57:20.128018 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.128859 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:57:20.147468 [debug] [MainThread]: SQL status: SELECT 32 in 0.018 seconds
[0m15:57:20.148740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f17768d1-09ed-422c-95e6-3a4cd35de63b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a3d9210>]}
[0m15:57:20.149082 [debug] [MainThread]: On master: ROLLBACK
[0m15:57:20.149676 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.149962 [debug] [MainThread]: On master: BEGIN
[0m15:57:20.150648 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m15:57:20.150867 [debug] [MainThread]: On master: COMMIT
[0m15:57:20.151047 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.151205 [debug] [MainThread]: On master: COMMIT
[0m15:57:20.151586 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:57:20.151761 [debug] [MainThread]: On master: Close
[0m15:57:20.153831 [debug] [Thread-1 (]: Began running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:20.154041 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:20.154241 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:20.154413 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:20.154616 [info ] [Thread-1 (]: 1 of 7 START test has_at_least_one_row_streams_ ................................ [RUN]
[0m15:57:20.154837 [info ] [Thread-2 (]: 2 of 7 START test not_null_streams_datetime .................................... [RUN]
[0m15:57:20.155066 [info ] [Thread-3 (]: 3 of 7 START test not_null_streams_id .......................................... [RUN]
[0m15:57:20.155355 [info ] [Thread-4 (]: 4 of 7 START test not_null_streams_movie_id .................................... [RUN]
[0m15:57:20.155632 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2)
[0m15:57:20.156159 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44'
[0m15:57:20.156508 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_id.616e67536f'
[0m15:57:20.156757 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983'
[0m15:57:20.156950 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:20.157124 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:20.157291 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:20.157477 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:20.183420 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:20.185667 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:20.188374 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:20.189149 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:20.189342 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:20.189518 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:20.202584 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:20.204138 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:20.205583 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:20.206103 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:20.206376 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:20.206609 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:20.206777 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: BEGIN
[0m15:57:20.206964 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: BEGIN
[0m15:57:20.207183 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: BEGIN
[0m15:57:20.207470 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:57:20.207642 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:57:20.207815 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:57:20.209881 [debug] [Thread-1 (]: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:57:20.210157 [error] [Thread-1 (]: 1 of 7 ERROR has_at_least_one_row_streams_ ..................................... [[31mERROR[0m in 0.05s]
[0m15:57:20.210425 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:20.210621 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:20.210847 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2' to be skipped because of status 'error'.  Reason: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:57:20.211071 [info ] [Thread-1 (]: 5 of 7 START test not_null_streams_user_id ..................................... [RUN]
[0m15:57:20.211737 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2, now test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8)
[0m15:57:20.212027 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:20.214743 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:20.215172 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:20.216779 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:20.217201 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:20.217415 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: BEGIN
[0m15:57:20.217610 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:57:20.218640 [debug] [Thread-3 (]: SQL status: BEGIN in 0.011 seconds
[0m15:57:20.218851 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:20.219137 [debug] [Thread-2 (]: SQL status: BEGIN in 0.011 seconds
[0m15:57:20.219345 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_id.616e67536f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."streams"
where id is null



  
  
      
    ) dbt_internal_test
[0m15:57:20.219635 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:20.219825 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m15:57:20.220052 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_datetime.e4db369b44"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."streams"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m15:57:20.220258 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:20.220489 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."streams"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m15:57:20.230542 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m15:57:20.231819 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:20.232664 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."streams"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:57:20.419371 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.199 seconds
[0m15:57:20.419953 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.199 seconds
[0m15:57:20.420485 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.200 seconds
[0m15:57:20.420962 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.188 seconds
[0m15:57:20.424358 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: ROLLBACK
[0m15:57:20.425607 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: ROLLBACK
[0m15:57:20.426805 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: ROLLBACK
[0m15:57:20.427563 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: ROLLBACK
[0m15:57:20.428676 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: Close
[0m15:57:20.429367 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: Close
[0m15:57:20.429555 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: Close
[0m15:57:20.429909 [info ] [Thread-1 (]: 5 of 7 PASS not_null_streams_user_id ........................................... [[32mPASS[0m in 0.22s]
[0m15:57:20.430310 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: Close
[0m15:57:20.430673 [info ] [Thread-3 (]: 3 of 7 PASS not_null_streams_id ................................................ [[32mPASS[0m in 0.27s]
[0m15:57:20.431223 [info ] [Thread-4 (]: 4 of 7 PASS not_null_streams_movie_id .......................................... [[32mPASS[0m in 0.27s]
[0m15:57:20.431636 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:20.432264 [info ] [Thread-2 (]: 2 of 7 PASS not_null_streams_datetime .......................................... [[32mPASS[0m in 0.28s]
[0m15:57:20.433084 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:20.433602 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:20.434041 [debug] [Thread-1 (]: Began running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:20.434420 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:20.434630 [debug] [Thread-3 (]: Began running node test.maker_warehouse.unique_streams_movie_id.3571880c06
[0m15:57:20.434951 [info ] [Thread-1 (]: 6 of 7 START test unique_streams_id ............................................ [RUN]
[0m15:57:20.435625 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8, now test.maker_warehouse.unique_streams_id.9d424521fe)
[0m15:57:20.435372 [info ] [Thread-3 (]: 7 of 7 START test unique_streams_movie_id ...................................... [RUN]
[0m15:57:20.435921 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:20.436126 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_id.616e67536f, now test.maker_warehouse.unique_streams_movie_id.3571880c06)
[0m15:57:20.441134 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:20.441406 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.unique_streams_movie_id.3571880c06
[0m15:57:20.443724 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_movie_id.3571880c06"
[0m15:57:20.444062 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:20.446028 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:20.446306 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.unique_streams_movie_id.3571880c06
[0m15:57:20.447726 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_movie_id.3571880c06"
[0m15:57:20.447993 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:20.448174 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: BEGIN
[0m15:57:20.448358 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:57:20.448758 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_streams_movie_id.3571880c06"
[0m15:57:20.448920 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_movie_id.3571880c06: BEGIN
[0m15:57:20.449088 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m15:57:20.457717 [debug] [Thread-1 (]: SQL status: BEGIN in 0.009 seconds
[0m15:57:20.458036 [debug] [Thread-3 (]: SQL status: BEGIN in 0.009 seconds
[0m15:57:20.458287 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:20.458518 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_streams_movie_id.3571880c06"
[0m15:57:20.458733 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_id.9d424521fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:57:20.458959 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_movie_id.3571880c06: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_movie_id.3571880c06"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    movie_id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where movie_id is not null
group by movie_id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:57:20.526473 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.067 seconds
[0m15:57:20.527541 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_movie_id.3571880c06: ROLLBACK
[0m15:57:20.528139 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_movie_id.3571880c06: Close
[0m15:57:20.528772 [error] [Thread-3 (]: 7 of 7 FAIL 7208 unique_streams_movie_id ....................................... [[31mFAIL 7208[0m in 0.09s]
[0m15:57:20.529161 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.unique_streams_movie_id.3571880c06
[0m15:57:20.747468 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.288 seconds
[0m15:57:20.750071 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: ROLLBACK
[0m15:57:20.750947 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: Close
[0m15:57:20.751741 [info ] [Thread-1 (]: 6 of 7 PASS unique_streams_id .................................................. [[32mPASS[0m in 0.32s]
[0m15:57:20.752494 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:20.755340 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.756159 [debug] [MainThread]: On master: BEGIN
[0m15:57:20.757144 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:57:20.768971 [debug] [MainThread]: SQL status: BEGIN in 0.012 seconds
[0m15:57:20.769295 [debug] [MainThread]: On master: COMMIT
[0m15:57:20.769489 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:20.769638 [debug] [MainThread]: On master: COMMIT
[0m15:57:20.770029 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:57:20.770223 [debug] [MainThread]: On master: Close
[0m15:57:20.770463 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:57:20.770615 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_id.9d424521fe' was properly closed.
[0m15:57:20.770758 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44' was properly closed.
[0m15:57:20.770883 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_movie_id.3571880c06' was properly closed.
[0m15:57:20.770997 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983' was properly closed.
[0m15:57:20.771170 [info ] [MainThread]: 
[0m15:57:20.771335 [info ] [MainThread]: Finished running 7 data tests in 0 hours 0 minutes and 0.86 seconds (0.86s).
[0m15:57:20.772047 [debug] [MainThread]: Command end result
[0m15:57:20.788363 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:57:20.789536 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:57:20.795757 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:57:20.796079 [info ] [MainThread]: 
[0m15:57:20.796576 [info ] [MainThread]: [31mCompleted with 2 errors, 0 partial successes, and 0 warnings:[0m
[0m15:57:20.797292 [info ] [MainThread]: 
[0m15:57:20.797866 [error] [MainThread]: [31mFailure in test has_at_least_one_row_streams_ (models/schema.yaml)[0m
[0m15:57:20.798141 [error] [MainThread]:   Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:57:20.798570 [info ] [MainThread]: 
[0m15:57:20.799077 [error] [MainThread]: [31mFailure in test unique_streams_movie_id (models/schema.yaml)[0m
[0m15:57:20.799297 [error] [MainThread]:   Got 7208 results, configured to fail if != 0
[0m15:57:20.799454 [info ] [MainThread]: 
[0m15:57:20.799632 [info ] [MainThread]:   compiled code at target/compiled/maker_warehouse/models/schema.yaml/unique_streams_movie_id.sql
[0m15:57:20.799783 [info ] [MainThread]: 
[0m15:57:20.799940 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=2 SKIP=0 NO-OP=0 TOTAL=7
[0m15:57:20.801631 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 1.5809376, "process_in_blocks": "0", "process_kernel_time": 0.254651, "process_mem_max_rss": "135479296", "process_out_blocks": "0", "process_user_time": 1.333476}
[0m15:57:20.801923 [debug] [MainThread]: Command `dbt test` failed at 15:57:20.801874 after 1.58 seconds
[0m15:57:20.802166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104a09110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109151250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a2aca10>]}
[0m15:57:20.802383 [debug] [MainThread]: Flushing usage events
[0m15:57:21.235078 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m15:57:47.731863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10907bf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108db6c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1090a5450>]}


============================== 15:57:47.734694 | d4d31fb4-101a-42c2-9370-268ea5115654 ==============================
[0m15:57:47.734694 [info ] [MainThread]: Running with dbt=1.10.13
[0m15:57:47.735035 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'printer_width': '80', 'introspect': 'True', 'version_check': 'True', 'static_parser': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'invocation_command': 'dbt test --select streams', 'no_print': 'None', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'empty': 'None', 'write_json': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'fail_fast': 'False', 'warn_error': 'None'}
[0m15:57:47.839848 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1082ab4d0>]}
[0m15:57:47.868810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1066fcbd0>]}
[0m15:57:47.869454 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m15:57:47.929048 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m15:57:48.013517 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:57:48.013901 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/schema.yaml
[0m15:57:48.240731 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109d45210>]}
[0m15:57:48.282599 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:57:48.283792 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:57:48.301276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a455e90>]}
[0m15:57:48.301537 [info ] [MainThread]: Found 5 models, 6 data tests, 1 source, 567 macros
[0m15:57:48.301716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1787d0>]}
[0m15:57:48.302540 [info ] [MainThread]: 
[0m15:57:48.302724 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m15:57:48.302861 [info ] [MainThread]: 
[0m15:57:48.303081 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m15:57:48.305014 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m15:57:48.333801 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:57:48.334015 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m15:57:48.334164 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:57:48.389326 [debug] [ThreadPool]: SQL status: BEGIN in 0.055 seconds
[0m15:57:48.389641 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m15:57:48.389838 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m15:57:48.402185 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.012 seconds
[0m15:57:48.403271 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m15:57:48.403907 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m15:57:48.408614 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.408852 [debug] [MainThread]: On master: BEGIN
[0m15:57:48.409020 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:57:48.416584 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m15:57:48.416864 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.417337 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m15:57:48.424653 [debug] [MainThread]: SQL status: SELECT 32 in 0.007 seconds
[0m15:57:48.425776 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd4d31fb4-101a-42c2-9370-268ea5115654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a20b450>]}
[0m15:57:48.426074 [debug] [MainThread]: On master: ROLLBACK
[0m15:57:48.426504 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.426701 [debug] [MainThread]: On master: BEGIN
[0m15:57:48.427339 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m15:57:48.427532 [debug] [MainThread]: On master: COMMIT
[0m15:57:48.427800 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.427997 [debug] [MainThread]: On master: COMMIT
[0m15:57:48.428393 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:57:48.428626 [debug] [MainThread]: On master: Close
[0m15:57:48.430618 [debug] [Thread-1 (]: Began running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:48.430832 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:48.431050 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:48.431387 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:48.431237 [info ] [Thread-1 (]: 1 of 6 START test has_at_least_one_row_streams_ ................................ [RUN]
[0m15:57:48.431632 [info ] [Thread-2 (]: 2 of 6 START test not_null_streams_datetime .................................... [RUN]
[0m15:57:48.431824 [info ] [Thread-3 (]: 3 of 6 START test not_null_streams_id .......................................... [RUN]
[0m15:57:48.432056 [info ] [Thread-4 (]: 4 of 6 START test not_null_streams_movie_id .................................... [RUN]
[0m15:57:48.432328 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2)
[0m15:57:48.432582 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44'
[0m15:57:48.432829 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_id.616e67536f'
[0m15:57:48.433079 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983'
[0m15:57:48.433271 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:48.433450 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:48.433634 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:48.433802 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:48.447154 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:48.452179 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:48.460531 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:48.466693 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:48.467203 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:48.477905 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:48.478131 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:48.479816 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:48.481238 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:48.481763 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:48.482001 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:48.482188 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:48.482356 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: BEGIN
[0m15:57:48.482549 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: BEGIN
[0m15:57:48.482721 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: BEGIN
[0m15:57:48.482932 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m15:57:48.483097 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m15:57:48.483268 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m15:57:48.485812 [debug] [Thread-1 (]: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:57:48.486101 [error] [Thread-1 (]: 1 of 6 ERROR has_at_least_one_row_streams_ ..................................... [[31mERROR[0m in 0.05s]
[0m15:57:48.486377 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m15:57:48.486575 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:48.486805 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2' to be skipped because of status 'error'.  Reason: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m15:57:48.487059 [info ] [Thread-1 (]: 5 of 6 START test not_null_streams_user_id ..................................... [RUN]
[0m15:57:48.487851 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2, now test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8)
[0m15:57:48.488062 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:48.490497 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:48.490824 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:48.492118 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:48.492516 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:48.492945 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: BEGIN
[0m15:57:48.493162 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:57:48.495016 [debug] [Thread-4 (]: SQL status: BEGIN in 0.012 seconds
[0m15:57:48.495317 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m15:57:48.495601 [debug] [Thread-3 (]: SQL status: BEGIN in 0.012 seconds
[0m15:57:48.496083 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m15:57:48.496685 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m15:57:48.497201 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m15:57:48.497722 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."streams"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m15:57:48.498051 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_datetime.e4db369b44"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."streams"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m15:57:48.498269 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_id.616e67536f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."streams"
where id is null



  
  
      
    ) dbt_internal_test
[0m15:57:48.503391 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m15:57:48.510851 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m15:57:48.514053 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."streams"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m15:57:48.582430 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.084 seconds
[0m15:57:48.582746 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.084 seconds
[0m15:57:48.582920 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.084 seconds
[0m15:57:48.585428 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: ROLLBACK
[0m15:57:48.585691 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.071 seconds
[0m15:57:48.586673 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: ROLLBACK
[0m15:57:48.587585 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: ROLLBACK
[0m15:57:48.588357 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: ROLLBACK
[0m15:57:48.588527 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: Close
[0m15:57:48.589182 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: Close
[0m15:57:48.589377 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: Close
[0m15:57:48.588998 [info ] [Thread-4 (]: 4 of 6 PASS not_null_streams_movie_id .......................................... [[32mPASS[0m in 0.16s]
[0m15:57:48.589592 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: Close
[0m15:57:48.589903 [info ] [Thread-2 (]: 2 of 6 PASS not_null_streams_datetime .......................................... [[32mPASS[0m in 0.16s]
[0m15:57:48.590465 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m15:57:48.590184 [info ] [Thread-1 (]: 5 of 6 PASS not_null_streams_user_id ........................................... [[32mPASS[0m in 0.10s]
[0m15:57:48.590816 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m15:57:48.591070 [info ] [Thread-3 (]: 3 of 6 PASS not_null_streams_id ................................................ [[32mPASS[0m in 0.16s]
[0m15:57:48.591297 [debug] [Thread-4 (]: Began running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:48.591570 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m15:57:48.591897 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m15:57:48.592071 [info ] [Thread-4 (]: 6 of 6 START test unique_streams_id ............................................ [RUN]
[0m15:57:48.592406 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_movie_id.fa18a97983, now test.maker_warehouse.unique_streams_id.9d424521fe)
[0m15:57:48.592617 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:48.596533 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:48.597011 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:48.598850 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:48.599318 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:48.599487 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: BEGIN
[0m15:57:48.599640 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m15:57:48.605848 [debug] [Thread-4 (]: SQL status: BEGIN in 0.006 seconds
[0m15:57:48.606045 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m15:57:48.606223 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_id.9d424521fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m15:57:48.914605 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.308 seconds
[0m15:57:48.918730 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: ROLLBACK
[0m15:57:48.919650 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: Close
[0m15:57:48.920526 [info ] [Thread-4 (]: 6 of 6 PASS unique_streams_id .................................................. [[32mPASS[0m in 0.33s]
[0m15:57:48.921273 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m15:57:48.922698 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.923187 [debug] [MainThread]: On master: BEGIN
[0m15:57:48.923618 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m15:57:48.933492 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m15:57:48.933839 [debug] [MainThread]: On master: COMMIT
[0m15:57:48.934031 [debug] [MainThread]: Using postgres connection "master"
[0m15:57:48.934181 [debug] [MainThread]: On master: COMMIT
[0m15:57:48.934529 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m15:57:48.934732 [debug] [MainThread]: On master: Close
[0m15:57:48.934990 [debug] [MainThread]: Connection 'master' was properly closed.
[0m15:57:48.935154 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8' was properly closed.
[0m15:57:48.935284 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44' was properly closed.
[0m15:57:48.935421 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_id.616e67536f' was properly closed.
[0m15:57:48.935539 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_id.9d424521fe' was properly closed.
[0m15:57:48.935708 [info ] [MainThread]: 
[0m15:57:48.935875 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 0.63 seconds (0.63s).
[0m15:57:48.937001 [debug] [MainThread]: Command end result
[0m15:57:48.955459 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m15:57:48.956878 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m15:57:48.970315 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m15:57:48.971963 [info ] [MainThread]: 
[0m15:57:48.972614 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m15:57:48.972866 [info ] [MainThread]: 
[0m15:57:48.973096 [error] [MainThread]: [31mFailure in test has_at_least_one_row_streams_ (models/schema.yaml)[0m
[0m15:57:48.973285 [error] [MainThread]:   Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m15:57:48.973490 [info ] [MainThread]: 
[0m15:57:48.973719 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m15:57:48.975602 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 1.2813708, "process_in_blocks": "0", "process_kernel_time": 0.184797, "process_mem_max_rss": "137003008", "process_out_blocks": "0", "process_user_time": 1.255703}
[0m15:57:48.975835 [debug] [MainThread]: Command `dbt test` failed at 15:57:48.975794 after 1.28 seconds
[0m15:57:48.976033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1048c8fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104820090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104820cd0>]}
[0m15:57:48.976214 [debug] [MainThread]: Flushing usage events
[0m15:57:49.324300 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:02:17.471446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076c7150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107989010>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1079a3fd0>]}


============================== 16:02:17.474222 | f76e16a6-0d3d-44fe-ad55-eb726d1a2128 ==============================
[0m16:02:17.474222 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:02:17.474549 [debug] [MainThread]: running dbt with arguments {'quiet': 'False', 'warn_error': 'None', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt test --select streams', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'partial_parse': 'True', 'debug': 'False', 'no_print': 'None', 'empty': 'None', 'fail_fast': 'False', 'use_colors': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'log_cache_events': 'False', 'introspect': 'True', 'write_json': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'cache_selected_only': 'False'}
[0m16:02:17.591642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x107fb7fd0>]}
[0m16:02:17.620720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10500cb10>]}
[0m16:02:17.621373 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m16:02:17.680595 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:02:17.768386 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 0 files changed.
[0m16:02:17.768688 [debug] [MainThread]: Partial parsing: added file: dbt_utils://macros/generic_tests/has_at_least_one_row.sql
[0m16:02:17.813539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10894f610>]}
[0m16:02:17.859171 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:02:17.860333 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:02:17.878231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108c4a1d0>]}
[0m16:02:17.878506 [info ] [MainThread]: Found 5 models, 6 data tests, 1 source, 568 macros
[0m16:02:17.878698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1087d87d0>]}
[0m16:02:17.879551 [info ] [MainThread]: 
[0m16:02:17.879735 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:02:17.879875 [info ] [MainThread]: 
[0m16:02:17.880129 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m16:02:17.882173 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m16:02:17.941570 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:02:17.941795 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m16:02:17.941935 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:02:18.020542 [debug] [ThreadPool]: SQL status: BEGIN in 0.079 seconds
[0m16:02:18.020798 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:02:18.020975 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m16:02:18.044521 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.023 seconds
[0m16:02:18.045626 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m16:02:18.046408 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m16:02:18.051732 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.051962 [debug] [MainThread]: On master: BEGIN
[0m16:02:18.052132 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:02:18.062025 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m16:02:18.062337 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.062680 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m16:02:18.069464 [debug] [MainThread]: SQL status: SELECT 32 in 0.006 seconds
[0m16:02:18.070654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f76e16a6-0d3d-44fe-ad55-eb726d1a2128', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108f7dc50>]}
[0m16:02:18.070972 [debug] [MainThread]: On master: ROLLBACK
[0m16:02:18.071465 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.071859 [debug] [MainThread]: On master: BEGIN
[0m16:02:18.072662 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m16:02:18.072865 [debug] [MainThread]: On master: COMMIT
[0m16:02:18.073030 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.073210 [debug] [MainThread]: On master: COMMIT
[0m16:02:18.073581 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:02:18.073786 [debug] [MainThread]: On master: Close
[0m16:02:18.075828 [debug] [Thread-1 (]: Began running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:02:18.076050 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:02:18.076235 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:02:18.076417 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:02:18.076607 [info ] [Thread-1 (]: 1 of 6 START test has_at_least_one_row_streams_ ................................ [RUN]
[0m16:02:18.076853 [info ] [Thread-2 (]: 2 of 6 START test not_null_streams_datetime .................................... [RUN]
[0m16:02:18.077093 [info ] [Thread-3 (]: 3 of 6 START test not_null_streams_id .......................................... [RUN]
[0m16:02:18.077320 [info ] [Thread-4 (]: 4 of 6 START test not_null_streams_movie_id .................................... [RUN]
[0m16:02:18.077584 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2)
[0m16:02:18.077853 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44'
[0m16:02:18.078099 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_id.616e67536f'
[0m16:02:18.078455 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983'
[0m16:02:18.078645 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:02:18.078828 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:02:18.079027 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:02:18.079214 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:02:18.091841 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:02:18.094830 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:02:18.098206 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:02:18.099137 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:02:18.110560 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:02:18.115785 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:02:18.116010 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:02:18.117570 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:02:18.119174 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:02:18.119625 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:02:18.119823 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:02:18.120143 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: BEGIN
[0m16:02:18.120357 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: BEGIN
[0m16:02:18.120544 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:02:18.120714 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:02:18.120936 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:02:18.121169 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: BEGIN
[0m16:02:18.121528 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:02:18.124662 [debug] [Thread-1 (]: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m16:02:18.124998 [error] [Thread-1 (]: 1 of 6 ERROR has_at_least_one_row_streams_ ..................................... [[31mERROR[0m in 0.05s]
[0m16:02:18.125320 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:02:18.125796 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:02:18.126034 [info ] [Thread-1 (]: 5 of 6 START test not_null_streams_user_id ..................................... [RUN]
[0m16:02:18.137568 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2' to be skipped because of status 'error'.  Reason: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m16:02:18.136502 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2, now test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8)
[0m16:02:18.145310 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:02:18.157356 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:02:18.157815 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:02:18.159439 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:02:18.160040 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:02:18.160239 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: BEGIN
[0m16:02:18.160461 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:02:18.160745 [debug] [Thread-3 (]: SQL status: BEGIN in 0.040 seconds
[0m16:02:18.161695 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:02:18.162076 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_id.616e67536f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."streams"
where id is null



  
  
      
    ) dbt_internal_test
[0m16:02:18.162366 [debug] [Thread-4 (]: SQL status: BEGIN in 0.042 seconds
[0m16:02:18.162621 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:02:18.162815 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."streams"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m16:02:18.163315 [debug] [Thread-2 (]: SQL status: BEGIN in 0.042 seconds
[0m16:02:18.163563 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:02:18.163742 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_datetime.e4db369b44"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."streams"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m16:02:18.170309 [debug] [Thread-1 (]: SQL status: BEGIN in 0.010 seconds
[0m16:02:18.170584 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:02:18.170878 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."streams"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:02:18.290365 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.128 seconds
[0m16:02:18.290622 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.128 seconds
[0m16:02:18.290816 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.127 seconds
[0m16:02:18.290981 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.120 seconds
[0m16:02:18.293221 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: ROLLBACK
[0m16:02:18.293937 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: ROLLBACK
[0m16:02:18.294862 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: ROLLBACK
[0m16:02:18.295576 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: ROLLBACK
[0m16:02:18.296093 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: Close
[0m16:02:18.296264 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: Close
[0m16:02:18.296430 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: Close
[0m16:02:18.296589 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: Close
[0m16:02:18.296926 [info ] [Thread-3 (]: 3 of 6 PASS not_null_streams_id ................................................ [[32mPASS[0m in 0.22s]
[0m16:02:18.297276 [info ] [Thread-1 (]: 5 of 6 PASS not_null_streams_user_id ........................................... [[32mPASS[0m in 0.17s]
[0m16:02:18.297691 [info ] [Thread-4 (]: 4 of 6 PASS not_null_streams_movie_id .......................................... [[32mPASS[0m in 0.22s]
[0m16:02:18.298233 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:02:18.297990 [info ] [Thread-2 (]: 2 of 6 PASS not_null_streams_datetime .......................................... [[32mPASS[0m in 0.22s]
[0m16:02:18.298538 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:02:18.298779 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:02:18.298974 [debug] [Thread-3 (]: Began running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:02:18.299226 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:02:18.299593 [info ] [Thread-3 (]: 6 of 6 START test unique_streams_id ............................................ [RUN]
[0m16:02:18.299887 [debug] [Thread-3 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_id.616e67536f, now test.maker_warehouse.unique_streams_id.9d424521fe)
[0m16:02:18.300054 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:02:18.304405 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:02:18.304946 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:02:18.306459 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:02:18.307298 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:02:18.307500 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: BEGIN
[0m16:02:18.307658 [debug] [Thread-3 (]: Opening a new connection, currently in state closed
[0m16:02:18.315008 [debug] [Thread-3 (]: SQL status: BEGIN in 0.007 seconds
[0m16:02:18.315253 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:02:18.315443 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_id.9d424521fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:02:18.836485 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.520 seconds
[0m16:02:18.839889 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: ROLLBACK
[0m16:02:18.841025 [debug] [Thread-3 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: Close
[0m16:02:18.842051 [info ] [Thread-3 (]: 6 of 6 PASS unique_streams_id .................................................. [[32mPASS[0m in 0.54s]
[0m16:02:18.842793 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:02:18.844681 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.845423 [debug] [MainThread]: On master: BEGIN
[0m16:02:18.845949 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:02:18.856345 [debug] [MainThread]: SQL status: BEGIN in 0.010 seconds
[0m16:02:18.856781 [debug] [MainThread]: On master: COMMIT
[0m16:02:18.856980 [debug] [MainThread]: Using postgres connection "master"
[0m16:02:18.857115 [debug] [MainThread]: On master: COMMIT
[0m16:02:18.857482 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:02:18.857678 [debug] [MainThread]: On master: Close
[0m16:02:18.857923 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:02:18.858076 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8' was properly closed.
[0m16:02:18.858207 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44' was properly closed.
[0m16:02:18.858339 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_id.9d424521fe' was properly closed.
[0m16:02:18.858455 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983' was properly closed.
[0m16:02:18.858648 [info ] [MainThread]: 
[0m16:02:18.858823 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 0.98 seconds (0.98s).
[0m16:02:18.859399 [debug] [MainThread]: Command end result
[0m16:02:18.888942 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:02:18.894281 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:02:18.900676 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m16:02:18.900900 [info ] [MainThread]: 
[0m16:02:18.901122 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:02:18.901389 [info ] [MainThread]: 
[0m16:02:18.901636 [error] [MainThread]: [31mFailure in test has_at_least_one_row_streams_ (models/schema.yaml)[0m
[0m16:02:18.901829 [error] [MainThread]:   Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m16:02:18.901991 [info ] [MainThread]: 
[0m16:02:18.902154 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m16:02:18.903956 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 1.4706888, "process_in_blocks": "0", "process_kernel_time": 0.214769, "process_mem_max_rss": "132399104", "process_out_blocks": "0", "process_user_time": 1.121238}
[0m16:02:18.904230 [debug] [MainThread]: Command `dbt test` failed at 16:02:18.904179 after 1.47 seconds
[0m16:02:18.904482 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10784e090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1067c40d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1075da450>]}
[0m16:02:18.904697 [debug] [MainThread]: Flushing usage events
[0m16:02:19.283021 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:03:10.812340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10595bb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d3f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d3fd0>]}


============================== 16:03:10.815106 | f5d78c91-01dd-4844-b1ee-994baf3e0d43 ==============================
[0m16:03:10.815106 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:03:10.815439 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'printer_width': '80', 'introspect': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'quiet': 'False', 'static_parser': 'True', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'use_colors': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'version_check': 'True', 'write_json': 'True', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'no_print': 'None', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs'}
[0m16:03:10.934161 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x104be33d0>]}
[0m16:03:10.964037 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103034dd0>]}
[0m16:03:10.964675 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m16:03:11.024625 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:03:11.116464 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:03:11.116684 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:03:11.139212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10684f510>]}
[0m16:03:11.184099 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:03:11.185357 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:03:11.199130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106bbd550>]}
[0m16:03:11.199376 [info ] [MainThread]: Found 5 models, 6 data tests, 1 source, 568 macros
[0m16:03:11.199553 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1060a3f10>]}
[0m16:03:11.200468 [info ] [MainThread]: 
[0m16:03:11.200656 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:03:11.200793 [info ] [MainThread]: 
[0m16:03:11.201039 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m16:03:11.202929 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb'
[0m16:03:11.259338 [debug] [ThreadPool]: Using postgres connection "list_mydb"
[0m16:03:11.259565 [debug] [ThreadPool]: On list_mydb: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb"} */

    select distinct nspname from pg_namespace
  
[0m16:03:11.259714 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:03:11.361744 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.102 seconds
[0m16:03:11.362540 [debug] [ThreadPool]: On list_mydb: Close
[0m16:03:11.363392 [debug] [ThreadPool]: Re-using an available connection from the pool (formerly list_mydb, now list_mydb_dbt_warehouse)
[0m16:03:11.366840 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:03:11.367022 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m16:03:11.367163 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m16:03:11.374941 [debug] [ThreadPool]: SQL status: BEGIN in 0.008 seconds
[0m16:03:11.375134 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:03:11.375314 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m16:03:11.382440 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.007 seconds
[0m16:03:11.383299 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m16:03:11.383752 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m16:03:11.387679 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:11.387878 [debug] [MainThread]: On master: BEGIN
[0m16:03:11.388029 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:03:11.394998 [debug] [MainThread]: SQL status: BEGIN in 0.007 seconds
[0m16:03:11.395205 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:11.395416 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m16:03:11.402691 [debug] [MainThread]: SQL status: SELECT 32 in 0.007 seconds
[0m16:03:11.403943 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106aca350>]}
[0m16:03:11.404224 [debug] [MainThread]: On master: ROLLBACK
[0m16:03:11.404737 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:11.404898 [debug] [MainThread]: On master: BEGIN
[0m16:03:11.405532 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m16:03:11.405688 [debug] [MainThread]: On master: COMMIT
[0m16:03:11.405843 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:11.406125 [debug] [MainThread]: On master: COMMIT
[0m16:03:11.406620 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:03:11.406826 [debug] [MainThread]: On master: Close
[0m16:03:11.409060 [debug] [Thread-1 (]: Began running node model.maker_warehouse.genres
[0m16:03:11.409290 [debug] [Thread-2 (]: Began running node model.maker_warehouse.movies
[0m16:03:11.409507 [debug] [Thread-3 (]: Began running node model.maker_warehouse.streams
[0m16:03:11.410523 [debug] [Thread-4 (]: Began running node model.maker_warehouse.test
[0m16:03:11.410105 [info ] [Thread-1 (]: 1 of 5 START sql table model dbt_warehouse.genres .............................. [RUN]
[0m16:03:11.411169 [info ] [Thread-2 (]: 2 of 5 START sql table model dbt_warehouse.movies .............................. [RUN]
[0m16:03:11.411724 [info ] [Thread-3 (]: 3 of 5 START sql table model dbt_warehouse.streams ............................. [RUN]
[0m16:03:11.412128 [info ] [Thread-4 (]: 4 of 5 START sql view model dbt_warehouse.test ................................. [RUN]
[0m16:03:11.412501 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now model.maker_warehouse.genres)
[0m16:03:11.412817 [debug] [Thread-2 (]: Acquiring new postgres connection 'model.maker_warehouse.movies'
[0m16:03:11.413188 [debug] [Thread-3 (]: Acquiring new postgres connection 'model.maker_warehouse.streams'
[0m16:03:11.413419 [debug] [Thread-4 (]: Acquiring new postgres connection 'model.maker_warehouse.test'
[0m16:03:11.413624 [debug] [Thread-1 (]: Began compiling node model.maker_warehouse.genres
[0m16:03:11.413805 [debug] [Thread-2 (]: Began compiling node model.maker_warehouse.movies
[0m16:03:11.413973 [debug] [Thread-3 (]: Began compiling node model.maker_warehouse.streams
[0m16:03:11.414139 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.test
[0m16:03:11.424624 [debug] [Thread-1 (]: Writing injected SQL for node "model.maker_warehouse.genres"
[0m16:03:11.428442 [debug] [Thread-2 (]: Writing injected SQL for node "model.maker_warehouse.movies"
[0m16:03:11.432028 [debug] [Thread-3 (]: Writing injected SQL for node "model.maker_warehouse.streams"
[0m16:03:11.433738 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.test"
[0m16:03:11.434623 [debug] [Thread-1 (]: Began executing node model.maker_warehouse.genres
[0m16:03:11.434809 [debug] [Thread-3 (]: Began executing node model.maker_warehouse.streams
[0m16:03:11.434990 [debug] [Thread-2 (]: Began executing node model.maker_warehouse.movies
[0m16:03:11.441461 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.test
[0m16:03:11.456025 [debug] [Thread-1 (]: Writing runtime sql for node "model.maker_warehouse.genres"
[0m16:03:11.457879 [debug] [Thread-3 (]: Writing runtime sql for node "model.maker_warehouse.streams"
[0m16:03:11.459646 [debug] [Thread-2 (]: Writing runtime sql for node "model.maker_warehouse.movies"
[0m16:03:11.469761 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.test"
[0m16:03:11.470762 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:11.471049 [debug] [Thread-1 (]: On model.maker_warehouse.genres: BEGIN
[0m16:03:11.471258 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:11.471460 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:03:11.471669 [debug] [Thread-3 (]: On model.maker_warehouse.streams: BEGIN
[0m16:03:11.471874 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.472052 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:11.472314 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:03:11.472510 [debug] [Thread-4 (]: On model.maker_warehouse.test: BEGIN
[0m16:03:11.472676 [debug] [Thread-2 (]: On model.maker_warehouse.movies: BEGIN
[0m16:03:11.472911 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:03:11.473168 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:03:11.484120 [debug] [Thread-1 (]: SQL status: BEGIN in 0.013 seconds
[0m16:03:11.484415 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:11.484731 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */

  
    

  create  table "mydb"."dbt_warehouse"."genres__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  genres as genre  
from mydb.public.raw_netflix
  );
  
[0m16:03:11.484993 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m16:03:11.485241 [debug] [Thread-3 (]: SQL status: BEGIN in 0.013 seconds
[0m16:03:11.485484 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:11.485667 [debug] [Thread-4 (]: SQL status: BEGIN in 0.013 seconds
[0m16:03:11.485858 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:11.486058 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */

  
    

  create  table "mydb"."dbt_warehouse"."movies__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    movie_id,
    title,
    genres,
    release_date,
    datetime,
    row_number() over (partition by movie_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  movie_id,      
  title,
  genres,
  release_date
from base
where rn = 1
  );
  
[0m16:03:11.486244 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.486425 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */

  
    

  create  table "mydb"."dbt_warehouse"."streams__dbt_tmp"
  
  
    as
  
  (
    

select distinct
  id,
  user_id,        
  movie_id,       
  genres,
  datetime
from mydb.public.raw_netflix
  );
  
[0m16:03:11.486656 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */

  create view "mydb"."dbt_warehouse"."test__dbt_tmp"
    
    
  as (
    

select

    
    'bank_transfer'
    
    'credit_card'
    
    'gift_card'
    
  );
[0m16:03:11.493768 [debug] [Thread-4 (]: SQL status: CREATE VIEW in 0.007 seconds
[0m16:03:11.497623 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.497838 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test" rename to "test__dbt_backup"
[0m16:03:11.499201 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:11.501539 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.501752 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
alter table "mydb"."dbt_warehouse"."test__dbt_tmp" rename to "test"
[0m16:03:11.502304 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.000 seconds
[0m16:03:11.517161 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m16:03:11.517477 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.517698 [debug] [Thread-4 (]: On model.maker_warehouse.test: COMMIT
[0m16:03:11.519830 [debug] [Thread-4 (]: SQL status: COMMIT in 0.002 seconds
[0m16:03:11.523404 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."test__dbt_backup"
[0m16:03:11.525916 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.test"
[0m16:03:11.526141 [debug] [Thread-4 (]: On model.maker_warehouse.test: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.test"} */
drop view if exists "mydb"."dbt_warehouse"."test__dbt_backup" cascade
[0m16:03:11.529081 [debug] [Thread-4 (]: SQL status: DROP VIEW in 0.003 seconds
[0m16:03:11.530815 [debug] [Thread-4 (]: On model.maker_warehouse.test: Close
[0m16:03:11.532113 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10586e1d0>]}
[0m16:03:11.532598 [info ] [Thread-4 (]: 4 of 5 OK created sql view model dbt_warehouse.test ............................ [[32mCREATE VIEW[0m in 0.12s]
[0m16:03:11.532935 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.test
[0m16:03:11.533144 [debug] [Thread-4 (]: Began running node model.maker_warehouse.users
[0m16:03:11.533419 [info ] [Thread-4 (]: 5 of 5 START sql table model dbt_warehouse.users ............................... [RUN]
[0m16:03:11.533688 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly model.maker_warehouse.test, now model.maker_warehouse.users)
[0m16:03:11.533888 [debug] [Thread-4 (]: Began compiling node model.maker_warehouse.users
[0m16:03:11.536971 [debug] [Thread-4 (]: Writing injected SQL for node "model.maker_warehouse.users"
[0m16:03:11.537369 [debug] [Thread-4 (]: Began executing node model.maker_warehouse.users
[0m16:03:11.540171 [debug] [Thread-4 (]: Writing runtime sql for node "model.maker_warehouse.users"
[0m16:03:11.540610 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:11.540841 [debug] [Thread-4 (]: On model.maker_warehouse.users: BEGIN
[0m16:03:11.541125 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:03:11.552462 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m16:03:11.552796 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:11.553071 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */

  
    

  create  table "mydb"."dbt_warehouse"."users__dbt_tmp"
  
  
    as
  
  (
    

with base as (
  select
    user_id,
    datetime,
    row_number() over (partition by user_id order by datetime desc) as rn
  from mydb.public.raw_netflix
)
select
  user_id 
from base
where rn = 1
  );
  
[0m16:03:12.917273 [debug] [Thread-1 (]: SQL status: SELECT 1190 in 1.427 seconds
[0m16:03:12.944003 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:12.944355 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres" rename to "genres__dbt_backup"
[0m16:03:12.948887 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.004 seconds
[0m16:03:12.951390 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:12.951602 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
alter table "mydb"."dbt_warehouse"."genres__dbt_tmp" rename to "genres"
[0m16:03:12.953405 [debug] [Thread-1 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:12.954536 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m16:03:12.954739 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:12.954910 [debug] [Thread-1 (]: On model.maker_warehouse.genres: COMMIT
[0m16:03:12.958665 [debug] [Thread-1 (]: SQL status: COMMIT in 0.004 seconds
[0m16:03:12.960824 [debug] [Thread-1 (]: Applying DROP to: "mydb"."dbt_warehouse"."genres__dbt_backup"
[0m16:03:12.963209 [debug] [Thread-1 (]: Using postgres connection "model.maker_warehouse.genres"
[0m16:03:12.963412 [debug] [Thread-1 (]: On model.maker_warehouse.genres: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.genres"} */
drop table if exists "mydb"."dbt_warehouse"."genres__dbt_backup" cascade
[0m16:03:12.969864 [debug] [Thread-1 (]: SQL status: DROP TABLE in 0.006 seconds
[0m16:03:12.973040 [debug] [Thread-1 (]: On model.maker_warehouse.genres: Close
[0m16:03:12.977473 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1059d8bd0>]}
[0m16:03:12.979006 [info ] [Thread-1 (]: 1 of 5 OK created sql table model dbt_warehouse.genres ......................... [[32mSELECT 1190[0m in 1.56s]
[0m16:03:12.979958 [debug] [Thread-1 (]: Finished running node model.maker_warehouse.genres
[0m16:03:13.420874 [debug] [Thread-2 (]: SQL status: SELECT 8472 in 1.934 seconds
[0m16:03:13.427819 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:13.428689 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies" rename to "movies__dbt_backup"
[0m16:03:13.430973 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.002 seconds
[0m16:03:13.436246 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:13.436797 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
alter table "mydb"."dbt_warehouse"."movies__dbt_tmp" rename to "movies"
[0m16:03:13.437793 [debug] [Thread-2 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:13.440011 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m16:03:13.440522 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:13.440967 [debug] [Thread-2 (]: On model.maker_warehouse.movies: COMMIT
[0m16:03:13.467077 [debug] [Thread-2 (]: SQL status: COMMIT in 0.026 seconds
[0m16:03:13.471806 [debug] [Thread-2 (]: Applying DROP to: "mydb"."dbt_warehouse"."movies__dbt_backup"
[0m16:03:13.472858 [debug] [Thread-2 (]: Using postgres connection "model.maker_warehouse.movies"
[0m16:03:13.473317 [debug] [Thread-2 (]: On model.maker_warehouse.movies: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.movies"} */
drop table if exists "mydb"."dbt_warehouse"."movies__dbt_backup" cascade
[0m16:03:13.491218 [debug] [Thread-2 (]: SQL status: DROP TABLE in 0.017 seconds
[0m16:03:13.493705 [debug] [Thread-2 (]: On model.maker_warehouse.movies: Close
[0m16:03:13.496275 [debug] [Thread-2 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106ea2b50>]}
[0m16:03:13.497646 [info ] [Thread-2 (]: 2 of 5 OK created sql table model dbt_warehouse.movies ......................... [[32mSELECT 8472[0m in 2.08s]
[0m16:03:13.498461 [debug] [Thread-2 (]: Finished running node model.maker_warehouse.movies
[0m16:03:14.089187 [debug] [Thread-3 (]: SQL status: SELECT 671736 in 2.602 seconds
[0m16:03:14.093909 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:14.094155 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams" rename to "streams__dbt_backup"
[0m16:03:14.095728 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:14.100608 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:14.100828 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
alter table "mydb"."dbt_warehouse"."streams__dbt_tmp" rename to "streams"
[0m16:03:14.101547 [debug] [Thread-3 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:14.102355 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m16:03:14.102549 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:14.102706 [debug] [Thread-3 (]: On model.maker_warehouse.streams: COMMIT
[0m16:03:14.114799 [debug] [Thread-3 (]: SQL status: COMMIT in 0.012 seconds
[0m16:03:14.116234 [debug] [Thread-3 (]: Applying DROP to: "mydb"."dbt_warehouse"."streams__dbt_backup"
[0m16:03:14.116590 [debug] [Thread-3 (]: Using postgres connection "model.maker_warehouse.streams"
[0m16:03:14.116754 [debug] [Thread-3 (]: On model.maker_warehouse.streams: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.streams"} */
drop table if exists "mydb"."dbt_warehouse"."streams__dbt_backup" cascade
[0m16:03:14.135748 [debug] [Thread-3 (]: SQL status: DROP TABLE in 0.018 seconds
[0m16:03:14.138169 [debug] [Thread-3 (]: On model.maker_warehouse.streams: Close
[0m16:03:14.139029 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10b0a4890>]}
[0m16:03:14.139864 [info ] [Thread-3 (]: 3 of 5 OK created sql table model dbt_warehouse.streams ........................ [[32mSELECT 671736[0m in 2.73s]
[0m16:03:14.140620 [debug] [Thread-3 (]: Finished running node model.maker_warehouse.streams
[0m16:03:14.162690 [debug] [Thread-4 (]: SQL status: SELECT 161918 in 2.609 seconds
[0m16:03:14.167314 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:14.167827 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users" rename to "users__dbt_backup"
[0m16:03:14.169338 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:14.172402 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:14.172617 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
alter table "mydb"."dbt_warehouse"."users__dbt_tmp" rename to "users"
[0m16:03:14.173320 [debug] [Thread-4 (]: SQL status: ALTER TABLE in 0.001 seconds
[0m16:03:14.174073 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m16:03:14.174234 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:14.174378 [debug] [Thread-4 (]: On model.maker_warehouse.users: COMMIT
[0m16:03:14.177926 [debug] [Thread-4 (]: SQL status: COMMIT in 0.003 seconds
[0m16:03:14.179477 [debug] [Thread-4 (]: Applying DROP to: "mydb"."dbt_warehouse"."users__dbt_backup"
[0m16:03:14.179853 [debug] [Thread-4 (]: Using postgres connection "model.maker_warehouse.users"
[0m16:03:14.180062 [debug] [Thread-4 (]: On model.maker_warehouse.users: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "model.maker_warehouse.users"} */
drop table if exists "mydb"."dbt_warehouse"."users__dbt_backup" cascade
[0m16:03:14.184013 [debug] [Thread-4 (]: SQL status: DROP TABLE in 0.004 seconds
[0m16:03:14.184607 [debug] [Thread-4 (]: On model.maker_warehouse.users: Close
[0m16:03:14.185093 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f5d78c91-01dd-4844-b1ee-994baf3e0d43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106b91650>]}
[0m16:03:14.185403 [info ] [Thread-4 (]: 5 of 5 OK created sql table model dbt_warehouse.users .......................... [[32mSELECT 161918[0m in 2.65s]
[0m16:03:14.185683 [debug] [Thread-4 (]: Finished running node model.maker_warehouse.users
[0m16:03:14.186904 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:14.187051 [debug] [MainThread]: On master: BEGIN
[0m16:03:14.187189 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:03:14.200175 [debug] [MainThread]: SQL status: BEGIN in 0.013 seconds
[0m16:03:14.200502 [debug] [MainThread]: On master: COMMIT
[0m16:03:14.200737 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:14.200973 [debug] [MainThread]: On master: COMMIT
[0m16:03:14.201651 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:03:14.201841 [debug] [MainThread]: On master: Close
[0m16:03:14.202374 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:03:14.202537 [debug] [MainThread]: Connection 'model.maker_warehouse.genres' was properly closed.
[0m16:03:14.202819 [debug] [MainThread]: Connection 'model.maker_warehouse.movies' was properly closed.
[0m16:03:14.202982 [debug] [MainThread]: Connection 'model.maker_warehouse.streams' was properly closed.
[0m16:03:14.203321 [debug] [MainThread]: Connection 'model.maker_warehouse.users' was properly closed.
[0m16:03:14.203786 [info ] [MainThread]: 
[0m16:03:14.204082 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 3.00 seconds (3.00s).
[0m16:03:14.204874 [debug] [MainThread]: Command end result
[0m16:03:14.232158 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:03:14.234307 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:03:14.239009 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m16:03:14.239228 [info ] [MainThread]: 
[0m16:03:14.239485 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:03:14.239664 [info ] [MainThread]: 
[0m16:03:14.239892 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m16:03:14.241742 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 3.4663131, "process_in_blocks": "0", "process_kernel_time": 0.24146, "process_mem_max_rss": "128729088", "process_out_blocks": "0", "process_user_time": 1.200149}
[0m16:03:14.242105 [debug] [MainThread]: Command `dbt run` succeeded at 16:03:14.242048 after 3.47 seconds
[0m16:03:14.242392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101200ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1056ee810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x101158ad0>]}
[0m16:03:14.242601 [debug] [MainThread]: Flushing usage events
[0m16:03:14.738543 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:03:22.951035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105069f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cceb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cf3fd0>]}


============================== 16:03:22.953600 | 8832bbf0-0318-483d-97d6-1a2eb8ad7580 ==============================
[0m16:03:22.953600 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:03:22.953928 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'indirect_selection': 'eager', 'quiet': 'False', 'write_json': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'printer_width': '80', 'introspect': 'True', 'version_check': 'True', 'empty': 'None', 'debug': 'False', 'use_experimental_parser': 'False', 'use_colors': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'invocation_command': 'dbt test --select streams', 'fail_fast': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'cache_selected_only': 'False', 'no_print': 'None'}
[0m16:03:23.042305 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10911c250>]}
[0m16:03:23.071323 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x105078a90>]}
[0m16:03:23.071849 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m16:03:23.206516 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:03:23.291533 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m16:03:23.291754 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m16:03:23.314587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a05ff10>]}
[0m16:03:23.358635 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:03:23.359890 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:03:23.376489 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10a1ecd10>]}
[0m16:03:23.376741 [info ] [MainThread]: Found 5 models, 6 data tests, 1 source, 568 macros
[0m16:03:23.376918 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x109c9b1d0>]}
[0m16:03:23.377755 [info ] [MainThread]: 
[0m16:03:23.377939 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:03:23.378104 [info ] [MainThread]: 
[0m16:03:23.378447 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m16:03:23.380604 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m16:03:23.436366 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:03:23.436593 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m16:03:23.436735 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:03:23.472627 [debug] [ThreadPool]: SQL status: BEGIN in 0.036 seconds
[0m16:03:23.472886 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:03:23.473073 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m16:03:23.498128 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.025 seconds
[0m16:03:23.499136 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m16:03:23.500490 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m16:03:23.504123 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:23.504360 [debug] [MainThread]: On master: BEGIN
[0m16:03:23.504540 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:03:23.514038 [debug] [MainThread]: SQL status: BEGIN in 0.009 seconds
[0m16:03:23.514312 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:23.514562 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m16:03:23.522479 [debug] [MainThread]: SQL status: SELECT 32 in 0.008 seconds
[0m16:03:23.523910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8832bbf0-0318-483d-97d6-1a2eb8ad7580', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10715ae10>]}
[0m16:03:23.524247 [debug] [MainThread]: On master: ROLLBACK
[0m16:03:23.524709 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:23.524901 [debug] [MainThread]: On master: BEGIN
[0m16:03:23.525545 [debug] [MainThread]: SQL status: BEGIN in 0.000 seconds
[0m16:03:23.525844 [debug] [MainThread]: On master: COMMIT
[0m16:03:23.526072 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:23.526238 [debug] [MainThread]: On master: COMMIT
[0m16:03:23.526657 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:03:23.526836 [debug] [MainThread]: On master: Close
[0m16:03:23.529320 [debug] [Thread-1 (]: Began running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:03:23.529572 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:03:23.529773 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:03:23.530010 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:03:23.530246 [info ] [Thread-1 (]: 1 of 6 START test has_at_least_one_row_streams_ ................................ [RUN]
[0m16:03:23.530492 [info ] [Thread-2 (]: 2 of 6 START test not_null_streams_datetime .................................... [RUN]
[0m16:03:23.530726 [info ] [Thread-3 (]: 3 of 6 START test not_null_streams_id .......................................... [RUN]
[0m16:03:23.530979 [info ] [Thread-4 (]: 4 of 6 START test not_null_streams_movie_id .................................... [RUN]
[0m16:03:23.531287 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2)
[0m16:03:23.531576 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44'
[0m16:03:23.531850 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_id.616e67536f'
[0m16:03:23.532116 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983'
[0m16:03:23.532320 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:03:23.532511 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:03:23.532698 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:03:23.532881 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:03:23.549027 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:03:23.551462 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:03:23.554058 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:03:23.556003 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:03:23.556302 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:03:23.556679 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:03:23.566031 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:03:23.567505 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:03:23.568956 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:03:23.569421 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:03:23.569611 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:03:23.569854 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: BEGIN
[0m16:03:23.570031 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:03:23.570230 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: BEGIN
[0m16:03:23.570405 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:03:23.570610 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: BEGIN
[0m16:03:23.570788 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:03:23.571090 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:03:23.574731 [debug] [Thread-1 (]: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m16:03:23.575056 [error] [Thread-1 (]: 1 of 6 ERROR has_at_least_one_row_streams_ ..................................... [[31mERROR[0m in 0.04s]
[0m16:03:23.575339 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2
[0m16:03:23.575700 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:03:23.575992 [debug] [Thread-7 (]: Marking all children of 'test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2' to be skipped because of status 'error'.  Reason: Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps"..
[0m16:03:23.576377 [info ] [Thread-1 (]: 5 of 6 START test not_null_streams_user_id ..................................... [RUN]
[0m16:03:23.577356 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.has_at_least_one_row_streams_.848e1f42f2, now test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8)
[0m16:03:23.577561 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:03:23.580960 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:03:23.581356 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:03:23.582751 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:03:23.583092 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:03:23.583257 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: BEGIN
[0m16:03:23.583409 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:03:23.585781 [debug] [Thread-4 (]: SQL status: BEGIN in 0.015 seconds
[0m16:03:23.585992 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:03:23.586181 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."streams"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m16:03:23.586847 [debug] [Thread-2 (]: SQL status: BEGIN in 0.016 seconds
[0m16:03:23.587245 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:03:23.587473 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_datetime.e4db369b44"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."streams"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m16:03:23.589797 [debug] [Thread-3 (]: SQL status: BEGIN in 0.019 seconds
[0m16:03:23.590122 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:03:23.590529 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_id.616e67536f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."streams"
where id is null



  
  
      
    ) dbt_internal_test
[0m16:03:23.597914 [debug] [Thread-1 (]: SQL status: BEGIN in 0.014 seconds
[0m16:03:23.598597 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:03:23.599109 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."streams"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:03:23.758003 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.171 seconds
[0m16:03:23.758279 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.171 seconds
[0m16:03:23.758468 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.168 seconds
[0m16:03:23.761122 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: ROLLBACK
[0m16:03:23.761315 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.162 seconds
[0m16:03:23.762154 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: ROLLBACK
[0m16:03:23.762796 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: ROLLBACK
[0m16:03:23.763553 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: ROLLBACK
[0m16:03:23.763730 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: Close
[0m16:03:23.764420 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: Close
[0m16:03:23.764592 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: Close
[0m16:03:23.764788 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: Close
[0m16:03:23.764254 [info ] [Thread-4 (]: 4 of 6 PASS not_null_streams_movie_id .......................................... [[32mPASS[0m in 0.23s]
[0m16:03:23.765176 [info ] [Thread-1 (]: 5 of 6 PASS not_null_streams_user_id ........................................... [[32mPASS[0m in 0.19s]
[0m16:03:23.765384 [info ] [Thread-2 (]: 2 of 6 PASS not_null_streams_datetime .......................................... [[32mPASS[0m in 0.23s]
[0m16:03:23.765797 [info ] [Thread-3 (]: 3 of 6 PASS not_null_streams_id ................................................ [[32mPASS[0m in 0.23s]
[0m16:03:23.766118 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:03:23.766373 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:03:23.766643 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:03:23.766889 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:03:23.767095 [debug] [Thread-4 (]: Began running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:03:23.767640 [info ] [Thread-4 (]: 6 of 6 START test unique_streams_id ............................................ [RUN]
[0m16:03:23.767910 [debug] [Thread-4 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_movie_id.fa18a97983, now test.maker_warehouse.unique_streams_id.9d424521fe)
[0m16:03:23.768081 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:03:23.774254 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:03:23.775046 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:03:23.776507 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:03:23.776867 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:03:23.777038 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: BEGIN
[0m16:03:23.777189 [debug] [Thread-4 (]: Opening a new connection, currently in state closed
[0m16:03:23.784075 [debug] [Thread-4 (]: SQL status: BEGIN in 0.007 seconds
[0m16:03:23.784292 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:03:23.784469 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_id.9d424521fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:03:24.081159 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.296 seconds
[0m16:03:24.083682 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: ROLLBACK
[0m16:03:24.084670 [debug] [Thread-4 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: Close
[0m16:03:24.085402 [info ] [Thread-4 (]: 6 of 6 PASS unique_streams_id .................................................. [[32mPASS[0m in 0.32s]
[0m16:03:24.086071 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:03:24.087585 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:24.088098 [debug] [MainThread]: On master: BEGIN
[0m16:03:24.088525 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:03:24.099177 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m16:03:24.099510 [debug] [MainThread]: On master: COMMIT
[0m16:03:24.099704 [debug] [MainThread]: Using postgres connection "master"
[0m16:03:24.099847 [debug] [MainThread]: On master: COMMIT
[0m16:03:24.100305 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:03:24.100495 [debug] [MainThread]: On master: Close
[0m16:03:24.100731 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:03:24.100884 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8' was properly closed.
[0m16:03:24.101014 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_datetime.e4db369b44' was properly closed.
[0m16:03:24.101140 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_id.616e67536f' was properly closed.
[0m16:03:24.101254 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_id.9d424521fe' was properly closed.
[0m16:03:24.101415 [info ] [MainThread]: 
[0m16:03:24.101585 [info ] [MainThread]: Finished running 6 data tests in 0 hours 0 minutes and 0.72 seconds (0.72s).
[0m16:03:24.102531 [debug] [MainThread]: Command end result
[0m16:03:24.120544 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:03:24.121748 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:03:24.126875 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m16:03:24.127056 [info ] [MainThread]: 
[0m16:03:24.127259 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m16:03:24.127669 [info ] [MainThread]: 
[0m16:03:24.128202 [error] [MainThread]: [31mFailure in test has_at_least_one_row_streams_ (models/schema.yaml)[0m
[0m16:03:24.128380 [error] [MainThread]:   Compilation Error in test has_at_least_one_row_streams_ (models/schema.yaml)
  'test_has_at_least_one_row' is undefined. This can happen when calling a macro that does not exist. Check for typos and/or install package dependencies with "dbt deps".
[0m16:03:24.128534 [info ] [MainThread]: 
[0m16:03:24.128691 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m16:03:24.130832 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 1.2150542, "process_in_blocks": "0", "process_kernel_time": 0.175527, "process_mem_max_rss": "129073152", "process_out_blocks": "0", "process_user_time": 1.092373}
[0m16:03:24.131145 [debug] [MainThread]: Command `dbt test` failed at 16:03:24.131103 after 1.22 seconds
[0m16:03:24.131359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x103005110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10890a750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108cf3b10>]}
[0m16:03:24.131561 [debug] [MainThread]: Flushing usage events
[0m16:03:24.518479 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m16:05:55.103870 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x106cb23d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708c590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708cd10>]}


============================== 16:05:55.106625 | 7be16d17-d271-45d0-af45-df3b60709b46 ==============================
[0m16:05:55.106625 [info ] [MainThread]: Running with dbt=1.10.13
[0m16:05:55.106951 [debug] [MainThread]: running dbt with arguments {'log_path': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/logs', 'target_path': 'None', 'version_check': 'True', 'empty': 'None', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'printer_width': '80', 'introspect': 'True', 'invocation_command': 'dbt test --select streams', 'profiles_dir': '/Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects', 'quiet': 'False', 'no_print': 'None', 'fail_fast': 'False', 'debug': 'False', 'indirect_selection': 'eager', 'partial_parse': 'True', 'warn_error': 'None', 'log_format': 'default', 'cache_selected_only': 'False', 'use_colors': 'True'}
[0m16:05:55.218763 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1070b0210>]}
[0m16:05:55.247639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1046e5450>]}
[0m16:05:55.248207 [info ] [MainThread]: Registered adapter: postgres=1.9.1
[0m16:05:55.307069 [debug] [MainThread]: checksum: 9f0c81e2574b4ff463f3d16d080df83c6982fc0372c9feeacae0504ac9ea3ffe, vars: {}, profile: , target: , version: 1.10.13
[0m16:05:55.395955 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m16:05:55.396351 [debug] [MainThread]: Partial parsing: updated file: maker_warehouse://models/schema.yaml
[0m16:05:55.623461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108094550>]}
[0m16:05:55.666727 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:05:55.667955 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:05:55.686931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10827f050>]}
[0m16:05:55.687181 [info ] [MainThread]: Found 5 models, 5 data tests, 1 source, 568 macros
[0m16:05:55.687360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x108372b10>]}
[0m16:05:55.688190 [info ] [MainThread]: 
[0m16:05:55.688373 [info ] [MainThread]: Concurrency: 4 threads (target='dev')
[0m16:05:55.688515 [info ] [MainThread]: 
[0m16:05:55.688741 [debug] [MainThread]: Acquiring new postgres connection 'master'
[0m16:05:55.690640 [debug] [ThreadPool]: Acquiring new postgres connection 'list_mydb_dbt_warehouse'
[0m16:05:55.719946 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:05:55.720144 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: BEGIN
[0m16:05:55.720277 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m16:05:55.824015 [debug] [ThreadPool]: SQL status: BEGIN in 0.104 seconds
[0m16:05:55.824236 [debug] [ThreadPool]: Using postgres connection "list_mydb_dbt_warehouse"
[0m16:05:55.824394 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "list_mydb_dbt_warehouse"} */
select
      'mydb' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'dbt_warehouse'
    union all
    select
      'mydb' as database,
      matviewname as name,
      schemaname as schema,
      'materialized_view' as type
    from pg_matviews
    where schemaname ilike 'dbt_warehouse'
  
[0m16:05:55.844544 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.020 seconds
[0m16:05:55.845447 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: ROLLBACK
[0m16:05:55.846215 [debug] [ThreadPool]: On list_mydb_dbt_warehouse: Close
[0m16:05:55.849450 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:55.849633 [debug] [MainThread]: On master: BEGIN
[0m16:05:55.849773 [debug] [MainThread]: Opening a new connection, currently in state init
[0m16:05:55.857501 [debug] [MainThread]: SQL status: BEGIN in 0.008 seconds
[0m16:05:55.857696 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:55.857942 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "connection_name": "master"} */
select distinct
        dependent_namespace.nspname as dependent_schema,
        dependent_class.relname as dependent_name,
        referenced_namespace.nspname as referenced_schema,
        referenced_class.relname as referenced_name

    -- Query for views: views are entries in pg_class with an entry in pg_rewrite, but we avoid
    -- a seq scan on pg_rewrite by leveraging the fact there is an "internal" row in pg_depend for
    -- the view...
    from pg_class as dependent_class
    join pg_namespace as dependent_namespace on dependent_namespace.oid = dependent_class.relnamespace
    join pg_depend as dependent_depend on dependent_depend.refobjid = dependent_class.oid
        and dependent_depend.classid = 'pg_rewrite'::regclass
        and dependent_depend.refclassid = 'pg_class'::regclass
        and dependent_depend.deptype = 'i'

    -- ... and via pg_depend (that has a row per column, hence the need for "distinct" above, and
    -- making sure to exclude the internal row to avoid a view appearing to depend on itself)...
    join pg_depend as joining_depend on joining_depend.objid = dependent_depend.objid
        and joining_depend.classid = 'pg_rewrite'::regclass
        and joining_depend.refclassid = 'pg_class'::regclass
        and joining_depend.refobjid != dependent_depend.refobjid

    -- ... we can find the tables they query from in pg_class, but excluding system tables. Note we
    -- don't need need to exclude _dependent_ system tables, because they only query from other
    -- system tables, and so are automatically excluded by excluding _referenced_ system tables
    join pg_class as referenced_class on referenced_class.oid = joining_depend.refobjid
    join pg_namespace as referenced_namespace on referenced_namespace.oid = referenced_class.relnamespace
        and referenced_namespace.nspname != 'information_schema'
        and referenced_namespace.nspname not like 'pg\_%'

    order by
        dependent_schema, dependent_name, referenced_schema, referenced_name;
[0m16:05:55.866385 [debug] [MainThread]: SQL status: SELECT 32 in 0.008 seconds
[0m16:05:55.867547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '7be16d17-d271-45d0-af45-df3b60709b46', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1076b5390>]}
[0m16:05:55.867847 [debug] [MainThread]: On master: ROLLBACK
[0m16:05:55.868385 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:55.868598 [debug] [MainThread]: On master: BEGIN
[0m16:05:55.869916 [debug] [MainThread]: SQL status: BEGIN in 0.001 seconds
[0m16:05:55.870119 [debug] [MainThread]: On master: COMMIT
[0m16:05:55.870576 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:55.871003 [debug] [MainThread]: On master: COMMIT
[0m16:05:55.871555 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:05:55.871740 [debug] [MainThread]: On master: Close
[0m16:05:55.873754 [debug] [Thread-1 (]: Began running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:05:55.873973 [debug] [Thread-2 (]: Began running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:05:55.874187 [debug] [Thread-3 (]: Began running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:05:55.874422 [info ] [Thread-1 (]: 1 of 5 START test not_null_streams_datetime .................................... [RUN]
[0m16:05:55.874812 [debug] [Thread-4 (]: Began running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:05:55.875150 [info ] [Thread-2 (]: 2 of 5 START test not_null_streams_id .......................................... [RUN]
[0m16:05:55.875373 [info ] [Thread-3 (]: 3 of 5 START test not_null_streams_movie_id .................................... [RUN]
[0m16:05:55.875631 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly list_mydb_dbt_warehouse, now test.maker_warehouse.not_null_streams_datetime.e4db369b44)
[0m16:05:55.875830 [info ] [Thread-4 (]: 4 of 5 START test not_null_streams_user_id ..................................... [RUN]
[0m16:05:55.876110 [debug] [Thread-2 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_id.616e67536f'
[0m16:05:55.876353 [debug] [Thread-3 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983'
[0m16:05:55.876536 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:05:55.876767 [debug] [Thread-4 (]: Acquiring new postgres connection 'test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8'
[0m16:05:55.876939 [debug] [Thread-2 (]: Began compiling node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:05:55.877112 [debug] [Thread-3 (]: Began compiling node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:05:55.888817 [debug] [Thread-4 (]: Began compiling node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:05:55.889685 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:05:55.892204 [debug] [Thread-2 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:05:55.894384 [debug] [Thread-3 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:05:55.898853 [debug] [Thread-4 (]: Writing injected SQL for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:05:55.899887 [debug] [Thread-4 (]: Began executing node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:05:55.900132 [debug] [Thread-3 (]: Began executing node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:05:55.900326 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:05:55.900496 [debug] [Thread-2 (]: Began executing node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:05:55.917363 [debug] [Thread-4 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:05:55.923831 [debug] [Thread-3 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:05:55.925235 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:05:55.926589 [debug] [Thread-2 (]: Writing runtime sql for node "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:05:55.927374 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:05:55.927585 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:05:55.927771 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:05:55.928079 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:05:55.928324 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: BEGIN
[0m16:05:55.928518 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: BEGIN
[0m16:05:55.928700 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: BEGIN
[0m16:05:55.928882 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: BEGIN
[0m16:05:55.929067 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m16:05:55.929235 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m16:05:55.929398 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:55.929555 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m16:05:55.938979 [debug] [Thread-3 (]: SQL status: BEGIN in 0.010 seconds
[0m16:05:55.939325 [debug] [Thread-3 (]: Using postgres connection "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"
[0m16:05:55.939582 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_movie_id.fa18a97983"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select movie_id
from "mydb"."dbt_warehouse"."streams"
where movie_id is null



  
  
      
    ) dbt_internal_test
[0m16:05:55.940389 [debug] [Thread-4 (]: SQL status: BEGIN in 0.011 seconds
[0m16:05:55.940691 [debug] [Thread-4 (]: Using postgres connection "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"
[0m16:05:55.940895 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select user_id
from "mydb"."dbt_warehouse"."streams"
where user_id is null



  
  
      
    ) dbt_internal_test
[0m16:05:55.941182 [debug] [Thread-1 (]: SQL status: BEGIN in 0.012 seconds
[0m16:05:55.941412 [debug] [Thread-2 (]: SQL status: BEGIN in 0.012 seconds
[0m16:05:55.941677 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.not_null_streams_datetime.e4db369b44"
[0m16:05:55.941882 [debug] [Thread-2 (]: Using postgres connection "test.maker_warehouse.not_null_streams_id.616e67536f"
[0m16:05:55.942545 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_datetime.e4db369b44"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select datetime
from "mydb"."dbt_warehouse"."streams"
where datetime is null



  
  
      
    ) dbt_internal_test
[0m16:05:55.942834 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.not_null_streams_id.616e67536f"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from "mydb"."dbt_warehouse"."streams"
where id is null



  
  
      
    ) dbt_internal_test
[0m16:05:56.017560 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.074 seconds
[0m16:05:56.017936 [debug] [Thread-3 (]: SQL status: SELECT 1 in 0.078 seconds
[0m16:05:56.018131 [debug] [Thread-4 (]: SQL status: SELECT 1 in 0.077 seconds
[0m16:05:56.018341 [debug] [Thread-2 (]: SQL status: SELECT 1 in 0.075 seconds
[0m16:05:56.020611 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: ROLLBACK
[0m16:05:56.021402 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: ROLLBACK
[0m16:05:56.022079 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: ROLLBACK
[0m16:05:56.022698 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: ROLLBACK
[0m16:05:56.023183 [debug] [Thread-1 (]: On test.maker_warehouse.not_null_streams_datetime.e4db369b44: Close
[0m16:05:56.023358 [debug] [Thread-4 (]: On test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8: Close
[0m16:05:56.023520 [debug] [Thread-3 (]: On test.maker_warehouse.not_null_streams_movie_id.fa18a97983: Close
[0m16:05:56.023670 [debug] [Thread-2 (]: On test.maker_warehouse.not_null_streams_id.616e67536f: Close
[0m16:05:56.024007 [info ] [Thread-1 (]: 1 of 5 PASS not_null_streams_datetime .......................................... [[32mPASS[0m in 0.15s]
[0m16:05:56.024356 [info ] [Thread-4 (]: 4 of 5 PASS not_null_streams_user_id ........................................... [[32mPASS[0m in 0.15s]
[0m16:05:56.024744 [info ] [Thread-3 (]: 3 of 5 PASS not_null_streams_movie_id .......................................... [[32mPASS[0m in 0.15s]
[0m16:05:56.025450 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.not_null_streams_datetime.e4db369b44
[0m16:05:56.025197 [info ] [Thread-2 (]: 2 of 5 PASS not_null_streams_id ................................................ [[32mPASS[0m in 0.15s]
[0m16:05:56.025749 [debug] [Thread-4 (]: Finished running node test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8
[0m16:05:56.026072 [debug] [Thread-3 (]: Finished running node test.maker_warehouse.not_null_streams_movie_id.fa18a97983
[0m16:05:56.026291 [debug] [Thread-1 (]: Began running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:05:56.026563 [debug] [Thread-2 (]: Finished running node test.maker_warehouse.not_null_streams_id.616e67536f
[0m16:05:56.026853 [info ] [Thread-1 (]: 5 of 5 START test unique_streams_id ............................................ [RUN]
[0m16:05:56.027141 [debug] [Thread-1 (]: Re-using an available connection from the pool (formerly test.maker_warehouse.not_null_streams_datetime.e4db369b44, now test.maker_warehouse.unique_streams_id.9d424521fe)
[0m16:05:56.027322 [debug] [Thread-1 (]: Began compiling node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:05:56.031003 [debug] [Thread-1 (]: Writing injected SQL for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:05:56.031456 [debug] [Thread-1 (]: Began executing node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:05:56.033171 [debug] [Thread-1 (]: Writing runtime sql for node "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:05:56.033624 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:05:56.033785 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: BEGIN
[0m16:05:56.033931 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m16:05:56.040575 [debug] [Thread-1 (]: SQL status: BEGIN in 0.007 seconds
[0m16:05:56.040792 [debug] [Thread-1 (]: Using postgres connection "test.maker_warehouse.unique_streams_id.9d424521fe"
[0m16:05:56.040979 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: /* {"app": "dbt", "dbt_version": "1.10.13", "profile_name": "maker_warehouse", "target_name": "dev", "node_id": "test.maker_warehouse.unique_streams_id.9d424521fe"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from "mydb"."dbt_warehouse"."streams"
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m16:05:56.280806 [debug] [Thread-1 (]: SQL status: SELECT 1 in 0.239 seconds
[0m16:05:56.284716 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: ROLLBACK
[0m16:05:56.285790 [debug] [Thread-1 (]: On test.maker_warehouse.unique_streams_id.9d424521fe: Close
[0m16:05:56.286974 [info ] [Thread-1 (]: 5 of 5 PASS unique_streams_id .................................................. [[32mPASS[0m in 0.26s]
[0m16:05:56.287821 [debug] [Thread-1 (]: Finished running node test.maker_warehouse.unique_streams_id.9d424521fe
[0m16:05:56.289367 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:56.289859 [debug] [MainThread]: On master: BEGIN
[0m16:05:56.290282 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m16:05:56.301016 [debug] [MainThread]: SQL status: BEGIN in 0.011 seconds
[0m16:05:56.301346 [debug] [MainThread]: On master: COMMIT
[0m16:05:56.301590 [debug] [MainThread]: Using postgres connection "master"
[0m16:05:56.301760 [debug] [MainThread]: On master: COMMIT
[0m16:05:56.302109 [debug] [MainThread]: SQL status: COMMIT in 0.000 seconds
[0m16:05:56.302262 [debug] [MainThread]: On master: Close
[0m16:05:56.302485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m16:05:56.302632 [debug] [MainThread]: Connection 'test.maker_warehouse.unique_streams_id.9d424521fe' was properly closed.
[0m16:05:56.302757 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_id.616e67536f' was properly closed.
[0m16:05:56.302878 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_movie_id.fa18a97983' was properly closed.
[0m16:05:56.302993 [debug] [MainThread]: Connection 'test.maker_warehouse.not_null_streams_user_id.eaddc7d8c8' was properly closed.
[0m16:05:56.303148 [info ] [MainThread]: 
[0m16:05:56.303337 [info ] [MainThread]: Finished running 5 data tests in 0 hours 0 minutes and 0.61 seconds (0.61s).
[0m16:05:56.303990 [debug] [MainThread]: Command end result
[0m16:05:56.323533 [debug] [MainThread]: Wrote artifact WritableManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/manifest.json
[0m16:05:56.324903 [debug] [MainThread]: Wrote artifact SemanticManifest to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/semantic_manifest.json
[0m16:05:56.329405 [debug] [MainThread]: Wrote artifact RunExecutionResult to /Users/harrydonnelly/Documents/Proj/dbt_datawarehousing/dbt_projects/target/run_results.json
[0m16:05:56.329623 [info ] [MainThread]: 
[0m16:05:56.329823 [info ] [MainThread]: [32mCompleted successfully[0m
[0m16:05:56.329976 [info ] [MainThread]: 
[0m16:05:56.330137 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m16:05:56.331741 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.2664518, "process_in_blocks": "0", "process_kernel_time": 0.214714, "process_mem_max_rss": "138723328", "process_out_blocks": "0", "process_user_time": 1.240713}
[0m16:05:56.332014 [debug] [MainThread]: Command `dbt test` succeeded at 16:05:56.331963 after 1.27 seconds
[0m16:05:56.332233 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x1028b1110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708cc90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x10708cfd0>]}
[0m16:05:56.332434 [debug] [MainThread]: Flushing usage events
[0m16:05:56.703870 [debug] [MainThread]: An error was encountered while trying to flush usage events
